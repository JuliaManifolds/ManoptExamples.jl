var documenterSearchIndex = {"docs":
[{"location":"examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","page":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Ronny Bergmann 2023-11-06","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Introduction","page":"Frank Wolfe comparison","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"In this example we compare the Difference of Convex Algprithm (DCA) [BFSS23] with the Frank-Wolfe Algorithm, which was introduced in [WS22]. This example reproduces the results from [BFSS23], Section 7.3.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We consider the collowing constraint maximimization problem of the Fréchet mean on the symmetric positive definite matrices mathcal P(n) with the affine invariant metric. Let q_1ldotsq_m in mathcal P(n) be a set of points and mu_1ldotsmu_m be a set of weights, such that they sum to one. We consider then","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmax_pinmathcal C  h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"h(p) =\nsum_j=1^m mu_j d^2(pq_i)\nquad text where \nd^2(pq_i) = operatornametrbigl(\n  log^2(p^-frac12q_jp^-frac12)\nbig)\nqquadtextandqquad\nmathcal C =  pin mathcal M  bar Lpreceq p preceq bar U ","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"for a lower bound L and an upper bound U for the matrices in the positive definite sense A preceq B Leftrightarrow (B-A) is positive semi-definite","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"When every one of the weights mu_1 ldots mu_m are equal, this function h is known as the of the set q_1 dots q_m.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And for our example we set","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Random.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use as lower and upper bound the arithmetic and geometric mean L and U, respectively.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"L = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As a starting point, the Frank-Wolfe algorithm requires a feasible point. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p0 = (L+U)/2","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can check that it is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","page":"Frank Wolfe comparison","title":"Common Functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Given p in mathcal M, X in T_pmathcal M on the symmetric positive definite matrices M, this method computes the closed form solution to","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_qin  mathcal C langle X log_p qrangle\n  = operatorname*argmin_qin  mathcal C operatornametr(Slog(YqY))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"where mathcal C =  q  L preceq q preceq U , S = p^-12Xp^-12, and Y=p^-12.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The solution is given by Z=X^-1Qbigl( P^mathrmT-operatornamesgn(D)_+P+hatLbigr)Q^mathrmTX^-1,@ where S=QDQ^mathrmT is a diagonalization of S, hatU-hatL=P^mathrmTP with hatL=Q^mathrmTXLXQ and hatU=Q^mathrmTXUXQ, where -mboxsgn(D)_+ is the diagonal matrix","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatornamediagbigl(\n  -operatornamesgn(d_11)_+ ldots -operatornamesgn(d_nn)_+\nbigr)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and D=(d_ij).","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closeed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","page":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use g(p) = iota_mathcal C(p) as the indicator funtion of the set mathcal C. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"So we can first check that p0 is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"g(p0,L,U) == 0.0","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"true","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Now setting","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pinmathcal M g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We look for a maximum of h, where g is minimal, i.e. g(p) is zero or in other words p in mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The gradient of h can also be implemented in closed form as","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"grad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can further define the cost, which will just be +infty outside of mathcal C. We define","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Here we can omit the gradient of g in the definition of operatornamegrad f, since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As the last step, we can provide the closed form solver for the DC sub problem given at iteration k by","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pin mathcal C\n  biglangle -operatornamegrad h(p^(k)) exp^-1_p^(k)pbigrangle","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Which we con compute","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For safety, we might want to avoid ending up at the boundary of mathcal C. That is we reduce the distance we walk towards the solution q a bit.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    α = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= α\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($α)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","page":"Frank Wolfe comparison","title":"The DoC solver run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s compare both methods when they have the same stopping criteria","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial F(p): -0.77661458292831\nAt iteration 23 the change of the gradient (3.192989916935325e-13) was less than 1.0e-9.\n 18.424563 seconds (17.10 M allocations: 1.618 GiB, 4.35% gc time, 87.39% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 23 iterations\n\n## Parameters\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 300:  not reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\n\", 30]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s extract the final point and look at its cost","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.784425242474807","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As well as whether (and how well) it is feasible, that is the following values should all be larger than zero.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (1.1886583723800445e-12, 0.06669240322431051)\n (1.3411042178831775e-5, 0.0671353506908023)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For the statistics we extract the recordings from the state","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","page":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For Frank wolfe, the cost is just defined as -h(p) but the minimisation is constraint to mathcal C, which is enfored by the oracle.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","page":"Frank Wolfe comparison","title":"The FW Solver Run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Similarly we can run the Frank-Wolfe algorithm with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial f(x): -0.776615\n# 2000     f(x): -0.784420 |Δp|: 0.04611942377596  |grad f(p)|: 0.17693408  |Δgrad f(p)|: 0.17555618\n# 4000     f(x): -0.784421 |Δp|: 0.00372201632005  |grad f(p)|: 0.17694619  |Δgrad f(p)|: 0.00749427\n# 6000     f(x): -0.784422 |Δp|: 0.00205683506784  |grad f(p)|: 0.17695204  |Δgrad f(p)|: 0.00414088\n# 8000     f(x): -0.784422 |Δp|: 0.00140675676260  |grad f(p)|: 0.17695565  |Δgrad f(p)|: 0.00283200\n# 10000    f(x): -0.784422 |Δp|: 0.00106177438611  |grad f(p)|: 0.17695815  |Δgrad f(p)|: 0.00213746\nThe algorithm reached its maximal number of iterations (10000).\n298.173386 seconds (56.06 M allocations: 93.635 GiB, 3.50% gc time, 0.43% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stepsize\nDecreasingStepsize(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2)\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 10000:    reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"f(x): %f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\n\", 2000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we take a look at this result as well","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844220281765162","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And its feasibility","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (4.904818928410655e-10, 0.06659173821656107)\n (3.245654983213335e-5, 0.06713970236096602)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Statistics","page":"Frank Wolfe comparison","title":"Statistics","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We extract the recorded values","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"# DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And let’s plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\")","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"This indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"On the other hand, Frank-Wolfe still has not reached this level function value after 10^4 iterations.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Literature","page":"Frank Wolfe comparison","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Pages = [\"examples/Difference-of-Convex-Frank-Wolfe.md\"]\nCanonical=false","category":"page"},{"location":"references/#Literature","page":"References","title":"Literature","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"EditURL = \"https://github.com/JuliaManifolds/ManoptExamples.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing/#Contributing-to-Manopt.jl","page":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The following is a set of guidelines to ManoptExamples.jl.","category":"page"},{"location":"contributing/#Table-of-Contents","page":"Contributing to ManoptExamples.jl","title":"Table of Contents","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Contributing to Manopt.jl\nTable of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd an objective\nCode style","category":"page"},{"location":"contributing/#I-just-have-a-question","page":"Contributing to ManoptExamples.jl","title":"I just have a question","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on our GitHub discussion.","category":"page"},{"location":"contributing/#How-can-I-file-an-issue?","page":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"page"},{"location":"contributing/#How-can-I-contribute?","page":"Contributing to ManoptExamples.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing/#Add-an-objective","page":"Contributing to ManoptExamples.jl","title":"Add an objective","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The objective in Manopt.jl represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a Problem.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have a specific objective you would like to provide here, feel free to start a new file in the src/objectives/ folder in your own fork and propose it later as a Pull Request.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in src/functions/ and follows the naming scheme:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"cost functions are always of the form cost_ and a fitting name\ngradient functions are always of the the gradient_ and a fitting name, followed by an !","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"for in-place gradients and by !! if it is a struct that can provide both.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"It would be great if you could also add a small test for the functions and the problem you defined in the test/ section.","category":"page"},{"location":"contributing/#Add-an-example","page":"Contributing to ManoptExamples.jl","title":"Add an example","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding Quarto Markdown file to the examples/ folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the bib/literature.yaml file to add references (in CSL_YAML, which can for example be exported e.g. from Zotero).","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Add any packages you need to the examples/ environment (see the containting Project.toml). The examples will not be run on CI, but their rendered CommonMark outpout should be included in the list of examples in the documentation of this package.","category":"page"},{"location":"contributing/#Code-style","page":"Contributing to ManoptExamples.jl","title":"Code style","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We also follow a few internal conventions:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Any implemented function should be accompanied by its mathematical formulae if a closed form exists.\nwithin a file the structs should come first and functions second. The only exception are constructors for the structs\nwithin both blocks an alphabetical order is preferable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including in-place/non-mutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature.\nAll import/using/include should be in the main module file.\nThere should only be a minimum of exports within this file, all problems should usually be later addressed as ManoptExamples.[...]\nthe Quarto Markdown files are excluded from this formatting.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","page":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Introduction","page":"A Benchmark","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"In this Benchmark we compare the Difference of Convex Algprithm (DCA) [BFSS23] and the Difference of Convex Proximal Point Algorithm (DCPPA) [SO15] which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from [BFSS23], Section 7.1.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"operatorname*argmin_pinmathcal M   g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where ghcolon mathcal M to mathbb R are geodesically convex function on the Riemannian manifold mathcal M.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#The-DC-Problem","page":"A Benchmark","title":"The DC Problem","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We start with defining the two convex functions gh and their gradients as well as the DC problem f and its gradient for the problem","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"    operatorname*argmin_pinmathcal M  bigl( logbigr(det(p)bigr)bigr)^4 - bigl(log det(p) bigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where the critical points obtain a functional value of -frac14.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where mathcal M is the manifold of symmetric positive definite (SPD) matrices with the affine invariant metric, which is the default.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We first define the corresponding functions","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"g(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and their gradients","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"grad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which we can use to verify that the gradients of g and h are correct. We use for that","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"n = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"to tall both checks","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, g, grad_g, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, h, grad_h, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which both pass the test. We continue to define their inplace variants","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"function grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And compare times for both algorithms, with a bit of debug output.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |δp|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_state=TrustRegionsState(M, copy(M, p0)),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |δp|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |δp|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |δp|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000003 |δp|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (3.9079528504063575e-11) is less than 1.0e-10.\n  7.078460 seconds (8.15 M allocations: 560.347 MiB, 4.11% gc time, 98.71% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The cost is","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dca)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Similarly the DCPPA performs","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    λ=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|δp|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantStepsize(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_state=TrustRegionsState(M, copy(M, p0)),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470 \n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|δp|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|δp|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|δp|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|δp|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|δp|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|δp|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|δp|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.770714 seconds (1.78 M allocations: 124.191 MiB, 2.11% gc time, 95.56% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"It needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dcppa)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","page":"A Benchmark","title":"Benchmark I: Time comparison","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and run a benchmark for both algorithms","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"for n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state=TrustRegionsState($Mn, copy($Mn, $pn)),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        λ=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantStepsize(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state=TrustRegionsState($Mn, copy($Mn, $pn)),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Since we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","page":"A Benchmark","title":"Benchmark II: Iterations and cost.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"As a second benchmark, let’s collect the number of iterations needed and the development of the cost over dimensions.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"N2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}()","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state = TrustRegionsState(Mn, copy(Mn, pn)),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        λ = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantStepsize(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state = TrustRegionsState(Mn, copy(Mn, pn)),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The iterations are like","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And for the developtment of the cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where we can see that the DCA needs less iterations than the DCPPA.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Literature","page":"A Benchmark","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Pages = [\"examples/Difference-of-Convex-Benchmark.md\"]\nCanonical=false","category":"page"},{"location":"objectives/#List-of-Objectives-defined-for-the-Examples","page":"Objectives","title":"List of Objectives defined for the Examples","text":"","category":"section"},{"location":"objectives/#Rayleigh","page":"Objectives","title":"Rayleigh Quotient on the Sphere","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rayleigh example (TODO) to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RayleighQuotient.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RayleighQuotientCost","page":"Objectives","title":"ManoptExamples.RayleighQuotientCost","text":"RayleighQuotientCost\n\nA functor representing the Rayleigh Quotient cost function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Rayleigh Quotient in two forms. Either\n\nf(p) = p^mathrmTApqquad p  𝕊^n-1\n\nor extended into the embedding as\n\nf(x) = x^mathrmTAx qquad x  ℝ^n\n\nwhich is not the orignal Rayleigh quotient for performance reasons, but useful if you want to use this as the Euclidean cost in the emedding of 𝕊^n-1.\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientCost(A)\n\nCreate the Rayleigh cost function.\n\nSee also\n\nRayleighQuotientGrad!!, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientGrad!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientGrad!!","text":"RayleighQuotientGrad!!\n\nA functor representing the Rayleigh Quotient gradient function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the gradient of the Rayleigh Quotient in two forms. Either\n\noperatornamegrad f(p) = 2 Ap - 2 (p^mathrmTAp)*pqquad p  𝕊^n-1\n\nor taking the Euclidean gradient of the Rayleigh quotient on the sphere as\n\nf(x) = 2Ax qquad x  ℝ^n\n\nFor details, see Example 3.62 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientGrad!!(A)\n\nCreate the Rayleigh quotient gradient function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientHess!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientHess!!","text":"RayleighQuotientHess!!\n\nA functor representing the Rayleigh Quotient Hessian.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Hessian of the Rayleigh Quotient in two forms. Either\n\noperatornameHess f(p)X = 2 bigl(AX - (p^mathrmTAX)p - (p^mathrmTAp)Xbigr)qquad p  𝕊^n-1 X in T_p𝕊^n-1\n\nor taking the Euclidean Hessian of the Rayleigh quotient on the sphere as\n\n^2f(x)V = 2AV qquad x V  ℝ^n\n\nFor details, see Example 5.27 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientHess!!(A)\n\nCreate the Rayleigh quotient Hessian function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientGrad!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#RiemannianMean","page":"Objectives","title":"Riemannian Mean","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Riemannian mean example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RiemannianMean.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RiemannianMeanCost","page":"Objectives","title":"ManoptExamples.RiemannianMeanCost","text":"RiemannianMeanCost{P}\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\nf(p) = sum_j=i^N d_mathcal M^2(d_i p)\n\nwhere d_mathcal M is the distance on a Riemannian manifold.\n\nConstructor\n\nRiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P}\n\nInitialize the cost function to a data set data of points on a manfiold M, where each point is of type P.\n\nSee also\n\nRiemannianMeanGradient!!, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RiemannianMeanGradient!!","page":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","text":"RiemannianMeanGradient!!{P} where P\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\noperatornamegradf(p) = sum_j=i^N log_p d_i\n\nwhere d_mathcal M is the distance on a Riemannian manifold and we employ grad_distance to compute the single summands.\n\nThis functor provides both the allocating variant grad_f(M,p) as well as the in-place variant grad_f!(M, X, p) which computes the gradient in-place of X.\n\nConstructors\n\nRiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T}\n\nGenerate the Riemannian mean gradient based on some data points data an intial tangent vector initial_vector. If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant.\n\nRiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T}\n\nInitialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold M is provideed.\n\nSee also\n\nRiemannianMeanCost, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.Riemannian_mean_objective-Tuple{AbstractVector}","page":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","text":"Riemannian_mean_objective(data, initial_vector=nothing, evaluation=AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n)\n\nGenerate the objective for the Riemannian mean task for some given vector of data points on the Riemannian manifold M.\n\nSee also\n\nRiemannianMeanCost, RiemannianMeanGradient!!\n\nnote: Note\nThe first constructor should only be used if an additional storage of the vector is not feasible, since initialising the initial_vector to nothing disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#RobustPCA","page":"Objectives","title":"Robust PCA","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Robust PCA example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RobustPCA.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RobustPCACost","page":"Objectives","title":"ManoptExamples.RobustPCACost","text":"RobustPCACost{D,F}\n\nA functor representing the Riemannian robust PCA function on the Grassmann manifold. For some given (column) data Dmathbb R^dtimes n the cost function is defined on some operatornameGr(dm), mn as the sum of the distances of the columns D_i to the subspace spanned by pinoperatornameGr(dm) (represented as a point on the Stiefel manifold). The function reads\n\nf(U) = frac1nsum_i=1^n lVert pp^mathrmTD_i - D_irVert\n\nThis cost additionally provides a Huber regularisation of the cost, that is for some ε0 one use ℓ_ε(x) = sqrtx^2+ε^2 - ε in\n\nf_ε(p) = frac1nsum_i=1^n ℓ_εbigl(lVert pp^mathrmTD_i - D_irVertbigr)\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCACost(data::AbstractMatrix, ε=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, ε=1.0)\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RobustPCAGrad!!","page":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","text":"RobustPCAGrad!!{D,F}\n\nA functor representing the Riemannian robust PCA gradient on the Grassmann manifold. For some given (column) data Xmathbb R^ptimes n the gradient of the RobustPCACost can be computed by projecting the Euclidean gradient onto the corresponding tangent space.\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCAGrad!!(data, ε=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, ε=1.0; evaluation=AllocatingEvaluation())\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the evaluation= keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.robust_PCA_objective","page":"Objectives","title":"ManoptExamples.robust_PCA_objective","text":"robust_PCA_objective(data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluton())\n\nGenerate the objective for the robust PCA task for some given data D and Huber regularization parameter ε.\n\nSee also\n\nRobustPCACost, RobustPCAGrad!!\n\nnote: Note\nSince the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the evaluation, indeed the gradient always allows for both the allocating and the inplace variant to be used, though that keyword is used to setup the objective.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#Rosenbrock","page":"Objectives","title":"Rosenbrock Function","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rosenbrock example  and The Difference of Convex Rosenbrock Example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/Rosenbrock.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RosenbrockCost","page":"Objectives","title":"ManoptExamples.RosenbrockCost","text":"RosenbrockCost\n\nProvide the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nf(mathcal M p) = a(p_1^2-p_2)^2 + (p_1-b)^2\n\nwhich means that for the 2D case, the manifold mathcal M is ignored.\n\nSee also 📖 Rosenbrock (with slightly different parameter naming).\n\nConstructor\n\nf = Rosenbrock(a,b)\n\ngenerates the struct/function of the Rosenbrock cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockGradient!!","page":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","text":"RosenbrockGradient\n\nProvide Eclidean GRadient fo the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nnabla f(mathcal M p) = beginpmatrix\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \n    -2a(p_1^2-p_2)\nendpmatrix\n\ni.e. also here the manifold is ignored.\n\nConstructor\n\nRosenbrockGradient(a,b)\n\nFunctors\n\ngrad_f!!(M,p)\ngrad_f!!(M, X, p)\n\nevaluate the gradient at p the manifoldmathcal M is ignored.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockMetric","page":"Objectives","title":"ManoptExamples.RosenbrockMetric","text":"RosenbrockMetric <: AbstractMetric\n\nA metric related to the Rosenbrock problem, where the metric at a point pmathbb R^2 is given by\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nwhere the mathrmRb stands for Rosenbrock\n\n\n\n\n\n","category":"type"},{"location":"objectives/#Base.exp-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","page":"Objectives","title":"Base.exp","text":"q = exp(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, q, p, X)\n\nCompute the exponential map with respect to the RosenbrockMetric.\n\n    q = beginpmatrix p_1 + X_1  p_2+X_2+X_1^2endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Base.log-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any}","page":"Objectives","title":"Base.log","text":"X = log(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, X, p, q)\n\nCompute the logarithmic map with respect to the RosenbrockMetric. The formula reads for any j  1m\n\nX = beginpmatrix\n  q_1 - p_1 \n  q_2 - p_2 + (q_1 - p_1)^2\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.inverse_local_metric-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.inverse_local_metric","text":"inverse_local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the inverse of the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG^-1_p =\nbeginpmatrix\n    1  2p_1\n    2p_1  1+4p_1^2 \nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.local_metric-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.local_metric","text":"local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.change_representer-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","page":"Objectives","title":"ManifoldsBase.change_representer","text":"Y = change_representer(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, Y, ::EuclideanMetric, p, X)\n\nGiven the Euclidan gradient X at p, this function computes the corresponting Riesz representer Ysuch that⟨X,Z⟩ = ⟨ Y, Z ⟩_{\\mathrm{Rb},p}holds for allZ, in other wordsY = G(p)^{-1}X`.\n\nthis function is used in riemannian_gradient to convert a Euclidean into a Riemannian gradient.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.inner-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","page":"Objectives","title":"ManifoldsBase.inner","text":"inner(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X, Y)\n\nCompute the inner product on mathbb R^2 with respect to the RosenbrockMetric, i.e. for XY in T_pmathcal M we have\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1\n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Rosenbrock_objective","page":"Objectives","title":"ManoptExamples.Rosenbrock_objective","text":"Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation())\n\nReturn the gradient objective of the Rosenbrock example.\n\nSee also RosenbrockCost, RosenbrockGradient!!\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","page":"Objectives","title":"ManoptExamples.minimizer","text":"minimizer(::RosenbrockCost)\n\nReturn the minimizer of the RosenbrockCost, which is given by\n\np^* = beginpmatrix bb^2 endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Literature","page":"Objectives","title":"Literature","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Pages = [\"objectives/index.md\"]\nCanonical=false","category":"page"},{"location":"examples/#List-of-Examples","page":"Overview","title":"List of Examples","text":"","category":"section"},{"location":"examples/","page":"Overview","title":"Overview","text":"Name provides Documentation Comment\nA Benchmark for Difference of Convex contains a few simple functions  \nSolving Rosenbrock with Difference of Convex DoC split of Rosenbrock 📚 uses a Rosenbrock based metric\nDifference of Convex vs. Frank-Wolfe   closed-form sub solver\nRiemannian Mean f, operatornamegradf (A/I), objective 📚 \nRobust PCA f, operatornamegradf (A/I), objective 📚 \nRosenbrock f, operatornamegradf (A/I), objective, minimizer 📚 \nThe Rayleigh Quotient f, operatornamegradf (A/I), operatornameHessf (A/I), objective 📚 ","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"Symbols:","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"A Allocating variant\nI In-place variant\n📚 link to documented functions in the documentation","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","page":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Introduction","page":"Rosenbrock Metric","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"This example illustrates how the 📖 Rosenbrock problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [BFSS23], Section 7.2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Both the Rosenbrock problem","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"    operatorname*argmin_xinmathbb R^2 abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where ab0 and usually b=1 and a gg b, we know the minimizer x^* = (bb^2)^mathrmT, and also the (Euclidean) gradient","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"nabla f(x) =\n  beginpmatrix\n  4a(x_1^2-x_2) -2a(x_1^2-x_2)\n  endpmatrix\n  +\n  beginpmatrix\n  2(x_1-b) 0\n  endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"They are even available already here in ManifoldExamples.jl, see RosenbrockCost and RosenbrockGradient!!.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Furthermore, the RosenbrockMetric can be used on mathbb R^2, that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"XY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e. using a gradient but not a Hessian.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"The Euclidean Gradient\nThe Riemannian gradient descent with respect to the RosenbrockMetric\nThe Euclidean Difference of Convex Algorithm\nThe Riemannian Difference of Convex Algorithm respect to the RosenbrockMetric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Where we obtain a difference of convex problem by writing","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f(x) = abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 - bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"g(x) = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 quadtext and quad h(x) = bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_manopt_parameter!\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"To emphasize the effect, we choose a quite large value of a.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"a = 2*10^5\nb = 1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and use the starting point and a direction to check gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"p0 = [0.1, 0.2]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Euclidean gradient we can just use the same approach as in the Rosenbrock example","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M = ℝ^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n∇f!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"define a common debug vector","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"debug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|δp|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and call the gradient descent algorithm","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Eucl_GD_state = gradient_descent(M, f, ∇f!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000000 F(x): 8.9937e-06 |δp|: 1.3835e+00 | |grad f|: 8.170355e-03\n# 20000000 F(x): 2.9474e-09 |δp|: 6.5764e-03 | |grad f|: 1.419191e-04\n# 30000000 F(x): 9.8376e-13 |δp|: 1.1918e-04 | |grad f|: 2.526295e-06\n# 40000000 F(x): 3.2830e-16 |δp|: 2.1773e-06 | |grad f|: 4.526313e-08\n# 50000000 F(x): 1.0154e-19 |δp|: 3.9803e-08 | |grad f|: 6.838240e-10\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 53073227 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 10000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Riemannian case, we define","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"MetricManifold(Euclidean(2; field = ℝ), ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and the gradient is now adopted to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_f!(M, X, p)\n    ∇f!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 1000000  F(x): 1.3571e-09 |δp|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |δp|: 3.6836e-05 | |grad f|: 9.240792e-09\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the convex case, we have to first introduce the two parts of the cost.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and their (Euclidan) gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function ∇h!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ∇h(M, p; a=100, b=1)\n    X = zero(p)\n    ∇h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ∇g!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ∇g(M, p; a=100, b=1)\n    X = zero(p)\n    ∇g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and we define for convenience","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_∇h!(M, X, p) = ∇h!(M, X, p; a=a, b=b)\ndocE_∇g!(M, X, p) = ∇g!(M, X, p; a=a, b=b)\nfunction docE_∇f!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_∇g!(M, X, p)\n  docE_∇h!(M, Y, p)\n  X .-= Y\n  return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we call the difference of convex algorithm on Eucldiean space mathbb R^2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"E_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_∇h!, p0;\n    gradient=docE_∇f!,\n    grad_g = docE_∇g!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000    F(x): 2.9705e-09 |δp|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |δp|: 1.2173e-04 | |grad f|: 4.541087e-08\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLineseach() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 10000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"We first have to again defined the gradients with respect to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_h!(M, X, p; a=100, b=1)\n    ∇h!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ∇g!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For X in partial h(p^(k)) the sunproblem top determine p^(k+1) reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatorname*argmin_pinmathcal M g(p) - langle X log_p^(k)prangle","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with X = operatornamegrad h(p^(k)) that","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"phi(p) = g(p) - langle X log_p^(k)prangle\n= abigl( p_1^2-p_2bigr)^2\n        + 2bigl(p_1-bbigr)^2 - 2(p^(k)_1-b)p_1 + 2(p^(k)_1-b)p^(k)_1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"its Euclidean gradient reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatornamegradphi(p) =\n    nabla varphi(p)\n    = beginpmatrix\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^(k)_1-b)\n        -2a(p_1^2-p_2)\n    endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where we can again employ the gradient conversion from before to obtain the Riemannian gradient.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"mutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (ϕ::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    ϕ(M, X, p)\n    return X\nend\nfunction (ϕ::SubGrad)(M, X, p)\n    X .= [\n        4 * ϕ.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - ϕ.b) - 2 * (ϕ.pk[1] - ϕ.b),\n        -2 * ϕ.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And in orer to update the subsolvers gradient correctly, we have to overwrite","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"set_manopt_parameter!(ϕ::SubGrad, ::Val{:p}, p) = (ϕ.pk .= p)\nset_manopt_parameter!(ϕ::SubGrad, ::Val{:X}, X) = (ϕ.Xk .= X)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we again introduce for ease of use","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLineseach() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","page":"Rosenbrock Metric","title":"Comparison in Iterations","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^8),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\")","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Literature","page":"Rosenbrock Metric","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Pages = [\"examples/Difference-of-Convex-Rosenbrock.md\"]\nCanonical=false","category":"page"},{"location":"examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","page":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Ronny BergmannLaura Weigl 2023-07-02","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For this example we first load the necessary packages.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Robust-PCA/#Computing-a-Robust-PCA","page":"Robust PCA","title":"Computing a Robust PCA","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For a given matrix D  ℝ^dn whose columns represent points in ℝ^d, a matrix p  ℝ^dm is computed for a given dimension m  n: p represents an ONB of ℝ^dm such that the column space of p approximates the points (columns of D), i.e. the vectors D_i as well as possible.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We compute p as a minimizer over the Grassmann manifold of the cost function:","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"beginsplit\nf(p)\n = frac1nsum_i=1^noperatornamedist(D_i operatornamespan(p))\n\n = frac1n sum_i=1^nlVert pp^TD_i - D_irVert\nendsplit","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The output cost represents the average distance achieved with the returned p, an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that f is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter ε.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f_ϵ(p) = frac1n sum_i=1^nℓ_ϵ(lVert pp^mathrmTD_i - D_irVert)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"where ℓ_ϵ(x) = sqrtx^2 + ϵ^2 - ϵ.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts).","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"First, we generate random data. For illustration purposes we take points in mathbb R^2 and m=1, that is we aim to find a robust regression line.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"n = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"M = Grassmann(d,m);","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For the initial matrix p_0 we use classical PCA via singular value decomposition. Thus, we use the first d left singular vectors.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Then, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in Manopt.jl.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Furthermore the cost and gradient are implemented in ManoptExamples.jl. Since these are Huber regularized, both functors have the ϵ as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection pp^T - I, which converts the Euclidean to the Riemannian gradient.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The trust-region method also requires the Hessian Matrix. By using ApproxHessianFiniteDifference using a finite difference scheme we get an approximation of the Hessian Matrix.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We run the procedure several times, where the smoothing parameter ε is reduced iteratively.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ε = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m]","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"2×1 Matrix{Float64}:\n -0.7494248652139397\n  0.6620893983436593","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Let’s generate the cost and gradient we aim to use here","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f = ManoptExamples.RobustPCACost(M, data, ε)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, ε)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([9.537606557855465 1.6583418797018163 … 30.833523701909474 30.512999245062304; -45.34339972619071 -1.7120433539256108 … -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"and check the initial cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f(M, p0)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.430690947905521","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Now we iterate the opimization with reducing ε after every iteration, which we update in f and grad_f.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"q = copy(M, p0)\nεi = ε\nfor i in 1:iterations\n    f.ε = εi\n    grad_f.ε = εi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global εi *= reduction\nend","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"When finally setting ε we can investigate the final cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f.ε = 0.0\nf(M, q)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.412973804873698","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Finally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"fig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Rosenbrock/#The-Rosenbrock-Function","page":"Rosenbrock","title":"The Rosenbrock Function","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Ronny Bergmann 2023-01-03","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"After loading the necessary packages","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Manifolds, Manopt, ManoptExamples\nusing Plots","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We fix the parameters for the 📖 Rosenbrock (where the wikipedia page has a slightly different parameter naming).","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"a = 100.0\nb = 1.0\np0 = [1/10, 2/10]","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which is defined on mathbb R^2, so we need","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"M = ℝ^2","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Euclidean(2; field = ℝ)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and can then generate both the cost and the gradient","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"ManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"For comparison, we look at the initial cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, p0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"4.42","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And to illustrate, we run two small solvers with their default settings as a comparison.","category":"page"},{"location":"examples/Rosenbrock/#Gradient-Descent","page":"Rosenbrock","title":"Gradient Descent","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We start with the gradient descent solver.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Since we need the state anyways to access the record, we also get from the return_state=true a short summary of the solver run.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"From the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.10562873187751265","category":"page"},{"location":"examples/Rosenbrock/#Quasi-Newton","page":"Rosenbrock","title":"Quasi Newton","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We can improve this using the quasi Newton algorithm","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 44 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 2), projections, and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector trnasport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(DefaultManifold(), 0.0001, 0.999) with keyword arguments\n  * retraction_method = ExponentialRetraction()\n  * vector_transport_method = ParallelTransport()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 1000: not reached\n    |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And we see it stops far earlier, after 45 Iterations. We again collect the recorded values","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"2.359559352025148e-14","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and see that the final value is close to the one of the minimizer","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, ManoptExamples.minimizer(f))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.0","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which we also see if we plot the recorded cost.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"fig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","page":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Ronny Bergmann 2023-07-02","category":"page"},{"location":"examples/Riemannian-mean/#Preliminary-Notes","page":"Riemannian Mean","title":"Preliminary Notes","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Each of the example objectives or problems stated in this package should be accompanied by a Quarto notebook that illustrates their usage, like this one.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For this first example, the objective is a very common one, for example also used in the Get Started: Optimize! tutorial of Manopt.jl.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"The second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"There are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in examples/. If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having ManoptExamples.jl in development mode. this requires that your example does not have any (additional) dependencies beyond the ones ManoptExamples.jl has anyways.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For registered versions of ManoptExamples.jl use the environment of examples/ and – under development – add ManoptExamples.jl in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the examples/ environment again.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Riemannian-mean/#Loading-packages-and-defining-data","page":"Riemannian Mean","title":"Loading packages and defining data","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Loading the necessary packages and defining a data set on a manifold","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nσ = π / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];","category":"page"},{"location":"examples/Riemannian-mean/#Variant-1:-Using-the-functions","page":"Riemannian Mean","title":"Variant 1: Using the functions","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"We can define both the cost and gradient, RiemannianMeanCost and RiemannianMeanGradient!!, respectively. For their mathematical derivation and further explanations, we again refer to Get Started: Optimize!.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"f = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Then we can for example directly call a gradient descent as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"x1 = gradient_descent(M, f, grad_f, first(data))","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794750908\n 0.00653160068349042\n 0.7267799820871861","category":"page"},{"location":"examples/Riemannian-mean/#Variant-2:-Using-the-objective","page":"Riemannian Mean","title":"Variant 2: Using the objective","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"A shorter way to directly obtain the Manifold objective including these two functions. Here, we want to specify that the objective can do inplace-evaluations using the evaluation=-keyword. The objective can be obtained calling Riemannian_mean_objective as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Together with a manifold, this forms a Manopt Problem, which would usually enable to switch manifolds between solver runs. Here we could for example switch to using Euclidean(3) instead for the same data the objective is build upon.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmp = DefaultManoptProblem(M, rmo)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"This enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s1 = GradientDescentState(M, copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794750908\n 0.00653160068349042\n 0.7267799820871861","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"but we can easily use a conjugate gradient instead","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s2 = ConjugateGradientDescentState(\n    M,\n    copy(M, first(data)),\n    StopAfterIteration(100),\n    ArmijoLinesearch(M),\n    FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868393613136017\n 0.006531541407458413\n 0.7267799052788726","category":"page"},{"location":"examples/RayleighQuotient/#The-Rayleigh-Quotient","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Ronny Bergmann 2024-03-09","category":"page"},{"location":"examples/RayleighQuotient/#Introduction","page":"The Rayleigh Quotient","title":"Introduction","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [Bou23] using the Rayleigh quotient and explores several different ways to use the algorithms from Manopt.jl.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For a symmetric matrix A in mathbb R^ntimes n we consider the 📖 Rayleigh Quotient","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"operatorname*argmin_x in mathbb R^n backslash 0\nfracx^mathrmTAxlVert x rVert^2","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"On the sphere we can omit the denominator and obtain","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f(p) = p^mathrmTApqquad p  𝕊^n-1","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"which by itself we can again continue in the embedding as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"tilde f(x) = x^mathrmTAxqquad x in mathbb R^n","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This cost has the nice feature that at the minimizer p^*inmathbb S^n-1 the function falue f(p^*) is the smalles eigenvalue of A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For the embedded function tilde f the gradient and Hessian can be computed with classical methods as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\ntilde f(x) = 2Ax qquad x  ℝ^n\n\n^2tilde f(x)V = 2AV qquad x V  ℝ^n\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Similarly, cf. Examples 3.62 and 5.27 of [Bou23], the Riemannian gradient and Hessian on the manifold mathcal M = mathbb S^n-1 are given by","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\noperatornamegrad f(p) = 2Ap - 2(p^mathrmTAp)*pqquad p  𝕊^n-1\n\noperatornameHess f(p)X =  2AX - 2(p^mathrmTAX)p - 2(p^mathrmTAp)Xqquad p  𝕊^n-1 X in T_p𝕊^n-1\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first generate an example martrx A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random\nRandom.seed!(42)\nn = 500\nA = Symmetric(randn(n,n))","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And the manifolds","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"M = Sphere(n-1)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Sphere(499, ℝ)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"E = get_embedding(M)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean(500; field = ℝ)","category":"page"},{"location":"examples/RayleighQuotient/#Setup-the-corresponding-functions","page":"The Rayleigh Quotient","title":"Setup the corresponding functions","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Since RayleighQuotientCost, RayleighQuotientGrad!!, and RayleighQuotientHess!! are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute f is called with M as their first argument and tilde f if called with E.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We instantiate","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f = ManoptExamples.RayleighQuotientCost(A)\ngrad_f = ManoptExamples.RayleighQuotientGrad!!(A)\nHess_f = ManoptExamples.RayleighQuotientHess!!(A)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"the suffix !! also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"p0 = [1.0, zeros(n-1)...]\nX = zero_vector(M, p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"we can both call","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Y = grad_f(M,p0)  # Allocates memory\ngrad_f(M,X,p0)    # Computes in place of X and returns the result in X.\nnorm(M, p0, X-Y)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"0.0","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Now we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"First of all let’s construct the actual result – since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"λ = min(eigvals(A)...)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"-44.838605046940486","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-on-gradient-information","page":"The Rayleigh Quotient","title":"A Solver based on gradient information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient nabla tilde f. We can “tell” the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally, Manopt.jl then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf. e.g. this tutorial section or Section 3.8 or more precisely Example 3.62 in [Bou23].","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"But instead of diving into all the tecnical details, we can just specify objective_type=:Euclidean to trigger the conversion. We start with a simple gradient descent","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"s = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n    return_state=true,\n)\nq1 = get_solver_result(s)\ns","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 50    f(x): -44.206244|grad f(p)|:2.387846624353276\n# 100   f(x): -44.546883|grad f(p)|:2.256125365459942\n# 150   f(x): -44.765220|grad f(p)|:1.3051578932969472\n# 200   f(x): -44.824730|grad f(p)|:0.5758153603739719\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), (:GradientNorm, \"|grad f(p)|:%s\"), \"\n\", 50]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"From the final cost we can already see that q1 is an eigenvector to the smallest eigenvalue we obtaines above.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And we can compare this to running with the Riemannian gradient, since the RayleighQuotientGrad!! returns this one as well, when just called with the sphere as first Argument, we just have to remove the objective_type.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q2 = gradient_descent(M, f, grad_f, p0;\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n)\n#Test that both are the same\nisapprox(M, q1,q2)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 50    f(x): -44.206244|grad f(p)|:2.387846624353294\n# 100   f(x): -44.546883|grad f(p)|:2.2561253654599915\n# 150   f(x): -44.765220|grad f(p)|:1.305157893296954\n# 200   f(x): -44.824730|grad f(p)|:0.5758153603739878\n\ntrue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We can also benchmark both","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 9 samples with 1 evaluation.\n Range (min … max):  570.377 ms … 600.167 ms  ┊ GC (min … max): 7.71% … 7.11%\n Time  (median):     580.657 ms               ┊ GC (median):    7.63%\n Time  (mean ± σ):   582.121 ms ±   8.723 ms  ┊ GC (mean ± σ):  7.64% ± 0.26%\n\n  █       █      █ █   █  █         ██                        █  \n  █▁▁▁▁▁▁▁█▁▁▁▁▁▁█▁█▁▁▁█▁▁█▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  570 ms           Histogram: frequency by time          600 ms <\n\n Memory estimate: 1.13 GiB, allocs estimate: 3852.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 51 samples with 1 evaluation.\n Range (min … max):  93.870 ms … 108.205 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     97.246 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   98.757 ms ±   3.496 ms  ┊ GC (mean ± σ):  1.21% ± 1.94%\n\n       ▁   █▄ ▄▄█▁   ▄            ▁   ▄  ▁                      \n  ▆▁▁▆▆█▆▆▆██▆████▆▁▁█▆▁▁▁▆▆▁▁▁▁▆▆█▁▁▆█▁▁█▁▁▆▁▁▆▁▁▁▁▁▆▆▁▁▁▁▁▁▆ ▁\n  93.9 ms         Histogram: frequency by time          107 ms <\n\n Memory estimate: 12.02 MiB, allocs estimate: 3246.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We see, that the conversion costs a bit of performance, but if the Euclidean gradient is easier to compute, this might still be ok.","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-(also)-on-(approximate)-Hessian-information","page":"The Rayleigh Quotient","title":"A Solver based (also) on (approximate) Hessian information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To also involve the Hessian, we consider the trust regions solver with three cases:","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean, approximating the Hessian\nEuclidean, providing the Hessian\nRiemannian, providing the Hessian but also using in-place evaluations.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -43.522431|grad f(p)|:9.779418323323288\n# 20    f(x): -44.838605|grad f(p)|:8.451144677175197e-12","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any struct is considered to (eventually) be the also optional starting point. For space reasons, let’s also shorten the debug print to only iterations 7 and 14.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q4 = trust_regions(M, f, grad_f, (E, p, X) -> Hess_f(E, p, X), p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -43.522431|grad f(p)|:9.779418323423874\n# 20    f(x): -44.838605|grad f(p)|:1.1638484603322704e-11","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q5 = trust_regions(M, f, grad_f, (M, Y, p, X) -> Hess_f(M, Y, p, X), p0;\n    evaluation=InplaceEvaluation(),\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -43.522431|grad f(p)|:9.779418323423881\n# 20    f(x): -44.838605|grad f(p)|:1.1445271377332597e-11","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s also here compare them in benchmarks. Let’s here compare all variants in their (more performant) in-place versions.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $p0;\n  objective_type=:Euclidean,\n  evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 3 samples with 1 evaluation.\n Range (min … max):  1.929 s …   1.971 s  ┊ GC (min … max): 9.24% … 9.07%\n Time  (median):     1.939 s              ┊ GC (median):    9.22%\n Time  (mean ± σ):   1.947 s ± 22.028 ms  ┊ GC (mean ± σ):  9.29% ± 0.25%\n\n  █            █                                          █  \n  █▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.93 s         Histogram: frequency by time        1.97 s <\n\n Memory estimate: 3.81 GiB, allocs estimate: 21954.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -> Hess_f(E, Y, p, X)), $p0;\n  evaluation=InplaceEvaluation(),\n  objective_type=:Euclidean\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 4 samples with 1 evaluation.\n Range (min … max):  1.331 s …   1.443 s  ┊ GC (min … max): 9.48% … 9.17%\n Time  (median):     1.342 s              ┊ GC (median):    9.33%\n Time  (mean ± σ):   1.365 s ± 52.784 ms  ┊ GC (mean ± σ):  9.25% ± 0.19%\n\n  ██       █                                              █  \n  ██▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.33 s         Histogram: frequency by time        1.44 s <\n\n Memory estimate: 2.56 GiB, allocs estimate: 19283.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -> Hess_f(M, Y, p, X)), $p0;\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 112 samples with 1 evaluation.\n Range (min … max):  40.403 ms … 68.382 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     43.043 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   44.695 ms ±  3.705 ms  ┊ GC (mean ± σ):  3.51% ± 4.37%\n\n         ▄▆█                                                   \n  ▅▃▁▅▁▄█████▆▇▃▃▃▄▁▃▁▄▄▁▃▄▅█▅▃█▄▄▃▄▁▅▁▃▁▁▁▁▁▃▃▁▁▁▁▁▁▁▁▁▁▁▃▃▃ ▃\n  40.4 ms         Histogram: frequency by time        54.3 ms <\n\n Memory estimate: 16.23 MiB, allocs estimate: 6802.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Of course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q1, q) for q ∈ [q2,q3] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 1.5505190490543112e-15\n 0.1301942153125503","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q3, q) for q ∈ [q4,q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 5.325642276241738e-14\n 5.1713218623657045e-14","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Which we can also see in the final cost, comparing it to the Eigenvalue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[f(M, q) - λ for q ∈ [q1, q2, q3, q4, q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"5-element Vector{Float64}:\n  0.013874911807278067\n  0.01387491180739886\n  6.679101716144942e-13\n -4.106937012693379e-12\n -3.964828465541359e-12","category":"page"},{"location":"examples/RayleighQuotient/#Summary","page":"The Rayleigh Quotient","title":"Summary","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute.","category":"page"},{"location":"examples/RayleighQuotient/#Literature","page":"The Rayleigh Quotient","title":"Literature","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Pages = [\"examples/RayleighQuotient.md\"]\nCanonical=false","category":"page"},{"location":"#Welcome-to-ManoptExample.jl","page":"Home","title":"Welcome to ManoptExample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ManoptExamples","category":"page"},{"location":"","page":"Home","title":"Home","text":"ManoptExamples.ManoptExamples","category":"page"},{"location":"#ManoptExamples.ManoptExamples","page":"Home","title":"ManoptExamples.ManoptExamples","text":"🏔️⛷️ ManoptExamples.jl – A collection of research and tutorial example problems for Manopt.jl\n\n📚 Documentation: juliamanifolds.github.io/ManoptExamples.jl\n📦 Repository: github.com/JuliaManifolds/ManoptExamples.jl\n💬 Discussions: github.com/JuliaManifolds/ManoptExamples.jl/discussions\n🎯 Issues: github.com/JuliaManifolds/ManoptExamples.jl/issues\n\n\n\n\n\n","category":"module"},{"location":"","page":"Home","title":"Home","text":"This package provides a set of example tasks for Manopt.jl based on either generic manifolds from the ManifoldsBase.jl interface or specific manifolds from Manifolds.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each example usually consists of","category":"page"},{"location":"","page":"Home","title":"Home","text":"a cost function and additional objects, like the gradient or proximal maps, see objectives\nan example explaining how to use these, see examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Helping functions that are used in one or more examples can be found in the section of functions in the menu.","category":"page"}]
}
