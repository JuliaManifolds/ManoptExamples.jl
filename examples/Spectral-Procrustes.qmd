---
title: "A comparison of the RCBM with the PBA, the SGM for solving the spectral Procrustes problem"
author: "Hajg Jasa"
date: 01/11/2023
---

## Introduction

In this example we compare the Riemannian Convex Bundle Method (RCBM) [BergmannHerzogJasa:2023](@cite)
with the Proximal Bundle Algorithm, which was introduced in [HoseiniMonjeziNobakhtianPouryayevali:2021:1](@cite), and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to solve the spectral Procrustes problem on $\mathrm{SO}(250)$.
This example reproduces the results from [BergmannHerzogJasa:2023](@cite), Section 5.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.

Pkg.develop(path="../") # a trick to work on the local dev version

export_table = false
benchmarking = false

experiment_name = "Spectral-Procrustes"
results_folder = joinpath(@__DIR__, experiment_name)
figures_folder = joinpath(@__DIR__, experiment_name, "figures")
!isdir(results_folder) && mkdir(results_folder)
!isdir(figures_folder) && mkdir(figures_folder)
```

```{julia}
#| output: false
using BenchmarkTools
using CSV, DataFrames
using ColorSchemes, Plots
using QuadraticModels, RipQP
using Random, LinearAlgebra, LRUCache
using ManifoldDiff, Manifolds, Manopt, ManoptExamples
```

## The Problem

Given two matrices $A, B \in \mathbb R^{n \times d}$ we aim to solve the orthogonal Procrustes problem
$$
	{\arg\min}_{p \in \mathrm{SO}(d)}\ \Vert A - B \, p \Vert_2
	,
$$
where $\mathrm{SO}(d)$ is equipped with the standard bi-invariant metric, and where $\Vert \,\cdot\, \Vert_2$ denotes the spectral norm of a matrix, \ie, its largest singular value.
We aim to find the best matrix $p \in \mathbb R^{d \times d}$ such that $p^\top p = \mathrm{id}$ is the identity matrix, or in other words $p$ is the best rotation.
Note that the spectral norm is convex in the Euclidean sense, but not geodesically convex on $\mathrm{SO}(d)$. 
If we define the objective as
$$
    f (p)
	= 
    \Vert A - B \, p \Vert_2
    ,
$$
its subdifferential is given by
$$
    \partial f(p) = \mathrm{proj}_p(-B^\top UV^\top)
$$
where $U$ and $V$ are some left and right singular vectors, respectively, corresponding to the largest singular value of $A - B \, p$, and $\mathrm{proj}_p$ is the projection onto
$$
	\mathcal T_p \mathrm{SO}(d)
	=
	\{
    A \in \mathbb R^{d,d} \, \vert \, pA^\top + Ap^\top = 0, \, \mathrm{trace}(p^{-1}A)=0
    \}
	.
$$

## Numerical Experiment

We initialize the experiment parameters, as well as some utility functions.
```{julia}
#| output: false
Random.seed!(42)
n = 1000
d = 250
A = rand(n, d)
B = randn(n, d)
tol = 1e-8
#
# Algorithm parameters
bundle_size = 15
max_iters = 5000
ϱ = 1e-2 # Curvature-dependent parameter for the convex bundle method
μ = 50.0 # Proximal parameter for the PBA
δ = 0.0 # Update parameter for μ
#
# Manifolds and data
M = SpecialOrthogonal(d)
p0 = rand(M)
```

We now define objective and subdifferential (first the Euclidean one, then the projected one).
```{julia}
#| output: false
f(M, p) = opnorm(A - B*p)
function ∂ₑf(M, p)
	cost_svd = svd(A - B*p)
	# Find all maxima in S – since S is sorted, these are the first n ones
	indices = [i for (i, v) in enumerate(cost_svd.S) if abs(v - cost_svd.S[1]) < eps()]
	ind = rand(indices)
	return -B'*(cost_svd.U[:,ind]*cost_svd.Vt[ind,:]')
end
rpb = Manifolds.RiemannianProjectionBackend(Manifolds.ExplicitEmbeddedBackend(M; gradient=∂ₑf))
∂f(M, p) = Manifolds.gradient(M, f, p, rpb)
```

We run the optimization algorithms...
```{julia}
#| output: false
p = proximal_bundle_method(
    M,
    f,
    ∂f,
    p0;
    δ=δ,
    μ=μ,
    count=[:Cost, :SubGradient],
    cache=(:LRU, [:Cost, :SubGradient], 50),
    stopping_criterion=StopWhenLagrangeMultiplierLess(tol)|StopAfterIteration(max_iters),
    debug=[
        :Iteration,
        :Stop,
        (:Cost, "F(p): %1.16f "),
        (:ν, "ν: %1.16f "),
        (:c, "c: %1.16f "),
        (:μ, "μ: %1.8f "),
        :Stop,
        :WarnBundle,
        1000,
        "\n",
    ],
    record=[:Iteration, :Cost, :Iterate],
    return_state=true,
    return_options=true,
    return_objective=true,
)
p_result = get_solver_result(p)
p_record = get_record(p)
#
b = convex_bundle_method(
    M,
    f,
    ∂f,
    p0;
    bundle_size=bundle_size,
    ϱ=ϱ,
    count=[:Cost, :SubGradient],
    cache=(:LRU, [:Cost, :SubGradient], 50),
    debug=[
        :Iteration,
        (:Cost, "F(p): %1.16f "),
        (:ξ, "ξ: %1.16f "),
        (:ϱ, "ϱ: %1.4f "),
        :WarnBundle,
        :Stop,
        1000,
        "\n",
    ],
    record=[:Iteration, :Cost, :Iterate],
    return_state=true,
    return_options=true,
    return_objective=true,
)
b_result = get_solver_result(b)
b_record = get_record(b)
#
s = subgradient_method(
    M,
    f,
    ∂f,
    p0;
    count=[:Cost, :SubGradient],
    cache=(:LRU, [:Cost, :SubGradient], 50),
    stepsize=DecreasingStepsize(1, 1, 0, 1, 0, :absolute),
    stopping_criterion=StopWhenSubgradientNormLess(√tol) | StopAfterIteration(max_iters),
    debug=[:Iteration, (:Cost, "F(p): %1.16f "), :Stop, 1000, "\n"],
    record=[:Iteration, :Cost, :Iterate],
    return_state=true,
    return_options=true,
)
s_result = get_solver_result(s)
s_record = get_record(s)
```

... And we benchmark their performance.
```{julia}
if benchmarking
    p_bm = @benchmark proximal_bundle_method(
        $M,
        $f,
        $∂f,
        $p0;
        δ=$δ,
        μ=$μ,
        cache=(:LRU, [:Cost, :SubGradient], 50),
        stopping_criterion=StopWhenLagrangeMultiplierLess($tol)|StopAfterIteration($max_iters),
    )
    b_bm = @benchmark convex_bundle_method(
        $M,
        $f,
        $∂f,
        $p0;
        bundle_size=$bundle_size,
        ϱ=$ϱ,
        cache=(:LRU, [:Cost, :SubGradient], 50),
    )
    s_bm = @benchmark subgradient_method(
        $M,
        $f,
        $∂f,
        $p0;
        count=[:Cost, :SubGradient],
        cache=(:LRU, [:Cost, :SubGradient], 50),
        stepsize=DecreasingStepsize(1, 1, 0, 1, 0, :absolute),
        stopping_criterion=StopWhenSubgradientNormLess(√$tol) | StopAfterIteration($max_iters),
    )
    #
    experiments = ["RCBM", "PBA", "SGM"]
    records = [b_record, p_record, s_record]
    results = [b_result, p_result, s_result]
    times = [
        median(b_bm).time * 1e-9,
        median(p_bm).time * 1e-9,
        median(s_bm).time * 1e-9,
    ]
    #
    # Finalize - export costs
    if export_table
        for (time, record, result, experiment) in zip(times, records, results, experiments)
            C1 = [0.5 f(M, p0)]
            C = cat(first.(record), [r[2] for r in record]; dims=2)
            CSV.write(
                joinpath(results_folder, experiment_name * "_" * experiment * "-result.csv"),
                DataFrame(vcat(C1, C), :auto);
                header=["i", "cost"],
            )
        end
        D = cat(
            experiments,
            [maximum(first.(record)) for record in records],
            [t for t in times],
            [minimum([r[2] for r in record]) for record in records];
            dims=2,
        )
        CSV.write(
            joinpath(results_folder, experiment_name * "-comparisons.csv"),
            DataFrame(D, :auto);
            header=["Algorithm", "Iterations", "Time (s)", "Objective"],
        )
    end
end
```
