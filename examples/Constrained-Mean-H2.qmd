---
title: "The Constrained mean on Hyperbolic space."
author: "Ronny Bergmann"
date: 27/03/2025
---

## Introduction

In this example we compare the Projected Gradient Algorithm (PGA) as introduced in [BergmannFerreiraNemethZhu:2025](@cite) with both the Augmented Lagrangian Method (ALM) and the Exact Penalty Method (EPM) [LiuBoumal:2019](@cite).

```{julia}
#| echo: false
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.
Pkg.develop(path="../") # a trick to work on the local dev version of ManoptExamples
ENV["GKSwstype"] = "100"
```

```{julia}
#| output: false
using Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random
```

Consider the constrained Riemannian center of mass
for a given set of points ``q_i \in \mathcal M$ $i=1,\ldots,N$
given by

```math
\operatorname*{arg\,min}_{p\in\mathcal C}
\sum_{i=1}^N d_{\mathrm{M}}^2(p,q_i)
```

constrained to a set $\mathcal C \subset \mathcal M$.

For this experiment set $\mathcal M = \mathbb H^2$ is the ``[Hyperbolic space](@extref Manifolds :std:doc:`manifolds/hyperbolic`)``{=commonmark}
and the constrained set $\mathcal C = C_{c,r}$ as the ball of radius $r$ around the center point $c$, where we choose here $r=1$ and $c = (0,0,1)^{\mathrm{T}}.

```{julia}
#| output: false
M = Hyperbolic(2)
c = Manifolds._hyperbolize(M, [0, 0])
radius = 1.0
# Sample the boundary
unit_circle = [
    exp(M, c, get_vector(M, c, radius .* [cos(α), sin(α)], DefaultOrthonormalBasis())) for
    α in 0:(2π / 720):(2π)
]
```

Our data consists of $N=200$ points, where we skew the data a bit to force the mean to be outside of the constrained set $\mathcal C$.

```{julia}
#| output: false
N = 200;
σ = 1.5
Random.seed!(42)
# N random points moved to top left to have a mean outside
data_pts = [
    exp(
        M,
        c,
        get_vector(
            M, c, σ .* randn(manifold_dimension(M)) .+ [2.5, 2.5], DefaultOrthonormalBasis()
        ),
    ) for _ in 1:N
]
```

```{julia}
#| echo: false
#| output: false
experiment_name = "C2Dball-mean-$(N)"
folder = (@__DIR__)*"/projected-gradient-results/"
write_csv = false;
(!isdir(folder) && write_csv) && mkpath(folder)
```
## Cost, gradient and projection

We can formulate the constrained problem above in two different forms.
Both share a cost and require a gradient. For performance reasons, we also provide a mutating variant of the gradient

```{julia}
#| output: false
f(M, p; pts=[op]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts);

grad_f(M, p; pts=[op]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts);

function grad_f!(M, X, p; pts=[op])
    zero_vector!(M, X, p)
    Y = zero_vector(M, p)
    for q in pts
        log!(M, Y, p, q)
        X .+= Y
    end
    X .*= -1 / length(pts)
    return X
end;
```

We can model the constrained either with an inequality constraint $g(p) \geq 0$ or using a projection onto the set. For the gradient of $g$ and the projection we again also provide mutating variants.

```{julia}
#| output: false
g(M, p) = distance(M, c, p)^2 - radius^2;
indicator_C(M, p) = (g(M, p) ≤ 0) ? 0 : Inf;

function project_C(M, p, r=radius)
    X = log(M, c, p)
    n = norm(M, c, X)
    q = (n > r) ? exp(M, c, (r / n) * X) : copy(M, p)
    return q
end;
function project_C!(M, q, p; X=zero_vector(M, c), r=radius)
    log!(M, X, c, p)
    n = norm(M, c, X)
    if (n > r)
        exp!(M, q, c, (r / n) * X)
    else
        copyto!(M, q, p)
    end
    return q
end;

grad_g(M, p) = -2 * log(M, p, c);
function grad_g!(M, X, p)
    log!(M, X, p, c)
    X .*= -2
    return X
end
```

## The mean

For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to $\mathcal C$. We can then
project this onto $\mathcal C$.
For the projected mean we obtain $g(p) = 0$ since the original mean is outside of the set, the projected one lies on the boundary.

```{julia}
mean_data = mean(M, data_pts)
mean_projected = project_C(M, mean_data)
g(M, mean_projected)
```

## The experiment

We first define the specific data cost functions

```{julia}
#| output: false
_f(M, p) = f(M, p; pts=data_pts)
_grad_f(M, p) = grad_f(M, p; pts=data_pts)
_grad_f!(M, X, p) = grad_f!(M, X, p; pts=data_pts)
```

and in a first run record a projected gradient method solver run

```{julia}
mean_pg = copy(M, c) # start at the center
@time pgms = projected_gradient_method!(
    M, _f, _grad_f!, project_C!, mean_pg;
    evaluation=InplaceEvaluation(),
    indicator=indicator_C,
    debug=[:Iteration, :Cost, " ", :GradientNorm, "\n", 1, :Stop],
    record=[:Iteration, :Iterate, :Cost, :Gradient],
    stopping_criterion=StopAfterIteration(150) |
                       StopWhenProjectedGradientStationary(M, 1e-7),
    return_state=true,
)
```

and similarly perform a first run of both the ``[augmented Lagrangian method](@extref `Manopt.augmented_Lagrangian_method`)``{=commonmark}
and the ``[exact penalty method](@extref `Manopt.exact_penalty_method`)``{=commonmark}

```{julia}
mean_alm = copy(M, c)
@time alms = augmented_Lagrangian_method!(
    M, _f, _grad_f!, mean_alm;
    evaluation=InplaceEvaluation(),
    g=[g], grad_g=[grad_g!],
    debug=[:Iteration, :Cost, " ", "\n", 10, :Stop],
    record=[:Iteration, :Iterate, :Cost],
    return_state=true,
)
```

```{julia}
mean_epm = copy(M, c)
@time epms = exact_penalty_method!(
    M, _f, _grad_f!, mean_epm;
    evaluation = InplaceEvaluation(),
    g = [g], grad_g = [grad_g!],
    debug = [:Iteration, :Cost, " ", "\n", 25, :Stop],
    record = [:Iteration, :Iterate, :Cost],
    return_state = true,
)
```

### Benchmark

After a first run we now Benchmark the three algorithms with [`Chairmarks.jl`](https://github.com/LilithHafner/Chairmarks.jl)

```{julia}
pg_b = @be projected_gradient_method!(
    $M, $_f, $_grad_f!, $project_C!, $(copy(M, c));
    evaluation=$(InplaceEvaluation()),
    indicator=$indicator_C,
    stopping_criterion=$(
        StopAfterIteration(150) | StopWhenProjectedGradientStationary(M, 1e-7)
    ),
) evals = 1 samples = 5 seconds = 100
```

```{julia}
alm_b = @be augmented_Lagrangian_method!(
    $M, $_f, $_grad_f!, $(copy(M, c));
    evaluation = $(InplaceEvaluation()),
    g = $([g]),
    grad_g = $([grad_g!]),
) evals = 1 samples = 5 seconds = 100
```

```{julia}
epm_b = @be exact_penalty_method!(
    $M, $_f, $_grad_f!, $(copy(M, c));
    evaluation = $(InplaceEvaluation()),
    g = $([g]),
    grad_g = $([grad_g!]),
) evals = 1 samples = 5 seconds = 100
```

## Plots & results

```{julia}
#| output: false
pb_x(data) = [convert(PoincareBallPoint, p).value[1] for p in data]
pb_y(data) = [convert(PoincareBallPoint, p).value[2] for p in data]
```

The results are

```{julia}
fig = Figure()
axis = Axis(fig[1, 1], title = "The ball constrained mean comparison", aspect = 1)
arc!(Point2f(0, 0), 1, -π, π; color = :black)
lines!(axis, pb_x(unit_circle), pb_y(unit_circle); label = L"δC")
scatter!(axis, pb_x(data_pts), pb_y(data_pts), label = L"d_i")
scatter!(axis, pb_x([mean_data]), pb_y([mean_data]), label = L"m")
scatter!(
    axis,
    pb_x([mean_projected]),
    pb_y([mean_projected]),
    label = L"m_{\text{proj}}",
)
scatter!(axis, pb_x([mean_alm]), pb_y([mean_alm]), label = L"m_{\text{alm}}")
scatter!(axis, pb_x([mean_epm]), pb_y([mean_epm]), label = L"m_{\text{epm}}")
scatter!(axis, pb_x([mean_pg]), pb_y([mean_pg]), label = L"m_{\text{pg}}")
axislegend(axis, position = :rt)
xlims!(axis, -1.02, 1.5)
ylims!(axis, -1.02, 1.5)
hidespines!(axis)
hidedecorations!(axis)
fig
```

```{julia}
min_cost = minimum(map(p -> _f(M, p), [mean_pg, mean_alm, mean_epm]))
```

```{julia}
fig2 = Figure()
axis2 = Axis(
    fig2[1, 1];
    title="Cost over iterations (log scale x)",
    xscale=log10,
    yscale=identity,
    xticks=[1, 10, 100],
)
lines!(
    axis2,
    get_record(pgms, :Iteration, :Iteration),
    get_record(pgms, :Iteration, :Cost);
    label="PG",
)
lines!(
    axis2,
    get_record(alms, :Iteration, :Iteration),
    get_record(alms, :Iteration, :Cost);
    label="ALM",
)
lines!(
    axis2,
    get_record(epms, :Iteration, :Iteration),
    get_record(epms, :Iteration, :Cost);
    label="EPM",
)
axislegend(axis2; position=:rb)
#ylims!(axis2, min_cost-0.001,)
axis2.xlabel = "Iterations (log scale)"
axis2.ylabel = "Cost f"
fig2
```

```{julia}
#| echo: false
#| output: false
if write_csv
    #
    #
    # All points from the H2 plot
    CSV.write("$(folder)$(experiment_name)-pts.csv",
        DataFrame(; x=pb_x(data_pts), y=pb_y(data_pts)))
    CSV.write("$(folder)$(experiment_name)-constraint.csv",
        DataFrame(; x=pb_x(unit_circle), y=pb_y(unit_circle)),
    )
    CSV.write("$(folder)$(experiment_name)-mean.csv",
        DataFrame(; x=pb_x([mean_data]), y=pb_y([mean_data]))
    )
    CSV.write("$(folder)$(experiment_name)-proj.csv",
        DataFrame(; x=pb_x([mean_projected]), y=pb_y([mean_projected])),
    )
    CSV.write("$(folder)$(experiment_name)-alm.csv",
        DataFrame(; x=pb_x([mean_alm]), y=pb_y([mean_alm]))
    )
    CSV.write("$(folder)$(experiment_name)-epm.csv",
        DataFrame(; x=pb_x([mean_epm]), y=pb_y([mean_epm]))
    )
    CSV.write("$(folder)$(experiment_name)-pg.csv",
        DataFrame(; x=pb_x([mean_pg]), y=pb_y([mean_pg]))
    )
    #
    #
    # Benchmark export
    CSV.write(
        "$(folder)$(experiment_name)-benchmark.csv",
        DataFrame(;
            names=["ALM", "EPM", "PG"],
            maxiter=[
                length(get_record(alms, :Iteration, :Cost)),
                length(get_record(epms, :Iteration, :Cost)),
                length(get_record(pgms, :Iteration, :Cost)),
            ],
            times=[mean(alm_b).time, mean(epm_b).time, mean(pg_b).time],
        ),
    )
    #
    # Cost exports
    #
    CSV.write(
        "$(folder)$(experiment_name)-cost-alm.csv",
        DataFrame(;
            i=get_record(alms, :Iteration, :Iteration),
            c=get_record(alms, :Iteration, :Cost),
        ),
    )
    CSV.write(
        "$(folder)$(experiment_name)-cost-epm.csv",
        DataFrame(;
            i=get_record(epms, :Iteration, :Iteration),
            c=get_record(epms, :Iteration, :Cost),
        ),
    )
    CSV.write(
        "$(folder)$(experiment_name)-cost-pg.csv",
        DataFrame(;
            i=get_record(pgms, :Iteration, :Iteration),
            c=get_record(pgms, :Iteration, :Cost),
        ),
    )
end
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["Constrained-Mean-H2.md"]
Canonical=false
```
````

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
#| code-summary: "Package versions"
#| echo: false
using Pkg
Pkg.status()
```

```{julia}
#| echo: false
#| output: asis
using Dates
println("This tutorial was last rendered $(Dates.format(now(), "U d, Y, H:M:S")).");
```