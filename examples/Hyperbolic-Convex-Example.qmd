---
title: "A Geodesically Convex Example on Hyperbolic Space"
author: "Hajg Jasa"
date: 04/14/2025
engine: julia
---

## Introduction

In this example we take a look at how the choice in the Convex Riemannian Proximal Gradient (CRPG) method [BergmannJasaJohnPfeffer:2025:2](@cite) influences its performance on Hadamard manifolds.
In particular, we will look at convergence rates of the function values, as well as the rate of convergence of the iterates.
This example reproduces the results from [BergmannJasaJohnPfeffer:2025:2](@cite), Section 5.2.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.

Pkg.develop(path="../") # a trick to work on the local dev version

export_orig = true
export_table = true
export_result = true
benchmarking = true

experiment_name = ""
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)
```

```{julia}
#| output: false
using PrettyTables
using BenchmarkTools
using CSV, DataFrames
using ColorSchemes, Plots
using Random, LinearAlgebra, LRUCache
using ManifoldDiff, Manifolds, Manopt, ManoptExamples
```

## The Problem

Let $\mathcal M = \mathcal H^2$ be the $2$-dimensional hyperbolic space.

Let $q_1 \in \mathcal M$ and $g \colon \mathcal M \to \mathbb R$ be defined by

$$
g(p) = \mathrm{dist}(p, q_1),
$$

Observe that the function $g$ is geodesically convex with respect to the Riemannian metric on $\mathcal M$.

Let now $q_2 \neq q_1$ be a given point, and let $h \colon \mathcal M \to \mathbb R$ be defined by

$$
h(p) = \tau \mathrm{dist}(p, q_2),
$$ 

for some $\tau > 0$.
We define our total objective function as $f = g + h$.
Notice that this objective function is also geodesically convex with respect to the Riemannian metric on $\mathcal M$.
The goal is to find the minimizer of $f$ on $\mathcal M$, which is an interpolation between the two points $q_1$ and $q_2$, depending on the value of $\tau$.
Namely, if $\tau < 1$, the minimizer is $q_1$; if $\tau > 1$, the minimizer is $q_2$; and if $\tau = 1$, the minimizer is the geodesic segment between $q_1$ and $q_2$.


## Numerical Experiment

We initialize the experiment parameters, as well as some utility functions.
```{julia}
#| output: false
Random.seed!(42)
experiment_name = "Hyperbolic-Convex-Example"
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)

atol = 1e-8
max_iters = 5000
τ = 3/2 # weight for the component h
```

```{julia}
#| output: false
# Define the manifold
M = Hyperbolic(2)
# Lower bound on the sectional curvature of the manifold
k_min = -1.0
# Initialize the first point
q1 = rand(M)
# Initialize the second point
X1 = rand(M; vector_at=q1)
X1 /= norm(M, q1, X1)
q2 = exp(M, q1, 2X1)
r = distance(M, q1, q2)
# Initialize the starting point for the algorithm
# can be random
X2 = rand(M; vector_at=q2)
X2 /=norm(M, q2, X2)
p0 = exp(M, q2, 2X2)
# Closed-form solution
γ(t) = geodesic(M, q1, log(M, q1, q2), t)
p_star = τ > 1 ? q2 : (τ < 1 ? q1 : γ(rand()))
# Diameter of the set containing all points
D = 2maximum([distance(M, p0, a) for a in [q1, q2, p_star]])
```

We can now define the functions we need for the optimization
```{julia}
#| output: false
# Distance to q1
g(M, p) = distance(M, p, q1)
grad_g(M, p) = ManifoldDiff.grad_distance(M, q1, p, 1)
# Distance to q2
h(M, p) = τ * distance(M, p, q2)
prox_h(M, λ, p) = ManifoldDiff.prox_distance(M, τ * λ, q2, p, 1)
# Total objective function
f(M, p) = g(M, p) + h(M, p)
# Curvature function
function ζ_1(k_min, dist)
    (k_min < zero(k_min)) && return sqrt(-k_min) * dist * coth(sqrt(-k_min) * dist)
    (k_min ≥ zero(k_min)) && return one(k_min)
end
# Smallest and largest eigenvalues of the Hessian of g at the minimum of f
λ_min = 1/g(M, p_star)
λ_max = coth(g(M, p_star))
λ_r = 2*g(M, p_star)/(g(M, p_star)*coth(g(M, p_star)) + 1)
ζ_D = ζ_1(k_min, D)
λ_D = 1/ζ_D
```

We now take a look at how the inequality in [Theorem 1](@cite) behaves in practice.
```{julia}
#| output: false
#| code-fold: true
# Function to calculate the inequality value
function calculate_inequality_value(λ, gradient_norm, k_min, λ_max, λ_min)
    if λ == 0 || gradient_norm == 0 || k_min == 0
        return 0.0
    end
    
    argument = λ * gradient_norm * √(-k_min)
    
    # For small arguments, use Taylor expansion to avoid precision issues
    sinh_term = if abs(argument) < 1e-12
        1.0
    else
        sinh(argument) / argument
    end
    
    max_term = max(abs(1 - λ_max * λ), abs(1 - λ_min * λ))
    
    return sinh_term * max_term
end
```

```{julia}
#| code-fold: true
begin
    # Generate data points for the plot
    λ_values = range(1e-6, max(g(M, p_star)/τ, 1/λ_min, 2/λ_max) + 1e-6, length=200)

    inequality_values = [calculate_inequality_value(λ, 1.0, k_min, λ_max, λ_min) for λ in λ_values]
    
    # Plot the inequality
    plot(
        λ_values, 
        inequality_values, 
        label="Inequality Value",
        xlabel="λ",
        ylabel="Value",
        # title="Inequality Analysis",
        lw=2,
        legend=:topright,
        size=(800, 500),
        grid=true,
        framestyle=:box
    )
    
    # Add a horizontal line at y = 1 to show the boundary
    hline!([1.0], label="Inequality Boundary", ls=:dash, lw=2, c=:red)
    
    # Add shaded region for where inequality is satisfied
    satisfied_indices = findall(v -> v < 1.0, inequality_values)
    if !isempty(satisfied_indices)
        first_idx = satisfied_indices[1]
        last_idx = satisfied_indices[end]
        if last_idx - first_idx > 0  # Ensure there's actually a region to shade
            plot!(λ_values[first_idx:last_idx], inequality_values[first_idx:last_idx], 
                  fillrange=1, fillalpha=0.2, c=:green, label="Satisfied Region")
        end
    end
    
    # Add specific vertical lines
    if λ_max > 0
        vline!([1/λ_max], label="λ = 1/λ_{max}", ls=:dot, lw=1.5, c=:blue)
		vline!([2/λ_max], label="λ = 2/λ_{max}", ls=:dot, lw=1.5, c=:black)
    end
    if λ_min > 0
        vline!([1/λ_min], label="λ = 1/λ_{min}", ls=:dot, lw=1.5, c=:purple)
    end
	vline!([λ_r], label="λ = 2r/(rcoth(r)+1)", ls=:dot, lw=1.5, c=:green)
	vline!([r/τ], label="λ = r/τ", ls=:dot, lw=1.5, c=:orange)
	vline!([1/ζ_D], label="λ = 1/ζ_D", ls=:dot, lw=1.5, c=:red)
    
    # Enforce y-axis limits 
    plot!(ylim=(0, max(1.5, maximum(inequality_values) * 1.1)))
end
```

### Constant Stepsize
We can now run the optimization, first using a constant stepsize $\lambda = 1/\zeta_{1, k_min}(D)$.
```{julia}
pg = proximal_gradient_method(
	M, 
	f, 
	g, 
	grad_g, 
	prox_h, 
	p0;
	stepsize=ConstantLength(λ_D),
	record=[:Iteration, :Cost, :Iterate],
	return_state=true,
	stopping_criterion=StopWhenGradientMappingNormLess(atol),
)
```

Let's first check that all iterates are within the set of points we are interested in...
```{julia}
# Extract recorded values
pg_result = get_solver_result(pg)
pg_record = get_record(pg)
all(x -> x == true, [distance(M, p0, q[3]) ≤ D for q in pg_record])
```
... and that the final iterate is approximately the closed-form solution
```{julia}
pg_result ≈ p_star
```

We now plot the decay of the function values
```{julia}
#| code-fold: true
# Calculate the minimum cost for relative error
min_cost_pg = minimum(record[2] for record in pg_record)
iterations = [record[1] for record in pg_record]
relative_errors_pg = [max(record[2] - min_cost_pg, eps()) for record in pg_record]
# Get initial error for scaling reference lines
initial_error_pg = relative_errors_pg[1]
# Create reference trajectories
ref_rate_1_pg = [initial_error_pg/k for k in iterations]
ref_rate_2_pg = [initial_error_pg/k^2 for k in iterations]
ref_rate_2k_pg = [initial_error_pg/2^k for k in iterations]
# Create the convergence plot
convergence_plot_pg = plot(
    iterations,
    relative_errors_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="f(pₖ) - f*",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
# Add reference lines
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2_pg;
    linestyle=:dot,
    linewidth=1.5,
    color=:blue,
    label="O(1/k²)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2^k)"
)
```

And the convergence rate of the iterates
```{julia}
#| code-fold: true
distances_pg = [max(distance(M, p_star, record[3]), eps()) for record in pg_record]
# Get initial error for scaling reference lines
initial_distance_pg = distances_pg[1]
# Create reference trajectories
dist_ref_rate_1_pg = [initial_distance_pg/k for k in iterations]
dist_ref_rate_2_pg = [initial_distance_pg/k^2 for k in iterations]
# Create the convergence plot
distances_plot_pg = plot(
    iterations,
    distances_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="dist(pₖ, p*)",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_2_pg;
    linestyle=:dot,
    linewidth=1.5,
    color=:blue,
    label="O(1/k²)"
)
```

### Backtracked Stepsize
We now run the optimization using a backtracked stepsize.
```{julia}
pg = proximal_gradient_method(
	M, 
	f, 
	g, 
	grad_g, 
	prox_h, 
	p0;
	stepsize=ProxGradBacktracking(; strategy=:convex, initial_stepsize=1/λ_max),
	record=[:Iteration, :Cost, :Iterate],
	return_state=true,
	stopping_criterion=StopWhenGradientMappingNormLess(atol),
)
```

Let's first check that all iterates are within the set of points we are interested in...
```{julia}
# Extract recorded values
pg_result = get_solver_result(pg)
pg_record = get_record(pg)
all(x -> x == true, [distance(M, p0, q[3]) ≤ D for q in pg_record])
```
... and that the final iterate is approximately the closed-form solution
```{julia}
pg_result ≈ p_star
```

We now plot the decay of the function values
```{julia}
#| code-fold: true
# Calculate the minimum cost for relative error
min_cost_pg = minimum(record[2] for record in pg_record)
iterations = [record[1] for record in pg_record]
relative_errors_pg = [max(record[2] - min_cost_pg, eps()) for record in pg_record]
# Get initial error for scaling reference lines
initial_error_pg = relative_errors_pg[1]
# Create reference trajectories
ref_rate_1_pg = [initial_error_pg/k for k in iterations]
ref_rate_2_pg = [initial_error_pg/k^2 for k in iterations]
ref_rate_2k_pg = [initial_error_pg/2^k for k in iterations]
# Create the convergence plot
convergence_plot_pg = plot(
    iterations,
    relative_errors_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="f(pₖ) - f*",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
# Add reference lines
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2_pg;
    linestyle=:dot,
    linewidth=1.5,
    color=:blue,
    label="O(1/k²)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2^k)"
)
```

And the convergence rate of the iterates
```{julia}
#| code-fold: true
distances_pg = [max(distance(M, p_star, record[3]), eps()) for record in pg_record]
# Get initial error for scaling reference lines
initial_distance_pg = distances_pg[1]
# Create reference trajectories
dist_ref_rate_1_pg = [initial_distance_pg/k for k in iterations]
dist_ref_rate_2_pg = [initial_distance_pg/k^2 for k in iterations]
# Create the convergence plot
distances_plot_pg = plot(
    iterations,
    distances_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="dist(pₖ, p*)",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_2_pg;
    linestyle=:dot,
    linewidth=1.5,
    color=:blue,
    label="O(1/k²)"
)
```

<!-- We introduce some keyword arguments for the solvers we will use in this experiment
```{julia}
#| output: false
pgm_kwargs = [
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
    :stepsize => ProxGradBacktracking(; strategy=:convex, initial_stepsize=initial_stepsize),
    :stopping_criterion => StopWhenAny(
        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)
    ),
]
pgm_bm_kwargs = [
    :stopping_criterion => StopWhenAny(
        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)
    ), 
]

cppa_kwargs(M) = [
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
    :stopping_criterion => StopWhenAny(
        StopAfterIteration(max_iters), StopWhenChangeLess(M, atol)
    ),
]
cppa_bm_kwargs(M) = [
    :stopping_criterion => StopWhenAny(
        StopAfterIteration(max_iters), StopWhenChangeLess(M, atol)
    ),
]
```

Before running the experiments, we initialize data collection functions that we will use later
```{julia}
#| output: false
global col_names_1 = [
    :Dimension,
    :Iterations_1,
    :Time_1,
    :Objective_1,
    :Iterations_2,
    :Time_2,
    :Objective_2,
]
col_types_1 = [
    Int64,
    Int64,
    Float64,
    Float64,
    Int64,
    Float64,
    Float64,
]
named_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)
global col_names_2 = [
    :Dimension,
    :Iterations,
    :Time,
    :Objective,
]
col_types_2 = [
    Int64,
    Int64,
    Float64,
    Float64,
]
named_tuple_2 = (; zip(col_names_2, type[] for type in col_types_2 )...)
function initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1, named_tuple_2)
    A1 = DataFrame(named_tuple_1)
    CSV.write(
        joinpath(
            results_folder,
            experiment_name * "_$subexperiment_name" * "-Comparisons.csv",
        ),
        A1;
        header=false,
    )
    # A2 = DataFrame(named_tuple_2)
    # CSV.write(
    #     joinpath(
    #         results_folder,
    #         experiment_name * "_$subexperiment_name" * "-Comparisons-Subgrad.csv",
    #     ),
    #     A2;
    #     header=false,
    # )
    return A1#, A2
end
```

```{julia}
#| output: false
function export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1, col_names_2)
    B1 = DataFrame(;
        Dimension=manifold_dimension(M),
        Iterations_1=maximum(first.(records[1])),
        Time_1=times[1],
        Objective_1=minimum([r[2] for r in records[1]]),
        Iterations_2=maximum(first.(records[2])),
        Time_2=times[2],
        Objective_2=minimum([r[2] for r in records[2]]),
    )
    # B2 = DataFrame(;
    #     Dimension=manifold_dimension(M),
    #     Iterations=maximum(first.(records[3])),
    #     Time=times[3],
    #     Objective=minimum([r[2] for r in records[3]]),
    # )
    return B1#, B2
end
function write_dataframes(
    B1, 
    # B2, 
    results_folder, 
    experiment_name, 
    subexperiment_name
)
    CSV.write(
        joinpath(
            results_folder,
            experiment_name *
            "_$subexperiment_name" *
            "-Comparisons.csv",
            # -Convex-Prox.csv",
        ),
        B1;
        append=true,
    )
    # CSV.write(
    #     joinpath(
    #         results_folder,
    #         experiment_name *
    #         "_$subexperiment_name" *
    #         "-Comparisons-Subgrad.csv",
    #     ),
    #     B2;
    #     append=true,
    # )
end
```

## On Hyperbolic Space
```{julia}
#| output: false
subexperiment_name = "Hn"
global A1 = initialize_dataframes(
    results_folder,
    experiment_name,
    subexperiment_name,
    named_tuple_1,
    named_tuple_2
)

for n in hn_dims

    Random.seed!(random_seed)

    M = Hyperbolic(Int(2^n))
    data = [rand(M) for _ in 1:N]
    q = rand(M) # we can artificially craft a point for the median component, i.e. h
    p0 = rand(M) #data[minimum(Tuple(findmax(dists)[2]))]

    g_hn(M, p) = g(M, p, data)
    # h_hn(M, p) = h(M, p, q)
    grad_g_hn(M, p) = grad_g(M, p, data)
    proxes_f_hn = proxes_f(data, q)
    prox_h_hn(M, λ, p) = prox_h(M, λ, p, q)
    f_hn(M, p) = f(M, p, data, q)

    # Optimization
    pgm = proximal_gradient_method(M, f_hn, g_hn, grad_g_hn, prox_h_hn, p0; pgm_kwargs...)
    pgm_result = get_solver_result(pgm)
    pgm_record = get_record(pgm)

    cppa = cyclic_proximal_point(M, f_hn, proxes_f_hn, p0; cppa_kwargs(M)...)
    cppa_result = get_solver_result(cppa)
    cppa_record = get_record(cppa)

    records = [
        pgm_record,
        cppa_record,
    ]

    if benchmarking
        pgm_bm = @benchmark proximal_gradient_method($M, $f_hn, $g_hn, $grad_g_hn, $prox_h_hn, $p0; $pgm_bm_kwargs...)
        cppa_bm = @benchmark cyclic_proximal_point($M, $f_hn, $proxes_f_hn, $p0; cppa_bm_kwargs($M)...)
        
        times = [
            median(pgm_bm).time * 1e-9,
            median(cppa_bm).time * 1e-9,
        ]

        B1 = export_dataframes(
            M,
            records,
            times,
            results_folder,
            experiment_name,
            subexperiment_name,
            col_names_1,
            col_names_2,
        )

        append!(A1, B1)
        # append!(A2, B2)
        (export_table) && (write_dataframes(B1, results_folder, experiment_name, subexperiment_name))
    end
end
```

We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the CRPG, while columns 5 to 7 refer to the CPPA... -->

<!-- ```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A1, tf = tf_markdown, header=col_names_1)
``` -->
<!-- ... Whereas the following table refers to the SGM
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A2, tf = tf_markdown, header=col_names_2)
``` -->

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
using Pkg
Pkg.status()
```
```{julia}
#| code-fold: true
#| echo: false
#| output: asis
using Dates
println("This tutorial was last rendered $(Dates.format(now(), "U d, Y, H:M:S")).");
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["Hadamard-CRPG.md"]
Canonical=false
```
````