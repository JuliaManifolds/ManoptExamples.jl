---
title: "A comparison of the RCBM with the PBA and the SGM for the Riemannian median"
author: "Hajg Jasa"
date: 06/27/2024
engine: julia
---

## Introduction

In this example we compare the Riemannian Convex Bundle Method (RCBM) [BergmannHerzogJasa:2024](@cite)
with the Proximal Bundle Algorithm, which was introduced in [HoseiniMonjeziNobakhtianPouryayevali:2021](@cite), and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to find the Riemannian median.
This example reproduces the results from [BergmannHerzogJasa:2024](@cite), Section 5.
The runtimes reported in the tables are measured in seconds.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.
Pkg.develop(path="../") # a trick to work on the local dev version
Pkg.develop(path="../../Manopt.jl") # a trick to work on the local dev version
benchmarking = true
export_table = true
plot = true
```

```{julia}
#| output: false
using PrettyTables
using BenchmarkTools
using CSV, DataFrames
using ColorSchemes, Plots
using QuadraticModels, RipQP
using LinearAlgebra, LRUCache, Random
using ManifoldDiff, Manifolds, Manopt, ManoptExamples
```

Let $\mathcal M$ be a Hadamard manifold and $\{q_1,\ldots,q_N\} \in \mathcal M$ denote $N = 1000$
Gaussian random data points.
Let $f \colon \mathcal M \to \mathbb R$ be defined by

```math
f(p) = \sum_{j = 1}^N w_j \, \mathrm{dist}(p, q_j),
```

where $w_j$, $j = 1, \ldots, N$ are positive weights such that $\sum_{j = 1}^N w_j = 1$.

The Riemannian geometric median $p^*$ of the dataset

```math
\mathcal D = \{
    q_1,\ldots,q_N \, \vert \, q_j \in \mathcal M\text{ for all } j = 1,\ldots,N
\}
```

is then defined as

```math
    p^* \coloneqq \operatorname*{arg\,min}_{p \in \mathcal M} f(p),
```

where equality is justified since $p^*$ is uniquely determined on Hadamard manifolds. In our experiments, we choose the weights $w_j = \frac{1}{N}$.

We initialize the experiment parameters, as well as utility functions.

```{julia}
#| output: false
experiment_name = "RCBM-Median"
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)
seed_argument = 57

atol = 1e-8
N = 1000 # number of data points
spd_dims = [2, 5, 10, 15]
hn_sn_dims = [1, 2, 5, 10]#, 15]

# Generate a point that is at most `tol` close to the point `p` on `M`
function close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))
    X = rand(M; vector_at = p)
    X .= tol * rand() * X / norm(M, p, X)
    return retract(M, p, X, retraction_method)
end
```

```{julia}
#| output: false
# Objective and subdifferential
f(M, p, data) = sum(1 / length(data) * distance.(Ref(M), Ref(p), data))
domf(M, p, centroid, diameter) = distance(M, p, centroid) < diameter / 2 ? true : false
function ∂f(M, p, data, atol=atol)
    return sum(
        1 / length(data) *
        ManifoldDiff.subgrad_distance.(Ref(M), data, Ref(p), 1; atol=atol),
    )
end
```

```{julia}
#| output: false
maxiter = 100000
rcbm_kwargs(contraction_factor, diameter, domf, k_max, k_min, retraction_method=ExponentialRetraction()) = [
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :count => [:Cost, :SubGradient],
    :domain => domf,
    :debug => [
        :Iteration,
        (:Cost, "F(p): %1.16f "),
        (:ξ, "ξ: %1.8f "),
        (:last_stepsize, "step size: %1.8f"),
        :Stop,
        1000,
        "\n",
    ],
    :diameter => diameter,
    :k_max => k_max,
    :k_min => k_min,
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
    :stepsize => Manopt.DomainBackTracking(contraction_factor=contraction_factor, retraction_method=retraction_method),
    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(maxiter),
]
rcbm_bm_kwargs(contraction_factor, diameter, domf, k_max, k_min, retraction_method=ExponentialRetraction()) = [
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :diameter => diameter,
    :domain => domf,
    :k_max => k_max,
    :k_min => k_min,
    :stepsize => Manopt.DomainBackTracking(contraction_factor=contraction_factor, retraction_method=retraction_method),
    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(maxiter),
]
pba_kwargs(ε) = [
    :ε => ε,
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :count => [:Cost, :SubGradient],
    :debug => [
        :Iteration,
        :Stop,
        (:Cost, "F(p): %1.16f "),
        (:ν, "ν: %1.16f "),
        (:c, "c: %1.16f "),
        (:μ, "μ: %1.8f "),
        :Stop,
        1000,
        "\n",
    ],
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(maxiter),
]
pba_bm_kwargs(ε) = [:ε => ε, :cache => (:LRU, [:Cost, :SubGradient], 50),]
sgm_kwargs = [
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :count => [:Cost, :SubGradient],
    :debug => [:Iteration, (:Cost, "F(p): %1.16f "), :Stop, 1000, "\n"],
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),
    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(maxiter),
]
sgm_bm_kwargs = [
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),
    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(maxiter),
]
```

Before running the experiments, we initialize data collection functions that we will use later
```{julia}
#| output: false
global col_names_1 = [
    :Dimension,
    :Iterations_1,
    :Time_1,
    :Objective_1,
    :Iterations_2,
    :Time_2,
    :Objective_2,
]
col_types_1 = [
    Int64,
    Int64,
    Float64,
    Float64,
    Int64,
    Float64,
    Float64,
]
named_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)
global col_names_2 = [
    :Dimension,
    :Iterations,
    :Time,
    :Objective,
]
col_types_2 = [
    Int64,
    Int64,
    Float64,
    Float64,
]
named_tuple_2 = (; zip(col_names_2, type[] for type in col_types_2 )...)
function initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1, named_tuple_2)
    A1 = DataFrame(named_tuple_1)
    CSV.write(
        joinpath(
            results_folder,
            experiment_name * "_$subexperiment_name" * "-Comparisons-Convex-Prox.csv",
        ),
        A1;
        header=false,
    )
    A2 = DataFrame(named_tuple_2)
    CSV.write(
        joinpath(
            results_folder,
            experiment_name * "_$subexperiment_name" * "-Comparisons-Subgrad.csv",
        ),
        A2;
        header=false,
    )
    return A1, A2
end
```

```{julia}
#| output: false
function export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1, col_names_2)
    B1 = DataFrame(;
        Dimension=manifold_dimension(M),
        Iterations_1=maximum(first.(records[1])),
        Time_1=times[1],
        Objective_1=minimum([r[2] for r in records[1]]),
        Iterations_2=maximum(first.(records[2])),
        Time_2=times[2],
        Objective_2=minimum([r[2] for r in records[2]]),
    )
    B2 = DataFrame(;
        Dimension=manifold_dimension(M),
        Iterations=maximum(first.(records[3])),
        Time=times[3],
        Objective=minimum([r[2] for r in records[3]]),
    )
    return B1, B2
end
function write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name)
    CSV.write(
        joinpath(
            results_folder,
            experiment_name *
            "_$subexperiment_name" *
            "-Comparisons-Convex-Prox.csv",
        ),
        B1;
        append=true,
    )
    CSV.write(
        joinpath(
            results_folder,
            experiment_name *
            "_$subexperiment_name" *
            "-Comparisons-Subgrad.csv",
        ),
        B2;
        append=true,
    )
end
```

## The Median on the Hyperboloid Model
```{julia}
#| output: false
subexperiment_name = "Hn"
k_max_hn = -1.0
k_min_hn = -1.0
contraction_factor = 0.975
ε_hn = 1e2

global A1, A2 = initialize_dataframes(
    results_folder,
    experiment_name,
    subexperiment_name,
    named_tuple_1,
    named_tuple_2
)

for n in hn_sn_dims
    Random.seed!(seed_argument)

    M = Hyperbolic(Int(2^n))
    data_hn = [rand(M) for _ in 1:N]
    dists = [distance(M, z, y) for z in data_hn, y in data_hn]
    diameter_hn = 2 * maximum(dists)
    p0 = data_hn[minimum(Tuple(findmax(dists)[2]))]

    f_hn(M, p) = f(M, p, data_hn)
    domf_hn(M, p) = domf(M, p, p0, diameter_hn)
    ∂f_hn(M, p) = ∂f(M, p, data_hn, atol)

    # Optimization
    rcbm = convex_bundle_method(M, f_hn, ∂f_hn, p0; rcbm_kwargs(contraction_factor, diameter_hn, domf_hn, k_max_hn, k_min_hn)...)
    rcbm_result = get_solver_result(rcbm)
    rcbm_record = get_record(rcbm)

    pba = proximal_bundle_method(M, f_hn, ∂f_hn, p0; pba_kwargs(ε_hn)...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    sgm = subgradient_method(M, f_hn, ∂f_hn, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        rcbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        rcbm_bm = @benchmark convex_bundle_method($M, $f_hn, $∂f_hn, $p0; rcbm_bm_kwargs($contraction_factor, $diameter_hn, $domf_hn, $k_max_hn, $k_min_hn)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_hn, $∂f_hn, $p0; pba_bm_kwargs($ε_hn)...)
        sgm_bm = @benchmark subgradient_method($M, $f_hn, $∂f_hn, $p0; $sgm_bm_kwargs...)
        
        times = [
            median(rcbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]

        B1, B2 = export_dataframes(
            M,
            records,
            times,
            results_folder,
            experiment_name,
            subexperiment_name,
            col_names_1,
            col_names_2,
        )

        append!(A1, B1)
        append!(A2, B2)
        (export_table) && (write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A1, tf = tf_markdown, header=col_names_1)
```
... Whereas the following table refers to the SGM
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A2, tf = tf_markdown, header=col_names_2)
```


## The Median on the Symmetric Positive Definite Matrix Space
```{julia}
#| output: false
subexperiment_name = "SPD"
k_max_spd = 0.0
k_min_spd = -1/2
contraction_factor = 0.975
ε_spd = 1e2

global A1_SPD, A2_SPD = initialize_dataframes(
    results_folder,
    experiment_name,
    subexperiment_name,
    named_tuple_1,
    named_tuple_2
)

for n in spd_dims
    Random.seed!(seed_argument)

    M = SymmetricPositiveDefinite(Int(n))
    data_spd = [rand(M) for _ in 1:N]
    dists = [distance(M, z, y) for z in data_spd, y in data_spd]
    diameter_spd = 2 * maximum(dists)
    p0 = data_spd[minimum(Tuple(findmax(dists)[2]))]
    
    f_spd(M, p) = f(M, p, data_spd)
    domf_spd(M, p) = domf(M, p, p0, diameter_spd)
    ∂f_spd(M, p) = ∂f(M, p, data_spd, atol)

    # Optimization
    rcbm = convex_bundle_method(M, f_spd, ∂f_spd, p0; rcbm_kwargs(contraction_factor, diameter_spd, domf_spd, k_max_spd, k_min_spd)...)
    rcbm_result = get_solver_result(rcbm)
    rcbm_record = get_record(rcbm)

    pba = proximal_bundle_method(M, f_spd, ∂f_spd, p0; pba_kwargs(ε_spd)...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    sgm = subgradient_method(M, f_spd, ∂f_spd, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        rcbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        rcbm_bm = @benchmark convex_bundle_method($M, $f_spd, $∂f_spd, $p0; rcbm_bm_kwargs($contraction_factor, $diameter_spd, $domf_spd, $k_max_spd, $k_min_spd)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_spd, $∂f_spd, $p0; pba_bm_kwargs($ε_spd)...)
        sgm_bm = @benchmark subgradient_method($M, $f_spd, $∂f_spd, $p0; $sgm_bm_kwargs...)

        times = [
            median(rcbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]

        B1_SPD, B2_SPD = export_dataframes(
            M,
            records,
            times,
            results_folder,
            experiment_name,
            subexperiment_name,
            col_names_1,
            col_names_2,
        )

        append!(A1_SPD, B1_SPD)
        append!(A2_SPD, B2_SPD)
        (export_table) && (write_dataframes(B1_SPD, B2_SPD, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A1_SPD, tf = tf_markdown, header=col_names_1)
```
... Whereas the following table refers to the SGM
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A2_SPD, tf = tf_markdown, header=col_names_2)
```

## The Median on the Sphere

For the last experiment, note that a major difference here is that the sphere has constant positive sectional curvature equal to $1$. In this case, we lose the global convexity of the Riemannian distance and thus of the objective. Minimizers still exist, but they may, in general, be non-unique.

```{julia}
#| output: false
subexperiment_name = "Sn"
k_max_sn = 1.0
k_min_sn = 1.0
contraction_factor = 0.65
diameter_sn = π / 3
ε_sn = 1e-2

global A1_Sn, A2_Sn = initialize_dataframes(
    results_folder,
    experiment_name,
    subexperiment_name,
    named_tuple_1,
    named_tuple_2
)

for n in hn_sn_dims
    Random.seed!(seed_argument)

    M = Sphere(Int(2^n))
    north = [0.0 for _ in 1:manifold_dimension(M)]
    push!(north, 1.0)
    data_sn = [close_point(M, north, diameter_sn / 2)]
    distance(M, data_sn[1], north) < diameter_sn / 2 ? pop!(data_sn) : nothing
    while length(data_sn) < N
        q = close_point(M, north, diameter_sn / 2)
        distance(M, q, north) < diameter_sn / 2 ? push!(data_sn, q) : nothing 
    end
    dists = [distance(M, z, y) for z in data_sn, y in data_sn]
    p0 = data_sn[minimum(Tuple(findmax(dists)[2]))]

    f_sn(M, p) = f(M, p, data_sn)
    domf_sn(M, p) = domf(M, p, north, diameter_sn)
    ∂f_sn(M, p) = ∂f(M, p, data_sn, atol)

    # Optimization
    rcbm = convex_bundle_method(M, f_sn, ∂f_sn, p0; rcbm_kwargs(contraction_factor, diameter_sn, domf_sn, k_max_sn, k_min_sn)...)
    rcbm_result = get_solver_result(rcbm)
    rcbm_record = get_record(rcbm)

    pba = proximal_bundle_method(M, f_sn, ∂f_sn, p0; pba_kwargs(ε_sn)...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    sgm = subgradient_method(M, f_sn, ∂f_sn, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        rcbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        rcbm_bm = @benchmark convex_bundle_method($M, $f_sn, $∂f_sn, $p0; rcbm_bm_kwargs($contraction_factor, $diameter_sn, $domf_sn, $k_max_sn, $k_min_sn)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $∂f_sn, $p0; pba_bm_kwargs($ε_sn)...)
        sgm_bm = @benchmark subgradient_method($M, $f_sn, $∂f_sn, $p0; $sgm_bm_kwargs...)

        times = [
            median(rcbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]

        B1_Sn, B2_Sn = export_dataframes(
            M,
            records,
            times,
            results_folder,
            experiment_name,
            subexperiment_name,
            col_names_1,
            col_names_2,
        )

        append!(A1_Sn, B1_Sn)
        append!(A2_Sn, B2_Sn)
        (export_table) && (write_dataframes(B1_Sn, B2_Sn, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A1_Sn, tf = tf_markdown, header=col_names_1)
```
... Whereas the following table refers to the SGM
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A2_Sn, tf = tf_markdown, header=col_names_2)
```

We now run the same experiment, but with the [ProjectionRetraction](@ref) instead of the exponential map.

```{julia}
#| output: false
subexperiment_name = "Sn_projection"
k_max_sn = 1.0
k_min_sn = 1.0
contraction_factor = 0.65
diameter_sn = π / 3
ε_sn = 1e-2

global A1_Sn_proj, A2_Sn_proj = initialize_dataframes(
    results_folder,
    experiment_name,
    subexperiment_name,
    named_tuple_1,
    named_tuple_2
)

for n in hn_sn_dims
    Random.seed!(seed_argument)

    M = Sphere(Int(2^n))
    north = [0.0 for _ in 1:manifold_dimension(M)]
    push!(north, 1.0)
    data_sn = [close_point(M, north, diameter_sn / 2;
        retraction_method=ProjectionRetraction())]
    distance(M, data_sn[1], north) < diameter_sn / 2 ? pop!(data_sn) : nothing
    while length(data_sn) < N
        q = close_point(M, north, diameter_sn / 2; 
            retraction_method=ProjectionRetraction())
        distance(M, q, north) < diameter_sn / 2 ? push!(data_sn, q) : nothing 
    end
    dists = [distance(M, z, y) for z in data_sn, y in data_sn]
    p0 = data_sn[minimum(Tuple(findmax(dists)[2]))]

    f_sn(M, p) = f(M, p, data_sn)
    domf_sn(M, p) = domf(M, p, north, diameter_sn)
    ∂f_sn(M, p) = ∂f(M, p, data_sn, atol)

    # Optimization
    rcbm = convex_bundle_method(M, f_sn, ∂f_sn, p0; 
        retraction_method=ProjectionRetraction(),
        inverse_retraction_method=ProjectionInverseRetraction(),
        rcbm_kwargs(contraction_factor, diameter_sn, domf_sn, k_max_sn, k_min_sn, ProjectionRetraction())...)
    rcbm_result = get_solver_result(rcbm)
    rcbm_record = get_record(rcbm)

    pba = proximal_bundle_method(M, f_sn, ∂f_sn, p0;
        retraction_method=ProjectionRetraction(),
        inverse_retraction_method=ProjectionInverseRetraction(),
        pba_kwargs(ε_sn)...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    sgm = subgradient_method(M, f_sn, ∂f_sn, p0; 
        retraction_method=ProjectionRetraction(),
        inverse_retraction_method=ProjectionInverseRetraction(),
        sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        rcbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        rcbm_bm = @benchmark convex_bundle_method($M, $f_sn, $∂f_sn, $p0; 
            retraction_method=$ProjectionRetraction(),
            inverse_retraction_method=$ProjectionInverseRetraction(),
            rcbm_bm_kwargs($contraction_factor, $diameter_sn, $domf_sn, $k_max_sn, $k_min_sn, $ProjectionRetraction())...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $∂f_sn, $p0; 
            retraction_method=$ProjectionRetraction(),
            inverse_retraction_method=$ProjectionInverseRetraction(),
            pba_bm_kwargs($ε_sn)...)
        sgm_bm = @benchmark subgradient_method($M, $f_sn, $∂f_sn, $p0; 
            retraction_method=$ProjectionRetraction(),
            inverse_retraction_method=$ProjectionInverseRetraction(),
            $sgm_bm_kwargs...)

        times = [
            median(rcbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]

        B1_Sn_proj, B2_Sn_proj = export_dataframes(
            M,
            records,
            times,
            results_folder,
            experiment_name,
            subexperiment_name,
            col_names_1,
            col_names_2,
        )

        append!(A1_Sn_proj, B1_Sn_proj)
        append!(A2_Sn_proj, B2_Sn_proj)
        (export_table) && (write_dataframes(B1_Sn_proj, B2_Sn_proj, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A1_Sn_proj, tf = tf_markdown, header=col_names_1)
```
... Whereas the following table refers to the SGM
```{julia}
#| echo: false
#| code-fold: true
benchmarking && pretty_table(A2_Sn_proj, tf = tf_markdown, header=col_names_2)
```

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
using Pkg
Pkg.status()
```
```{julia}
#| code-fold: true
using Dates
now()
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["RCBM-Median.md"]
Canonical=false
```
````