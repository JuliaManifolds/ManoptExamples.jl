---
title: "A comparison of the RCBM with the PBA and the SGM for the Riemannian median"
author: "Hajg Jasa"
date: 20/10/2023
---

## Introduction

In this example we compare the Riemannian Convex Bundle Method (RCBM) [BergmannHerzogJasa:2024](@cite)
with the Proximal Bundle Algorithm, which was introduced in [HoseiniMonjeziNobakhtianPouryayevali:2021:1](@cite), and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to find the Riemannian median.
This example reproduces the results from [BergmannHerzogJasa:2024](@cite), Section 5.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.
Pkg.develop(path="../") # a trick to work on the local dev version
benchmarking = true
export_table = true
plot = true
```

```{julia}
#| output: false
using BenchmarkTools
using CSV, DataFrames
using ColorSchemes, Plots
using QuadraticModels, RipQP
using Random, LinearAlgebra, LRUCache
using ManifoldDiff, Manifolds, Manopt, ManoptExamples
```

Let $\mathcal M$ be a Hadamard manifold and $\{q_1,\ldots,q_N\} \in \mathcal M$ denote $N = 1000$
Gaussian random data points.
Let $f \colon \mathcal M \to \mathbb R$ be defined by

```math
f(p) = \sum_{j = 1}^N w_j \, \mathrm{dist}(p, q_j),
```

where $w_j$, $j = 1, \ldots, N$ are positive weights such that $\sum_{j = 1}^N w_j = 1$.

The Riemannian geometric median $p^*$ of the dataset

```math
\mathcal D = \{
    q_1,\ldots,q_N \, \vert \, q_j \in \mathcal M\text{ for all } j = 1,\ldots,N
\}
```

is then defined as

```math
    p^* \coloneqq \operatorname*{arg\,min}_{p \in \mathcal M} f(p),
```

where equality is justified since $p^*$ is uniquely determined on Hadamard manifolds. In our experiments, we choose the weights $w_j = \frac{1}{N}$.

We initialize the experiment parameters, as well as utility functions.

```{julia}
#| output: false
Random.seed!(33)
experiment_name = "RCBM-Median"
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)

atol = 1e-8
N = 1000 # number of data points
spd_dims = [2, 5, 10, 15]
hn_sn_dims = [1, 2, 5, 10, 15]

# Generate a point that is at least `tol` close to the point `p` on `M`
function close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))
    X = rand(M; vector_at = p)
    X .= tol * rand() * X / norm(M, p, X)
    return retract(M, p, X, retraction_method)
end
```

```{julia}
#| output: false
# Objective and subdifferential
f(M, p, data) = sum(1 / length(data) * distance.(Ref(M), Ref(p), data))
domf(M, p, p0, diameter) = distance(M, p, p0) < diameter / 2 ? true : false
function ∂f(M, p, data, atol=atol)
    return sum(
        1 / length(data) *
        ManifoldDiff.subgrad_distance.(Ref(M), data, Ref(p), 1; atol=atol),
    )
end
```

```{julia}
#| output: false
cbm_kwargs(diameter, domf, k_max) = [
    :diameter => diameter,
    :domain => domf,
    :k_max => k_max,
    :count => [:Cost, :SubGradient],
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :debug => [
        :Iteration,
        (:Cost, "F(p): %1.16f "),
        (:ξ, "ξ: %1.8f "),
        (:last_stepsize, "step size: %1.8f"),
        :Stop,
        1000,
        "\n",
    ],
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
]
cbm_bm_kwargs(diameter, domf, k_max) = [
    :diameter => diameter,
    :domain => domf,
    :k_max => k_max,
    :cache => (:LRU, [:Cost, :SubGradient], 50),
]
pba_kwargs = [
    :count => [:Cost, :SubGradient],
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :debug => [
        :Iteration,
        :Stop,
        (:Cost, "F(p): %1.16f "),
        (:ν, "ν: %1.16f "),
        (:c, "c: %1.16f "),
        (:μ, "μ: %1.8f "),
        :Stop,
        1000,
        "\n",
    ],
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
]
pba_bm_kwargs = [:cache => (:LRU, [:Cost, :SubGradient], 50),]
sgm_kwargs = [
    :count => [:Cost, :SubGradient],
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),
    :stopping_criterion => StopWhenSubgradientNormLess(1e-4) | StopAfterIteration(5000),
    :debug => [:Iteration, (:Cost, "F(p): %1.16f "), :Stop, 1000, "\n"],
    :record => [:Iteration, :Cost, :Iterate],
    :return_state => true,
]
sgm_bm_kwargs = [
    :cache => (:LRU, [:Cost, :SubGradient], 50),
    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),
    :stopping_criterion => StopWhenSubgradientNormLess(1e-4) | StopAfterIteration(5000),
]
```

Before running the experiments, we initialize data collection functions that we will use later
```{julia}
#| output: false
function initialize_dataframes(results_folder, experiment_name, subexperiment_name)
    A1 = DataFrame(;
            a="Dimension",
            b="Iterations",
            c="Time (s)",
            d="Objective",
            e="Iterations",
            f="Time (s)",
            g="Objective",
        )
    CSV.write(
        joinpath(
            results_folder,
            experiment_name * "_$subexperiment_name" * "-Comparisons-Convex-Prox.csv",
        ),
        A1;
        header=false,
    )
    A2 = DataFrame(;
        a="Dimension",
        b="Iterations",
        c="Time (s)",
        d="Objective",
    )
    CSV.write(
        joinpath(
            results_folder,
            experiment_name * "_$subexperiment_name" * "-Comparisons-Subgrad.csv",
        ),
        A2;
        header=false,
    )
end
```

```{julia}
#| output: false
function write_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name)
    B1 = DataFrame(;
        a=manifold_dimension(M),
        b=maximum(first.(records[1])),
        c=times[1],
        d=minimum([r[2] for r in records[1]]),
        e=maximum(first.(records[2])),
        f=times[2],
        g=minimum([r[2] for r in records[2]]),
    )
    CSV.write(
        joinpath(
            results_folder,
            experiment_name *
            "_$subexperiment_name" *
            "-Comparisons-Convex-Prox.csv",
        ),
        B1;
        append=true,
    )
    B2 = DataFrame(;
        a=manifold_dimension(M),
        b=maximum(first.(records[3])),
        c=times[3],
        d=minimum([r[2] for r in records[3]]),
    )
    CSV.write(
        joinpath(
            results_folder,
            experiment_name *
            "_$subexperiment_name" *
            "-Comparisons-Subgrad.csv",
        ),
        B2;
        append=true,
    )
end
```

## The Median on the Hyperboloid Model
```{julia}
#| output: false
subexperiment_name = "Hn"
k_max_hn = 0.0
diameter_hn = floatmax(Float64)
initialize_dataframes(results_folder, experiment_name, subexperiment_name)
for n in hn_sn_dims

    M = Hyperbolic(Int(2^n))
    data = [rand(M) for _ in 1:N]
    dists = [distance(M, z, y) for z in data, y in data]
    p0 = data[minimum(Tuple(findmax(dists)[2]))]

    f_hn(M, p) = f(M, p, data)
    domf_hn(M, p) = domf(M, p, p0, diameter_hn)
    ∂f_hn(M, p) = ∂f(M, p, data, atol)

    # Optimization
    pba = proximal_bundle_method(M, f_hn, ∂f_hn, p0; pba_kwargs...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    cbm = convex_bundle_method(M, f_hn, ∂f_hn, p0; cbm_kwargs(diameter_hn, domf_hn, k_max_hn)...)
    cbm_result = get_solver_result(cbm)
    cbm_record = get_record(cbm)

    sgm = subgradient_method(M, f_hn, ∂f_hn, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        cbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        cbm_bm = @benchmark convex_bundle_method($M, $f_hn, $∂f_hn, $p0; cbm_bm_kwargs($diameter_hn, $domf_hn, $k_max_hn)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_hn, $∂f_hn, $p0; $pba_bm_kwargs...)
        sgm_bm = @benchmark subgradient_method($M, $f_hn, $∂f_hn, $p0; $sgm_bm_kwargs...)
        times = [
            median(cbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]
        (export_table) && (write_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Convex-Prox.csv"), DataFrame; delim = ","), tf = tf_markdown)
```
... Whereas the following table refers to the SGM
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Subgrad.csv"), DataFrame; delim = ","), tf = tf_markdown)
```


## The Median on the Symmetric Positive Definite Matrix Space
```{julia}
#| output: false
subexperiment_name = "SPD"
k_max_spd = 0.0
diameter_spd = floatmax(Float64)
initialize_dataframes(results_folder, experiment_name, subexperiment_name)
for n in spd_dims

    M = SymmetricPositiveDefinite(Int(2^n))
    data = [rand(M) for _ in 1:N]
    dists = [distance(M, z, y) for z in data, y in data]
    p0 = data[minimum(Tuple(findmax(dists)[2]))]

    f_spd(M, p) = f(M, p, data)
    domf_spd(M, p) = domf(M, p, p0, diameter_spd)
    ∂f_spd(M, p) = ∂f(M, p, data, atol)

    # Optimization
    pba = proximal_bundle_method(M, f_spd, ∂f_spd, p0; pba_kwargs...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    cbm = convex_bundle_method(M, f_spd, ∂f_spd, p0; cbm_kwargs(diameter_spd, domf_spd, k_max_spd)...)
    cbm_result = get_solver_result(cbm)
    cbm_record = get_record(cbm)

    sgm = subgradient_method(M, f_spd, ∂f_spd, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        cbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        cbm_bm = @benchmark convex_bundle_method($M, $f_spd, $∂f_spd, $p0; cbm_bm_kwargs($diameter_spd, $domf_spd, $k_max_spd)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_spd, $∂f_spd, $p0; $pba_bm_kwargs...)
        sgm_bm = @benchmark subgradient_method($M, $f_spd, $∂f_spd, $p0; $sgm_bm_kwargs...)
        times = [
            median(cbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]
        (export_table) && (write_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name)) 
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Convex-Prox.csv"), DataFrame; delim = ","), tf = tf_markdown)
```
... Whereas the following table refers to the SGM
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Subgrad.csv"), DataFrame; delim = ","), tf = tf_markdown)
```

## The Median on the Sphere
```{julia}
#| output: false
subexperiment_name = "Sn"
k_max_sn = 1.0
diameter_sn = π / 4
initialize_dataframes(results_folder, experiment_name, subexperiment_name)
for n in hn_sn_dims

    M = Sphere(Int(2^n))
    north = [0.0 for _ in 1:manifold_dimension(M)]
    push!(north, 1.0)
    diameter = π / 4 #2 * π/7
    data = [close_point(M, north, diameter / 2) for _ in 1:n]
    p0 = data[1]

    f_sn(M, p) = f_spd(M, p_spd, data)
    domf_sn(M, p) = domf(M, p, p0, diameter_sn)
    ∂f_sn(M, p) = ∂f(M, p, data, atol)

    # Optimization
    pba = proximal_bundle_method(M, f_sn, ∂f_sn, p0; pba_kwargs...)
    pba_result = get_solver_result(pba)
    pba_record = get_record(pba)

    cbm = convex_bundle_method(M, f_sn, ∂f_sn, p0; cbm_kwargs(diameter_sn, domf_sn, k_max_sn)...)
    cbm_result = get_solver_result(cbm)
    cbm_record = get_record(cbm)

    sgm = subgradient_method(M, f_sn, ∂f_sn, p0; sgm_kwargs...)
    sgm_result = get_solver_result(sgm)
    sgm_record = get_record(sgm)

    records = [
        cbm_record,
        pba_record,
        sgm_record,
    ]

    if benchmarking
        cbm_bm = @benchmark convex_bundle_method($M, $f_sn, $∂f_sn, $p0; cbm_bm_kwargs($diameter_sn, $domf_sn, $k_max_sn)...)
        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $∂f_sn, $p0; $pba_bm_kwargs...)
        sgm_bm = @benchmark subgradient_method($M, $f_sn, $∂f_sn, $p0; $sgm_bm_kwargs...)
        times = [
            median(cbm_bm).time * 1e-9,
            median(pba_bm).time * 1e-9,
            median(sgm_bm).time * 1e-9,
        ]
        (export_table) && (write_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name))
    end
end
```
We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA...
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Convex-Prox.csv"), DataFrame; delim = ","), tf = tf_markdown)
```
... Whereas the following table refers to the SGM
```{julia}
#| code-fold: true
export_table && pretty_table(CSV.read(joinpath(experiment_name, experiment_name * "_" * subexperiment_name * "-Comparisons-Subgrad.csv"), DataFrame; delim = ","), tf = tf_markdown)
```