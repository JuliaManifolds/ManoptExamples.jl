---
title: "Proximal Gradient Method: A Geodesically Convex Example on Hyperbolic Space"
author: "Hajg Jasa"
date: 04/14/2025
engine: julia
---

## Introduction

In this example we take a look at how the choice in the Convex Riemannian Proximal Gradient (CRPG) method [BergmannJasaJohnPfeffer:2025:2](@cite) influences its performance on Hadamard manifolds.
In particular, we will look at convergence rates of the function values, as well as the rate of convergence of the iterates.
This example reproduces the results from [BergmannJasaJohnPfeffer:2025:2](@cite), Section 5.2.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.

Pkg.develop(path="../") # a trick to work on the local dev version

export_orig = true
export_table = true
export_result = true
benchmarking = true

experiment_name = ""
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)
```

```{julia}
#| output: false
using PrettyTables
using BenchmarkTools
using CSV, DataFrames
using ColorSchemes, Plots
using Random, LinearAlgebra, LRUCache
using ManifoldDiff, Manifolds, Manopt, ManoptExamples
```

## The Problem

Let $\mathcal M = \mathcal H^2$ be the $2$-dimensional hyperbolic space.

Let $q_1 \in \mathcal M$ and $g \colon \mathcal M \to \mathbb R$ be defined by

```math
g(p) = \mathrm{dist}(p, q_1),
```

Observe that the function $g$ is geodesically convex with respect to the Riemannian metric on $\mathcal M$.

Let now $q_2 \neq q_1$ be a given point, and let $h \colon \mathcal M \to \mathbb R$ be defined by

```math
h(p) = \tau \mathrm{dist}(p, q_2),
```

for some $\tau > 0$.
We define our total objective function as $f = g + h$.
Notice that this objective function is also geodesically convex with respect to the Riemannian metric on $\mathcal M$.
The goal is to find the minimizer of $f$ on $\mathcal M$, which is an interpolation between the two points $q_1$ and $q_2$, depending on the value of $\tau$.
Namely, if $\tau < 1$, the minimizer is $q_1$; if $\tau > 1$, the minimizer is $q_2$; and if $\tau = 1$, the minimizer is the geodesic segment between $q_1$ and $q_2$.


## Numerical Experiment

We initialize the experiment parameters, as well as some utility functions.
```{julia}
#| output: false
Random.seed!(42)
experiment_name = "CRPG-Convex-Stepsize"
results_folder = joinpath(@__DIR__, experiment_name)
!isdir(results_folder) && mkdir(results_folder)

tol = 1e-7
max_iters = 5000
τ = 3/2 # weight for the component h
```

```{julia}
#| output: false
# Define the manifold
M = Hyperbolic(2)
# Lower bound on the sectional curvature of the manifold
k_min = -1.0
# Initialize the first point
q1 = rand(M)
# Initialize the second point
X1 = rand(M; vector_at=q1)
X1 /= norm(M, q1, X1)
q2 = exp(M, q1, 4X1)
r = distance(M, q1, q2)
# Initialize the starting point for the algorithm
# can be random
X2 = rand(M; vector_at=q2)
X2 /=norm(M, q2, X2)
p0 = exp(M, q2, X2)
# Closed-form solution
γ(t) = geodesic(M, q1, log(M, q1, q2), t)
p_star = τ > 1 ? q2 : (τ < 1 ? q1 : γ(rand()))
# Estimate of the diameter of the set containing all points
# D = 2maximum([distance(M, p0, a) for a in [q1, q2, p_star]])
# Diameter of the set containing p0 and p_star
D = 4 #2 * distance(M, p0, p_star)
```

We can now define the functions we need for the optimization
```{julia}
#| output: false
# Distance to q1
g(M, p) = distance(M, p, q1)
grad_g(M, p) = ManifoldDiff.grad_distance(M, q1, p, 1)
# Distance to q2
h(M, p) = τ * distance(M, p, q2)
prox_h(M, λ, p) = ManifoldDiff.prox_distance(M, τ * λ, q2, p, 1)
# Total objective function
f(M, p) = g(M, p) + h(M, p)
# Gradient norm at p_star
gn = norm(M, p_star, grad_g(M, p_star))
```

```{julia}
#| output: true
# Function to generate points close to the given point p
function close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))
    X = rand(M; vector_at = p)
    X .= tol * rand() * X / norm(M, p, X)
    return retract(M, p, X, retraction_method)
end
# Estimate Lipschitz constant of the gradient of g
function estimate_lipschitz_constant(M, g, grad_g, anchor, R=D/2, N=10_000)
    constants = []
    for i in 1:N
        p = close_point(M, anchor, R)
        q = close_point(M, anchor, R)

        push!(constants, 2/distance(M, q, p)^2 * (g(M, q) - g(M, p) - inner(M, p, grad_g(M, p), log(M, p, q))))
    end
    return maximum(constants)
end
 function theoretical_lipschitz_constant(M, g, anchor, R=D/2, N=10_000)
    constants = []
    for i in 1:N
        p = close_point(M, anchor, R)
        push!(constants, max(coth(g(M,p)), 1/g(M,p)))
        end
    return maximum(constants)
end
est_1 = [estimate_lipschitz_constant(M, g, grad_g, p0, D/2) for _ in 1:1000]
est_2 = [theoretical_lipschitz_constant(M, g, p0, D/2) for _ in 1:1000]
# Conservative estimate of the Lipschitz constant
L_g = 1.05 * max(maximum(est_1), maximum(est_2))
```

And check that the stepsize is valid
```{julia}
# Smallest and largest eigenvalues of the Hessian of g at the minimum of f
λ_min = 1/g(M, p_star)
λ_max = coth(g(M, p_star))
λ_r = 2*g(M, p_star)/(g(M, p_star)*coth(g(M, p_star)) + 1)
λ_D = 1/L_g
```

We now take a look at how the inequality in Theorem 4.10 in [BergmannJasaJohnPfeffer:2025:2](@cite) behaves in practice.
```{julia}
#| output: false
#| code-fold: true
# Function to calculate the inequality value
function calculate_inequality_value(λ, gradient_norm, k_min, λ_max, λ_min)
    
    argument = λ * gradient_norm * √(-k_min)
    
    # For small arguments, use Taylor expansion to avoid precision issues
    sinh_term = if abs(argument) < tol^2
        1.0
    else
        sinh(argument) / argument
    end
    
    coth_term = if abs(argument) < tol^2
        1.0
    else
        coth(argument) * argument
    end
    
	max_term = max(abs(coth_term - λ_max * λ), abs(coth_term - λ_min * λ)) 

    return sinh_term * max_term
end
```

```{julia}
#| code-fold: true
begin
    # Generate data points for the plot
    λ_values = range(1e-6, r/τ + 1e-6, length=200)
    # max(g(M, p_star)/τ, 1/λ_min, 2/λ_max) + 1e-6, length=200)

    inequality_values = [calculate_inequality_value(λ, 1.0, k_min, λ_max, λ_min) for λ in λ_values]
    
    # Plot the inequality
    plot(
        λ_values, 
        inequality_values, 
        label="Inequality Value",
        xlabel="λ",
        ylabel="Value",
        lw=2,
        legend=:topright,
        size=(800, 500),
        grid=true,
        framestyle=:box
    )
    
    # Add a horizontal line at y = 1 to show the boundary
    hline!([1.0], label="Inequality Boundary", ls=:dash, lw=2, c=:red)
    
    # Add shaded region for where inequality is satisfied
    satisfied_indices = findall(v -> v < 1.0, inequality_values)
    if !isempty(satisfied_indices)
        first_idx = satisfied_indices[1]
        last_idx = satisfied_indices[end]
        if last_idx - first_idx > 0  # Ensure there's actually a region to shade
            plot!(λ_values[first_idx:last_idx], inequality_values[first_idx:last_idx], 
                  fillrange=1, fillalpha=0.2, c=:green, label="Satisfied Region")
        end
    end
    
    # Add specific vertical lines
    vline!([1/λ_max], label="λ = 1/λ_{max}", ls=:dot, lw=1.5, c=:blue)
    vline!([2/λ_max], label="λ = 2/λ_{max}", ls=:dot, lw=1.5, c=:black)
	vline!([r/τ], label="λ = r/τ", ls=:dot, lw=1.5, c=:orange)
	vline!([λ_D], label="λ = 1/L_g", ls=:dot, lw=1.5, c=:red)
    
    # Enforce y-axis limits 
    plot!(ylim=(0, max(1.5, maximum(inequality_values) * 1.1)))
end
```

Hence, we see that the inequality is not satisfied for $\lambda = 1/L_g$.
We thus reduce it to satisfy the inequality

```{julia}
#| output: true
while calculate_inequality_value(λ_D, 1.0, k_min, λ_max, λ_min) ≥ 1.0
    λ_D *= 0.9
end
λ_D
```

### Constant Stepsize
We can now run the optimization, first using a constant stepsize $\lambda = \lambda_D$.
```{julia}
@time pg = proximal_gradient_method(
	M, 
	f, 
	g, 
	grad_g, 
	p0;
	prox_nonsmooth=prox_h, 
	stepsize=ConstantLength(λ_D),
	record=[:Iteration, :Cost, :Iterate],
	return_state=true,
	stopping_criterion=StopWhenAny(
        StopWhenGradientMappingNormLess(tol), StopAfterIteration(max_iters)
    ),
)
```

Let's first check that all iterates are within the set of points we are interested in...
```{julia}
# Extract recorded values
pg_result = get_solver_result(pg)
pg_record = get_record(pg)
all(x -> x == true, [distance(M, p0, q[3]) ≤ D/2 for q in pg_record])
```

... and that the final iterate is approximately the closed-form solution
```{julia}
pg_result ≈ p_star
```

We now plot the decay of the function values
```{julia}
# | code-fold: true
# | echo: false
# Calculate the minimum cost for relative error
min_cost_pg = minimum(record[2] for record in pg_record)
iterations = [Float64(record[1]) for record in pg_record]
pushfirst!(iterations, 0.5) # Add 0 for the initial point
relative_errors_pg = [max(record[2] - min_cost_pg, eps()) for record in pg_record]
pushfirst!(relative_errors_pg, f(M, p0) - min_cost_pg) # Add initial error for the initial point
# Get initial error for scaling reference lines
initial_error_pg = relative_errors_pg[1]
# Create reference trajectories
ref_rate_1_pg = [initial_error_pg/k for k in iterations]
# ref_rate_2_pg = [initial_error_pg/k^2 for k in iterations]
ref_rate_2k_pg = [initial_error_pg/2^k for k in iterations]
# Create the convergence plot
convergence_plot_pg = plot(
    iterations,
    relative_errors_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="f(pₖ) - f*",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
# Add reference lines
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2ᵏ)"
)
```

And the convergence rate of the iterates
```{julia}
# | code-fold: true
# | echo: false 
distances_pg = [max(distance(M, p_star, record[3]), eps()) for record in pg_record]
pushfirst!(distances_pg, distance(M, p_star, p0)) # Add initial distance for the initial point
# Get initial error for scaling reference lines
initial_distance_pg = distances_pg[1]
# Create reference trajectories
dist_ref_rate_1_pg = [initial_distance_pg/k for k in iterations]
dist_ref_rate_2_pg = [initial_distance_pg/k^2 for k in iterations]
# Create the convergence plot
distances_plot_pg = plot(
    iterations,
    distances_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="dist(pₖ, p*)",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    distances_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2ᵏ)"
)
```

We can write the recorded values to a CSV file
```{julia}
# | output: false
# Create a DataFrame from the recorded values
df_pg = DataFrame(
    Iterations = iterations,
    CostDiff = relative_errors_pg,
    Distances = distances_pg
)
# Write the DataFrame to a CSV file
csv_file_pg = joinpath(results_folder, "constant-stepsize.csv")
CSV.write(csv_file_pg, df_pg)
```

### Backtracked Stepsize
We now run the optimization using a backtracked stepsize starting from $\lambda_D$.
```{julia}
@time pg = proximal_gradient_method(
	M, 
	f, 
	g, 
	grad_g, 
	p0;
	prox_nonsmooth=prox_h, 
	stepsize=ProximalGradientMethodBacktracking(; 
        strategy=:convex, 
        initial_stepsize=λ_D,
        stop_when_stepsize_less=tol,
    ),
	record=[:Iteration, :Cost, :Iterate],
	return_state=true,
	stopping_criterion=StopWhenAny(
        StopWhenGradientMappingNormLess(tol), StopAfterIteration(max_iters)
    ),
)
```

Let's first check that all iterates are within the set of points we are interested in...
```{julia}
# Extract recorded values
pg_result = get_solver_result(pg)
pg_record = get_record(pg)
all(x -> x == true, [distance(M, p0, q[3]) ≤ D/2 for q in pg_record])
```

... and that the final iterate is approximately the closed-form solution
```{julia}
pg_result ≈ p_star
```

We now plot the decay of the function values
```{julia}
# | code-fold: true
# | echo: false
# Calculate the minimum cost for relative error
min_cost_pg = minimum(record[2] for record in pg_record)
iterations = [Float64(record[1]) for record in pg_record]
pushfirst!(iterations, 0.5) # Add 0 for the initial point
relative_errors_pg = [max(record[2] - min_cost_pg, eps()) for record in pg_record]
pushfirst!(relative_errors_pg, f(M, p0) - min_cost_pg) # Add initial error for the initial point
# Get initial error for scaling reference lines
initial_error_pg = relative_errors_pg[1]
# Create reference trajectories
ref_rate_1_pg = [initial_error_pg/k for k in iterations]
ref_rate_2_pg = [initial_error_pg/k^2 for k in iterations]
ref_rate_2k_pg = [initial_error_pg/2^k for k in iterations]
# Create the convergence plot
convergence_plot_pg = plot(
    iterations,
    relative_errors_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="f(pₖ) - f*",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
# Add reference lines
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2^k)"
)
```

And the convergence rate of the iterates
```{julia}
# | code-fold: true
# | echo: false
distances_pg = [max(distance(M, p_star, record[3]), eps()) for record in pg_record]
pushfirst!(distances_pg, distance(M, p_star, p0)) # Add initial distance for the initial point
# Get initial error for scaling reference lines
initial_distance_pg = distances_pg[1]
# Create reference trajectories
dist_ref_rate_1_pg = [initial_distance_pg/k for k in iterations]
dist_ref_rate_2_pg = [initial_distance_pg/k^2 for k in iterations]
# Create the convergence plot
distances_plot_pg = plot(
    iterations,
    distances_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="dist(pₖ, p*)",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    distances_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2^k)"
)
```

We can write the recorded values to a CSV file
```{julia}
# | output: false
# Create a DataFrame from the recorded values
df_pg = DataFrame(
    Iterations = iterations,
    CostDiff = relative_errors_pg,
    Distances = distances_pg,
)
# Write the DataFrame to a CSV file
csv_file_pg = joinpath(results_folder, "backtracked-stepsize.csv")
CSV.write(csv_file_pg, df_pg)
```

<!-- ### Large Stepsize
We can also run the optimization using a large constant stepsize $\lambda = 4.0$ -->
<!-- ```{julia}
@time pg = proximal_gradient_method(
    M, 
    f, 
    g, 
    grad_g, 
    p0;
    prox_nonsmooth=prox_h, 
    stepsize=ConstantLength(4.0),
    record=[:Iteration, :Cost, :Iterate],
    return_state=true,
    stopping_criterion=StopWhenAny(
        StopWhenGradientMappingNormLess(tol), StopAfterIteration(max_iters)
    ),
)
``` -->
<!-- Let's first check that all iterates are within the set of points we are interested in... -->
<!-- ```{julia}
# Extract recorded values
pg_result = get_solver_result(pg)
pg_record = get_record(pg)
all(x -> x == true, [distance(M, p0, q[3]) ≤ D/2 for q in pg_record])
``` -->
<!-- ... and that the final iterate is approximately the closed-form solution -->
<!-- ```{julia}
pg_result ≈ p_star
``` -->
<!-- We now plot the decay of the function values -->
<!-- ```{julia}
# | code-fold: true
# | echo: false
# Calculate the minimum cost for relative error
min_cost_pg = minimum(record[2] for record in pg_record)
iterations = [Float64(record[1]) for record in pg_record]
pushfirst!(iterations, 0.5) # Add 0 for the initial point
relative_errors_pg = [max(record[2] - min_cost_pg, eps()) for record in pg_record]
pushfirst!(relative_errors_pg, f(M, p0) - min_cost_pg) # Add initial error for the initial point
# Get initial error for scaling reference lines
initial_error_pg = relative_errors_pg[1]
# Create reference trajectories
ref_rate_1_pg = [initial_error_pg/k for k in iterations]
# ref_rate_2_pg = [initial_error_pg/k^2 for k in iterations]
ref_rate_2k_pg = [initial_error_pg/2^k for k in iterations]
# Create the convergence plot
convergence_plot_pg = plot(
    iterations,
    relative_errors_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="f(pₖ) - f*",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
# Add reference lines
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    convergence_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2ᵏ)"
)
``` -->

<!-- And the convergence rate of the iterates -->
<!-- ```{julia}
# | code-fold: true
# | echo: false 
distances_pg = [max(distance(M, p_star, record[3]), eps()) for record in pg_record]
pushfirst!(distances_pg, distance(M, p_star, p0)) # Add initial distance for the initial point
# Get initial error for scaling reference lines
initial_distance_pg = distances_pg[1]
# Create reference trajectories
dist_ref_rate_1_pg = [initial_distance_pg/k for k in iterations]
dist_ref_rate_2_pg = [initial_distance_pg/k^2 for k in iterations]
# Create the convergence plot
distances_plot_pg = plot(
    iterations,
    distances_pg;
    xscale=:log10,
    yscale=:log10,
    xlabel="Iteration (k)",
    ylabel="dist(pₖ, p*)",
    label="Proximal Gradient",
    linewidth=2,
    color=:orange,
    marker=:none,
    grid=true,
    legend=:bottomleft
)
plot!(
    distances_plot_pg,
    iterations,
    dist_ref_rate_1_pg;
    linestyle=:dashdot,
    linewidth=1.5,
    color=:black,
    label="O(1/k)"
)
plot!(
    distances_plot_pg,
    iterations,
    ref_rate_2k_pg;
    linestyle=:solid,
    linewidth=1.5,
    color=:green,
    label="O(1/2ᵏ)"
)
``` -->

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
using Pkg
Pkg.status()
```
```{julia}
#| code-fold: true
#| echo: false
#| output: asis
using Dates
println("This tutorial was last rendered $(Dates.format(now(), "U d, Y, H:M:S")).");
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["CRPG-Convex-Stepsize.md"]
Canonical=false
```
````