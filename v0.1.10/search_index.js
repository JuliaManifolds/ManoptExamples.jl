var documenterSearchIndex = {"docs":
[{"location":"examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","page":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Ronny Bergmann 2023-11-06","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Introduction","page":"Frank Wolfe comparison","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"In this example we compare the Difference of Convex Algprithm (DCA) [BFSS23] with the Frank-Wolfe Algorithm, which was introduced in [WS22]. This example reproduces the results from [BFSS23], Section 7.3.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We consider the collowing constraint maximimization problem of the Fréchet mean on the symmetric positive definite matrices mathcal P(n) with the affine invariant metric. Let q_1ldotsq_m in mathcal P(n) be a set of points and mu_1ldotsmu_m be a set of weights, such that they sum to one. We consider then","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmax_pinmathcal C  h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"h(p) =\nsum_j=1^m mu_j d^2(pq_i)\nquad text where \nd^2(pq_i) = operatornametrbigl(\n  log^2(p^-frac12q_jp^-frac12)\nbig)\nqquadtextandqquad\nmathcal C =  pin mathcal M  bar Lpreceq p preceq bar U ","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"for a lower bound L and an upper bound U for the matrices in the positive definite sense A preceq B Leftrightarrow (B-A) is positive semi-definite","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"When every one of the weights mu_1 ldots mu_m are equal, this function h is known as the of the set q_1 dots q_m.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And for our example we set","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Random.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use as lower and upper bound the arithmetic and geometric mean L and U, respectively.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"L = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As a starting point, the Frank-Wolfe algorithm requires a feasible point. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p0 = (L+U)/2","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can check that it is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","page":"Frank Wolfe comparison","title":"Common Functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Given p in mathcal M, X in T_pmathcal M on the symmetric positive definite matrices M, this method computes the closed form solution to","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_qin  mathcal C langle X log_p qrangle\n  = operatorname*argmin_qin  mathcal C operatornametr(Slog(YqY))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"where mathcal C =  q  L preceq q preceq U , S = p^-12Xp^-12, and Y=p^-12.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The solution is given by Z=X^-1Qbigl( P^mathrmT-operatornamesgn(D)_+P+hatLbigr)Q^mathrmTX^-1,@ where S=QDQ^mathrmT is a diagonalization of S, hatU-hatL=P^mathrmTP with hatL=Q^mathrmTXLXQ and hatU=Q^mathrmTXUXQ, where -mboxsgn(D)_+ is the diagonal matrix","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatornamediagbigl(\n  -operatornamesgn(d_11)_+ ldots -operatornamesgn(d_nn)_+\nbigr)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and D=(d_ij).","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closeed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","page":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use g(p) = iota_mathcal C(p) as the indicator funtion of the set mathcal C. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"So we can first check that p0 is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"g(p0,L,U) == 0.0","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"true","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Now setting","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pinmathcal M g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We look for a maximum of h, where g is minimal, i.e. g(p) is zero or in other words p in mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The gradient of h can also be implemented in closed form as","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"grad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can further define the cost, which will just be +infty outside of mathcal C. We define","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Here we can omit the gradient of g in the definition of operatornamegrad f, since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As the last step, we can provide the closed form solver for the DC sub problem given at iteration k by","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pin mathcal C\n  biglangle -operatornamegrad h(p^(k)) exp^-1_p^(k)pbigrangle","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Which we con compute","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For safety, we might want to avoid ending up at the boundary of mathcal C. That is we reduce the distance we walk towards the solution q a bit.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    α = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= α\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($α)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","page":"Frank Wolfe comparison","title":"The DoC solver run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s compare both methods when they have the same stopping criteria","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial F(p): -0.77661458292831\nAt iteration 23 the change of the gradient (3.192989916935325e-13) was less than 1.0e-9.\n 16.468298 seconds (17.31 M allocations: 1.680 GiB, 3.17% gc time, 92.02% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 23 iterations\n\n## Parameters\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{InplaceEvaluation}()\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 300:  not reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 30]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s extract the final point and look at its cost","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.784425242474807","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As well as whether (and how well) it is feasible, that is the following values should all be larger than zero.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (1.1886583723800445e-12, 0.06669240322431051)\n (1.3411042178831775e-5, 0.0671353506908023)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For the statistics we extract the recordings from the state","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","page":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For Frank wolfe, the cost is just defined as -h(p) but the minimisation is constraint to mathcal C, which is enfored by the oracle.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","page":"Frank Wolfe comparison","title":"The FW Solver Run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Similarly we can run the Frank-Wolfe algorithm with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial f(x): -0.776615\n# 2000     f(x): -0.784420 |Δp|: 0.04611942377596  |grad f(p)|: 0.17693408  |Δgrad f(p)|: 0.17555618\n# 4000     f(x): -0.784421 |Δp|: 0.00372201632005  |grad f(p)|: 0.17694619  |Δgrad f(p)|: 0.00749427\n# 6000     f(x): -0.784422 |Δp|: 0.00205683506784  |grad f(p)|: 0.17695204  |Δgrad f(p)|: 0.00414088\n# 8000     f(x): -0.784422 |Δp|: 0.00140675676260  |grad f(p)|: 0.17695565  |Δgrad f(p)|: 0.00283200\n# 10000    f(x): -0.784422 |Δp|: 0.00106177438611  |grad f(p)|: 0.17695815  |Δgrad f(p)|: 0.00213746\nThe algorithm reached its maximal number of iterations (10000).\n152.676058 seconds (54.52 M allocations: 93.992 GiB, 2.15% gc time, 0.59% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{InplaceEvaluation}()\n\n## Stepsize\nDecreasingStepsize(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 10000:    reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"f(x): %f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 2000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we take a look at this result as well","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844220281765162","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And its feasibility","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (4.904818928410655e-10, 0.06659173821656107)\n (3.245654983213335e-5, 0.06713970236096602)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Statistics","page":"Frank Wolfe comparison","title":"Statistics","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We extract the recorded values","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"# DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And let’s plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\")","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"This indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"On the other hand, Frank-Wolfe still has not reached this level function value after 10^4 iterations.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Literature","page":"Frank Wolfe comparison","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\n","category":"page"},{"location":"references/#Literature","page":"References","title":"Literature","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2019), arXiv:1907.10902.\n\n\n\nS. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nM. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"examples/Spectral-Procrustes/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM-for-solving-the-spectral-Procrustes-problem","page":"Spectral Procrustes","title":"A comparison of the RCBM with the PBA, the SGM for solving the spectral Procrustes problem","text":"","category":"section"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"Hajg Jasa 6/27/24","category":"page"},{"location":"examples/Spectral-Procrustes/#Introduction","page":"Spectral Procrustes","title":"Introduction","text":"","category":"section"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to solve the spectral Procrustes problem on mathrmSO(250). This example reproduces the results from [BHJ24], Section 5.","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"page"},{"location":"examples/Spectral-Procrustes/#The-Problem","page":"Spectral Procrustes","title":"The Problem","text":"","category":"section"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"Given two matrices A B in mathbb R^n times d we aim to solve the Procrustes problem","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"    argmin_p in mathrmSO(d) Vert A - B  p Vert_2\n    ","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"where mathrmSO(d) is equipped with the standard bi-invariant metric, and where Vert cdot Vert_2 denotes the spectral norm of a matrix, , its largest singular value. We aim to find the best matrix p in mathbb R^d times d such that p^top p = mathrmid is the identity matrix, or in other words p is the best rotation. Note that the spectral norm is convex in the Euclidean sense, but not geodesically convex on mathrmSO(d). Let us define the objective as","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"    f (p)\n    =\n    Vert A - B  p Vert_2\n    ","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"To obtain subdifferential information, we use","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"    mathrmproj_p(-B^top UV^top)","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"as a substitute for partial f(p), where U and V are some left and right singular vectors, respectively, corresponding to the largest singular value of A - B  p, and mathrmproj_p is the projection onto","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"    mathcal T_p mathrmSO(d)\n    =\n    \n    A in mathbb R^dd  vert  pA^top + Ap^top = 0  mathrmtrace(p^-1A)=0\n    \n    ","category":"page"},{"location":"examples/Spectral-Procrustes/#Numerical-Experiment","page":"Spectral Procrustes","title":"Numerical Experiment","text":"","category":"section"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"We initialize the experiment parameters, as well as some utility functions.","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"Random.seed!(33)\nn = 1000\nd = 250\nA = rand(n, d)\nB = randn(n, d)\ntol = 1e-8\n#\n# Compute the orthogonal Procrustes minimizer given A and B\nfunction orthogonal_procrustes(A, B)\n    s =  svd((A'*B)')\n    R = s.U* s.Vt\n    return R\nend\n#\n# Algorithm parameters\nbundle_cap = 25\nmax_iters = 5000\nδ = 0.#1e-2 # Update parameter for μ\nμ = 50. # Initial proxiaml parameter for the proximal bundle method\nk_max = 1/4\ndiameter = π/(3*√k_max)\n#\n# Manifolds and data\nM = SpecialOrthogonal(d)\np0 = orthogonal_procrustes(A, B) #rand(M)\nproject!(M, p0, p0)","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"We now define objective and subdifferential (first the Euclidean one, then the projected one).","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"f(M, p) = opnorm(A - B*p)\nfunction ∂ₑf(M, p)\n    cost_svd = svd(A - B*p)\n    # Find all maxima in S – since S is sorted, these are the first n ones\n    indices = [i for (i, v) in enumerate(cost_svd.S) if abs(v - cost_svd.S[1]) < eps()]\n    ind = rand(indices)\n    return -B'*(cost_svd.U[:,ind]*cost_svd.Vt[ind,:]')\nend\nrpb = Manifolds.RiemannianProjectionBackend(Manifolds.ExplicitEmbeddedBackend(M; gradient=∂ₑf))\n∂f(M, p) = Manifolds.gradient(M, f, p, rpb)\ndomf(M, p) = distance(M, p, p0) < diameter/2 ? true : false","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"We introduce some keyword arguments for the solvers we will use in this experiment","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"rcbm_kwargs = [\n    :bundle_cap => bundle_cap,\n    :k_max => k_max,\n    :domain => domf,\n    :diameter => diameter,\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(tol) | StopAfterIteration(max_iters),\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ξ, \"ξ: %1.8f \"),\n        (:ε, \"ε: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :WarnBundle,\n        :Stop,\n        10,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs = [\n    :k_max => k_max,\n    :domain => domf,\n    :diameter => diameter,\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(tol) | StopAfterIteration(max_iters),\n]\npba_kwargs = [\n    :bundle_size => bundle_cap,\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(tol)|StopAfterIteration(max_iters),\n    :debug =>[\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        :WarnBundle,\n        10,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(tol) |                                   StopAfterIteration(max_iters),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√tol) | StopAfterIteration(max_iters),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :p_star],\n    :return_state => true,\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√tol) |\n                           StopAfterIteration(max_iters),\n]\nglobal header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\"]","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"We run the optimization algorithms…","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"rcbm = convex_bundle_method(M, f, ∂f, p0; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(M, f, ∂f, p0; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(M, f, ∂f, p0; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm)","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"… And we benchmark their performance.","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"if benchmarking\n    pba_bm = @benchmark proximal_bundle_method($M, $f, $∂f, $p0; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($M, $f, $∂f, $p0; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($M, $f, $∂f, $p0; $sgm_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\"]\n    records = [rcbm_record, pba_record, sgm_record]\n    results = [rcbm_result, pba_result, sgm_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n    ]\n    if show_plot\n        global fig = plot(xscale=:log10)\n    end\n    #\n    global D = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records];\n        dims=2,\n    )\n    # \n    \n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            C1 = [0.5 f(M, p0)]\n            C = cat(first.(record), [r[2] for r in record]; dims=2)\n            bm_data = vcat(C1, C)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(bm_data, :auto);\n                header=[\"i\", \"cost\"],\n            )\n            if show_plot\n                plot!(fig, bm_data[:,1], bm_data[:,2]; label=experiment)\n            end\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(D, :auto);\n            header=header,\n        )\n    end\nend","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"We can take a look at how the algorithms compare to each other in their performance with the following table…","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"Algorithm Iterations Time (s) Objective\nRCBM 26 13.7482 235.46\nPBA 31 3.31156 235.46\nSGM 5000 292.542 235.46","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"… and this cost versus iterations plot","category":"page"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"(Image: )","category":"page"},{"location":"examples/Spectral-Procrustes/#Literature","page":"Spectral Procrustes","title":"Literature","text":"","category":"section"},{"location":"examples/Spectral-Procrustes/","page":"Spectral Procrustes","title":"Spectral Procrustes","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing/#Contributing-to-Manopt.jl","page":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The following is a set of guidelines to ManoptExamples.jl.","category":"page"},{"location":"contributing/#Table-of-Contents","page":"Contributing to ManoptExamples.jl","title":"Table of Contents","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Contributing to Manopt.jl\nTable of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd an objective\nCode style","category":"page"},{"location":"contributing/#I-just-have-a-question","page":"Contributing to ManoptExamples.jl","title":"I just have a question","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on our GitHub discussion.","category":"page"},{"location":"contributing/#How-can-I-file-an-issue?","page":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"page"},{"location":"contributing/#How-can-I-contribute?","page":"Contributing to ManoptExamples.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing/#Add-an-objective","page":"Contributing to ManoptExamples.jl","title":"Add an objective","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The objective in Manopt.jl represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a Problem.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have a specific objective you would like to provide here, feel free to start a new file in the src/objectives/ folder in your own fork and propose it later as a Pull Request.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in src/functions/ and follows the naming scheme:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"cost functions are always of the form cost_ and a fitting name\ngradient functions are always of the gradient_ and a fitting name, followed by an !","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"for in-place gradients and by !! if it is a struct that can provide both.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"It would be great if you could also add a small test for the functions and the problem you defined in the test/ section.","category":"page"},{"location":"contributing/#Add-an-example","page":"Contributing to ManoptExamples.jl","title":"Add an example","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding Quarto Markdown file to the examples/ folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the bib/literature.yaml file to add references (in CSL_YAML, which can for example be exported e.g. from Zotero).","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Add any packages you need to the examples/ environment (see the containting Project.toml). The examples will not be run on CI, but their rendered CommonMark outpout should be included in the list of examples in the documentation of this package.","category":"page"},{"location":"contributing/#Code-style","page":"Contributing to ManoptExamples.jl","title":"Code style","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We also follow a few internal conventions:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Any implemented function should be accompanied by its mathematical formulae if a closed form exists.\nwithin a file the structs should come first and functions second. The only exception are constructors for the structs\nwithin both blocks an alphabetical order is preferable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including in-place/non-mutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature.\nAll import/using/include should be in the main module file.\nThere should only be a minimum of exports within this file, all problems should usually be later addressed as ManoptExamples.[...]\nthe Quarto Markdown files are excluded from this formatting.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","page":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Introduction","page":"A Benchmark","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"In this Benchmark we compare the Difference of Convex Algprithm (DCA) [BFSS23] and the Difference of Convex Proximal Point Algorithm (DCPPA) [SO15] which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from [BFSS23], Section 7.1.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"operatorname*argmin_pinmathcal M   g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where ghcolon mathcal M to mathbb R are geodesically convex function on the Riemannian manifold mathcal M.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#The-DC-Problem","page":"A Benchmark","title":"The DC Problem","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We start with defining the two convex functions gh and their gradients as well as the DC problem f and its gradient for the problem","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"    operatorname*argmin_pinmathcal M  bigl( logbigr(det(p)bigr)bigr)^4 - bigl(log det(p) bigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where the critical points obtain a functional value of -frac14.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where mathcal M is the manifold of symmetric positive definite (SPD) matrices with the affine invariant metric, which is the default.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We first define the corresponding functions","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"g(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and their gradients","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"grad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which we can use to verify that the gradients of g and h are correct. We use for that","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"n = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"to tall both checks","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, g, grad_g, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, h, grad_h, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which both pass the test. We continue to define their inplace variants","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"function grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And compare times for both algorithms, with a bit of debug output.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |δp|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |δp|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |δp|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |δp|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000005 |δp|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (7.619584706652929e-11) is less than 1.0e-10.\n  3.531235 seconds (8.71 M allocations: 628.709 MiB, 3.52% gc time, 67.16% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The cost is","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dca)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25000000000000006","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Similarly the DCPPA performs","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    λ=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|δp|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantStepsize(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470 \n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|δp|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|δp|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|δp|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|δp|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|δp|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|δp|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|δp|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.341931 seconds (2.55 M allocations: 180.474 MiB, 2.46% gc time, 59.94% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"It needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dcppa)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","page":"A Benchmark","title":"Benchmark I: Time comparison","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and run a benchmark for both algorithms","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"for n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        λ=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantStepsize(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Since we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","page":"A Benchmark","title":"Benchmark II: Iterations and cost.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"As a second benchmark, let’s collect the number of iterations needed and the development of the cost over dimensions.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"N2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}()","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        λ = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantStepsize(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The iterations are like","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And for the developtment of the cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where we can see that the DCA needs less iterations than the DCPPA.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Literature","page":"A Benchmark","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\n","category":"page"},{"location":"examples/RCBM-Median/#A-comparison-of-the-RCBM-with-the-PBA-and-the-SGM-for-the-Riemannian-median","page":"Riemannian Median","title":"A comparison of the RCBM with the PBA and the SGM for the Riemannian median","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Hajg Jasa 6/27/24","category":"page"},{"location":"examples/RCBM-Median/#Introduction","page":"Riemannian Median","title":"Introduction","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to find the Riemannian median. This example reproduces the results from [BHJ24], Section 5. The runtimes reported in the tables are measured in seconds.","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Let mathcal M be a Hadamard manifold and q_1ldotsq_N in mathcal M denote N = 1000 Gaussian random data points. Let f colon mathcal M to mathbb R be defined by","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"f(p) = sum_j = 1^N w_j  mathrmdist(p q_j)","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"where w_j, j = 1 ldots N are positive weights such that sum_j = 1^N w_j = 1.","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"The Riemannian geometric median p^* of the dataset","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"mathcal D = \n    q_1ldotsq_N  vert  q_j in mathcal Mtext for all  j = 1ldotsN\n","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"is then defined as","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"    p^* coloneqq operatorname*argmin_p in mathcal M f(p)","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"where equality is justified since p^* is uniquely determined on Hadamard manifolds. In our experiments, we choose the weights w_j = frac1N.","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"We initialize the experiment parameters, as well as utility functions.","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Random.seed!(33)\nexperiment_name = \"RCBM-Median\"\nresults_folder = joinpath(@__DIR__, experiment_name)\n!isdir(results_folder) && mkdir(results_folder)\n\natol = 1e-8\nN = 1000 # number of data points\nspd_dims = [2, 5, 10, 15]\nhn_sn_dims = [1, 2, 5, 10, 15]\n\n# Generate a point that is at least `tol` close to the point `p` on `M`\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"# Objective and subdifferential\nf(M, p, data) = sum(1 / length(data) * distance.(Ref(M), Ref(p), data))\ndomf(M, p, p0, diameter) = distance(M, p, p0) < diameter / 2 ? true : false\nfunction ∂f(M, p, data, atol=atol)\n    return sum(\n        1 / length(data) *\n        ManifoldDiff.subgrad_distance.(Ref(M), data, Ref(p), 1; atol=atol),\n    )\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"rcbm_kwargs(diameter, domf, k_max) = [\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :count => [:Cost, :SubGradient],\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ξ, \"ξ: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs(diameter, domf, k_max) = [\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n]\npba_kwargs = [\n    :count => [:Cost, :SubGradient],\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\npba_bm_kwargs = [:cache => (:LRU, [:Cost, :SubGradient], 50),]\nsgm_kwargs = [\n    :count => [:Cost, :SubGradient],\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(1e-4) | StopAfterIteration(5000),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingStepsize(1, 1, 0, 1, 0, :absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(1e-4) | StopAfterIteration(5000),\n]","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Before running the experiments, we initialize data collection functions that we will use later","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"global col_names_1 = [\n    :Dimension,\n    :Iterations_1,\n    :Time_1,\n    :Objective_1,\n    :Iterations_2,\n    :Time_2,\n    :Objective_2,\n]\ncol_types_1 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)\nglobal col_names_2 = [\n    :Dimension,\n    :Iterations,\n    :Time,\n    :Objective,\n]\ncol_types_2 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_2 = (; zip(col_names_2, type[] for type in col_types_2 )...)\nfunction initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1, named_tuple_2)\n    A1 = DataFrame(named_tuple_1)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Convex-Prox.csv\",\n        ),\n        A1;\n        header=false,\n    )\n    A2 = DataFrame(named_tuple_2)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Subgrad.csv\",\n        ),\n        A2;\n        header=false,\n    )\n    return A1, A2\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"function export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1, col_names_2)\n    B1 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations_1=maximum(first.(records[1])),\n        Time_1=times[1],\n        Objective_1=minimum([r[2] for r in records[1]]),\n        Iterations_2=maximum(first.(records[2])),\n        Time_2=times[2],\n        Objective_2=minimum([r[2] for r in records[2]]),\n    )\n    B2 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations=maximum(first.(records[3])),\n        Time=times[3],\n        Objective=minimum([r[2] for r in records[3]]),\n    )\n    return B1, B2\nend\nfunction write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Convex-Prox.csv\",\n        ),\n        B1;\n        append=true,\n    )\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Subgrad.csv\",\n        ),\n        B2;\n        append=true,\n    )\nend","category":"page"},{"location":"examples/RCBM-Median/#The-Median-on-the-Hyperboloid-Model","page":"Riemannian Median","title":"The Median on the Hyperboloid Model","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"subexperiment_name = \"Hn\"\nk_max_hn = 0.0\ndiameter_hn = floatmax(Float64)\nglobal A1, A2 = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n\n    M = Hyperbolic(Int(2^n))\n    data = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data, y in data]\n    p0 = data[minimum(Tuple(findmax(dists)[2]))]\n\n    f_hn(M, p) = f(M, p, data)\n    domf_hn(M, p) = domf(M, p, p0, diameter_hn)\n    ∂f_hn(M, p) = ∂f(M, p, data, atol)\n\n    # Optimization\n    pba = proximal_bundle_method(M, f_hn, ∂f_hn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    rcbm = convex_bundle_method(M, f_hn, ∂f_hn, p0; rcbm_kwargs(diameter_hn, domf_hn, k_max_hn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    sgm = subgradient_method(M, f_hn, ∂f_hn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_hn, $∂f_hn, $p0; rcbm_bm_kwargs($diameter_hn, $domf_hn, $k_max_hn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_hn, $∂f_hn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_hn, $∂f_hn, $p0; $sgm_bm_kwargs...)\n        \n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1, B2 = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1, B1)\n        append!(A2, B2)\n        (export_table) && (write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name))\n    end\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n2 9 0.00479385 1.07452 225 0.113286 1.07452\n4 8 0.00464954 1.09164 235 0.120968 1.09164\n32 13 0.0110309 1.10437 223 0.159239 1.10437\n1024 16 0.250066 1.09989 240 3.53749 1.09989\n32768 14 5.66661 1.07903 223 81.9645 1.07903","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"… Whereas the following table refers to the SGM","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations Time Objective\n2 15 0.00681038 1.02872\n4 18 0.00879092 1.08236\n32 21 0.0168037 1.10419\n1024 24 0.342026 1.09989\n32768 19 6.39479 1.07883","category":"page"},{"location":"examples/RCBM-Median/#The-Median-on-the-Symmetric-Positive-Definite-Matrix-Space","page":"Riemannian Median","title":"The Median on the Symmetric Positive Definite Matrix Space","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"subexperiment_name = \"SPD\"\nk_max_spd = 0.0\ndiameter_spd = floatmax(Float64)\nglobal A1_SPD, A2_SPD = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in spd_dims\n\n    M = SymmetricPositiveDefinite(Int(n))\n    data = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data, y in data]\n    p0 = data[minimum(Tuple(findmax(dists)[2]))]\n\n    f_spd(M, p) = f(M, p, data)\n    domf_spd(M, p) = domf(M, p, p0, diameter_spd)\n    ∂f_spd(M, p) = ∂f(M, p, data, atol)\n\n    # Optimization\n    pba = proximal_bundle_method(M, f_spd, ∂f_spd, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    rcbm = convex_bundle_method(M, f_spd, ∂f_spd, p0; rcbm_kwargs(diameter_spd, domf_spd, k_max_spd)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    sgm = subgradient_method(M, f_spd, ∂f_spd, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_spd, $∂f_spd, $p0; rcbm_bm_kwargs($diameter_spd, $domf_spd, $k_max_spd)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_spd, $∂f_spd, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_spd, $∂f_spd, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_SPD, B2_SPD = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_SPD, B1_SPD)\n        append!(A2_SPD, B2_SPD)\n        (export_table) && (write_dataframes(B1_SPD, B2_SPD, results_folder, experiment_name, subexperiment_name))\n    end\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n3 16 0.152925 0.254264 56 0.525707 0.254264\n15 11 0.265103 0.433549 83 1.9075 0.433549\n55 10 0.599664 0.624809 91 4.86849 0.624809\n120 9 1.03983 0.769986 117 11.5079 0.769986","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"… Whereas the following table refers to the SGM","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations Time Objective\n3 5000 41.8571 0.254264\n15 1758 40.2008 0.433549\n55 768 40.049 0.624809\n120 429 41.8372 0.769986","category":"page"},{"location":"examples/RCBM-Median/#The-Median-on-the-Sphere","page":"Riemannian Median","title":"The Median on the Sphere","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"For the last experiment, note that a major difference here is that the sphere has constant positive sectional curvature equal to 1. In this case, we lose the global convexity of the Riemannian distance and thus of the objective. Minimizers still exist, but they may, in general, be non-unique.","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"subexperiment_name = \"Sn\"\nk_max_sn = 1.0\ndiameter_sn = π / 4\nglobal A1_Sn, A2_Sn = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n\n    M = Sphere(Int(2^n))\n    north = [0.0 for _ in 1:manifold_dimension(M)]\n    push!(north, 1.0)\n    diameter = π / 4 #2 * π/7\n    data = [close_point(M, north, diameter / 2) for _ in 1:n]\n    p0 = data[1]\n\n    f_sn(M, p) = f(M, p, data)\n    domf_sn(M, p) = domf(M, p, p0, diameter_sn)\n    ∂f_sn(M, p) = ∂f(M, p, data, atol)\n\n    # Optimization\n    pba = proximal_bundle_method(M, f_sn, ∂f_sn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    rcbm = convex_bundle_method(M, f_sn, ∂f_sn, p0; rcbm_kwargs(diameter_sn, domf_sn, k_max_sn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    sgm = subgradient_method(M, f_sn, ∂f_sn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_sn, $∂f_sn, $p0; rcbm_bm_kwargs($diameter_sn, $domf_sn, $k_max_sn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $∂f_sn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_sn, $∂f_sn, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_Sn, B2_Sn = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_Sn, B1_Sn)\n        append!(A2_Sn, B2_Sn)\n        (export_table) && (write_dataframes(B1_Sn, B2_Sn, results_folder, experiment_name, subexperiment_name))\n    end\nend","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n2 2 0.000121292 0.0 3 0.00012375 0.0\n4 5000 0.740364 0.163604 30 0.00401738 0.163604\n32 68 0.00849283 0.103703 37 0.00136038 0.103703\n1024 65 0.0922398 0.119482 32 0.00716244 0.119482\n32768 39 1.2733 0.176366 37 0.361279 0.176366","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"… Whereas the following table refers to the SGM","category":"page"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"Dimension Iterations Time Objective\n2 5000 0.0143786 9.98865e-15\n4 30 0.000119917 0.163604\n32 5000 0.0257619 0.103703\n1024 5000 0.718539 0.119482\n32768 5000 33.9769 0.176366","category":"page"},{"location":"examples/RCBM-Median/#Literature","page":"Riemannian Median","title":"Literature","text":"","category":"section"},{"location":"examples/RCBM-Median/","page":"Riemannian Median","title":"Riemannian Median","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"page"},{"location":"examples/HyperparameterOptimization/#Hyperparameter-optimization","page":"Hyperparameter optimziation","title":"Hyperparameter optimization","text":"","category":"section"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Mateusz Baran 2024-08-03","category":"page"},{"location":"examples/HyperparameterOptimization/#Introduction","page":"Hyperparameter optimziation","title":"Introduction","text":"","category":"section"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"This example shows how to automatically select the best values of hyperparameters of optimization procedures such as retraction, vector transport, size of memory in L-BFGS or line search coefficients. Hyperparameter optimization relies on the Optuna [ASY+19] Python library because it is much more advanced than similar Julia projects, offering Bayesian optimization with conditional hyperparameters and early stopping.","category":"page"},{"location":"examples/HyperparameterOptimization/#General-definitions","page":"Hyperparameter optimziation","title":"General definitions","text":"","category":"section"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Here are some general definitions that you will most likely be able to directly use for your problem without any changes. Just remember to install optuna, for example using CondaPkg Julia library.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"using Manifolds, Manopt\nusing PythonCall\nusing BenchmarkTools\nusing LineSearches\n\n# This script requires optuna to be available through PythonCall\n# You can install it for example using\n# using CondaPkg\n# ]conda add optuna\n\noptuna = pyimport(\"optuna\")\n\nnorm_inf(M::AbstractManifold, p, X) = norm(X, Inf)\n\n# TTsuggest_ structs collect data from a calibrating optimization run\n# that is handled by compute_pruning_losses function\n\nstruct TTsuggest_int\n    suggestions::Dict{String,Int}\nend\nfunction (s::TTsuggest_int)(name::String, a, b)\n    return s.suggestions[name]\nend\nstruct TTsuggest_float\n    suggestions::Dict{String,Float64}\nend\nfunction (s::TTsuggest_float)(name::String, a, b; log::Bool=false)\n    return s.suggestions[name]\nend\nstruct TTsuggest_categorical\n    suggestions::Dict{String,Any}\nend\nfunction (s::TTsuggest_categorical)(name::String, vals)\n    return s.suggestions[name]\nend\nstruct TTreport\n    reported_vals::Vector{Float64}\nend\nfunction (r::TTreport)(val, i)\n    return push!(r.reported_vals, val)\nend\nstruct TTshould_prune end\n(::TTshould_prune)() = Py(false)\nstruct TracingTrial\n    suggest_int::TTsuggest_int\n    suggest_float::TTsuggest_float\n    suggest_categorical::TTsuggest_categorical\n    report::TTreport\n    should_prune::TTshould_prune\nend\n\nfunction compute_pruning_losses(\n    od,\n    int_suggestions::Dict{String,Int},\n    float_suggestions::Dict{String,Float64},\n    categorical_suggestions::Dict{String,Int},\n)\n    tt = TracingTrial(\n        TTsuggest_int(int_suggestions),\n        TTsuggest_float(float_suggestions),\n        TTsuggest_categorical(categorical_suggestions),\n        TTreport(Float64[]),\n        TTshould_prune(),\n    )\n    od(tt)\n    return tt.report.reported_vals\nend","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"The next part is your hyperparameter optimization objective. The ObjectiveData struct contains all relevant information about the sequence of specific problems. The outermost key part is the N_range field. Early stopping requires a series of progressively more complex problems. They will be attempted from the most simple one to the most complex one, and are specified by the values of N in that vector.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"mutable struct ObjectiveData{TObj,TGrad}\n    obj::TObj\n    grad::TGrad\n    N_range::Vector{Int}\n    gtol::Float64\n    vts::Vector{AbstractVectorTransportMethod}\n    retrs::Vector{AbstractRetractionMethod}\n    manifold_constructors::Vector{Tuple{String,Any}}\n    pruning_losses::Vector{Float64}\n    manopt_stepsize::Vector{Tuple{String,Any}}\n    obj_loss_coeff::Float64\nend","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"In the example below we optimize hyperparameters on a sequence of Rosenbrock-type problems restricted to spheres:","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"argmin_p in S^N-1 sum_i=1^N2 (1-p_2i)^2 + 100 (p_2i+1 - p_2i^2)^2","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"where N in 2 16 128 1024 8192 65536.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"obj and grad are the objective and gradient, here defined as below. Note that gradient works in-place and variants without manifolds are also provided for easier comparison with other libraries like Optim.jl. It is easiest when problems for different values N can be distinguished by being defined on successively larger manifolds but the script could be modified so that it’s not necessary.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"pruning_losses and compute_pruning_losses are related to early pruning used in Optuna and you shouldn’t have to modify them.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"function f_rosenbrock(x)\n    result = 0.0\n    for i in 1:2:length(x)\n        result += (1.0 - x[i])^2 + 100.0 * (x[i + 1] - x[i]^2)^2\n    end\n    return result\nend\nfunction f_rosenbrock(::AbstractManifold, x)\n    return f_rosenbrock(x)\nend\n\nfunction g_rosenbrock!(storage, x)\n    for i in 1:2:length(x)\n        storage[i] = -2.0 * (1.0 - x[i]) - 400.0 * (x[i + 1] - x[i]^2) * x[i]\n        storage[i + 1] = 200.0 * (x[i + 1] - x[i]^2)\n    end\n    return storage\nend\nfunction g_rosenbrock!(M::AbstractManifold, storage, x)\n    g_rosenbrock!(storage, x)\n    riemannian_gradient!(M, storage, x, storage)\n    return storage\nend","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Next, gtol is the tolerance used for the stopping criterion in optimization. vts and retrs are, respectively, vector transports and retraction methods selected through hyperparameter optimization. Some items need to be different for different values of N, for example the manifold over which the problem is defined. This is handled by manifold_constructors which is then defined as Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))], where the string \"Sphere\" is used to identify the manifold family and the next element is a function that transforms the value of N to the manifold for the problem of size N.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Similarly, different stepsize selection methods may be considered. This is handled by the field manopt_stepsize. It will be easiest to see how it works by looking at how it is initialized:","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Tuple{String,Any}[\n    (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n    (\"Wolfe-Powell\", (M, c1, c2) -> Manopt.WolfePowellLinesearch(M, c1, c2)),\n]","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"We have a string that identifies the line search method name and a constructor of the line search which takes relevant arguments like the manifold or a numerical parameter.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"The next part is the trial evaluation procedure. This is one of the more important places which need to be customized to your problem. This is the point where we tell Optuna about the relevant optimization hyperparameters and use them to define specific problems. The hyperparameter optimization is a multiobjective problem: we want as good problem objectives as possible and as low times as possible. As Optuna doesn’t currently support multicriteria pruning, which is important for obtaining a solution in a reasonable amount of time, we use a linear combination of sub-objectives to turn the problem into a single-criterion optimization. The hyperparameter optimization objective is a linear combination of achieved objectives the relative weight is controlled by objective.obj_loss_coeff.","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"function (objective::ObjectiveData)(trial)\n    # Here we use optuna to select memory length for L-BFGS -- an integer in the range between 2 and 30, referenced by name \"mem_len\"\n    mem_len = trial.suggest_int(\"mem_len\", 2, 30)\n\n    # Here we select a vector transport and retraction methods, one of those specified in the `ObjectiveData`.\n    vt = objective.vts[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"vector_transport_method\", Vector(eachindex(objective.vts))\n        ),\n    )]\n    retr = objective.retrs[pyconvert(\n        Int,\n        trial.suggest_categorical(\"retraction_method\", Vector(eachindex(objective.retrs))),\n    )]\n\n    # Here we select the manifold constructor, in case we want to try different manifolds for our problem. For example one could try defining a problem with orthogonality constraints on Stiefel, Grassmann or flag manifold.\n    manifold_name, manifold_constructor = objective.manifold_constructors[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manifold\", Vector(eachindex(objective.manifold_constructors))\n        ),\n    )]\n\n    # Here the stepsize selection method type is selected.\n    manopt_stepsize_name, manopt_stepsize_constructor = objective.manopt_stepsize[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manopt_stepsize\", Vector(eachindex(objective.manopt_stepsize))\n        ),\n    )]\n\n    # This parametrizes stepsize selection methods with relevant numerical parameters.\n    local c1_val, c2_val, hz_sigma\n    if manopt_stepsize_name == \"Wolfe-Powell\"\n        c1_val = pyconvert(\n            Float64, trial.suggest_float(\"Wolfe-Powell c1\", 1e-5, 1e-2; log=true)\n        )\n        c2_val =\n            1.0 - pyconvert(\n                Float64, trial.suggest_float(\"Wolfe-Powell 1-c2\", 1e-4, 1e-2; log=true)\n            )\n    elseif manopt_stepsize_name == \"Improved HZ\"\n        hz_sigma = pyconvert(Float64, trial.suggest_float(\"Improved HZ sigma\", 0.1, 0.9))\n    end\n\n    # The current loss estimate, taking into account estimated loss values for larger, not-yet-evaluated values of `N`.\n    loss = sum(objective.pruning_losses)\n\n    # Here iterate over problems we want to optimize for\n    # from smallest to largest; pruning should stop the iteration early\n    # if the hyperparameter set is not promising\n    cur_i = 0\n    for N in objective.N_range\n        # Here we define the initial point for the optimization procedure\n        p0 = zeros(N)\n        p0[1] = 1\n        M = manifold_constructor(N)\n        # Here we construct the specific line search to be used\n        local ls\n        if manopt_stepsize_name == \"Wolfe-Powell\"\n            ls = manopt_stepsize_constructor(M, c1_val, c2_val)\n        elseif manopt_stepsize_name == \"Improved HZ\"\n            ls = manopt_stepsize_constructor(M, hz_sigma)\n        else\n            ls = manopt_stepsize_constructor(M)\n        end\n        manopt_time, manopt_iters, manopt_obj = benchmark_time_state(\n            ManoptQN(),\n            M,\n            N,\n            objective.obj,\n            objective.grad,\n            p0,\n            ls,\n            pyconvert(Int, mem_len),\n            objective.gtol;\n            vector_transport_method=vt,\n            retraction_method=retr,\n        )\n        # TODO: turn this into multi-criteria optimization when Optuna starts supporting\n        # pruning in such problems\n        loss -= objective.pruning_losses[cur_i + 1]\n        loss += manopt_time + objective.obj_loss_coeff * manopt_obj\n        trial.report(loss, cur_i)\n        if pyconvert(Bool, trial.should_prune().__bool__())\n            throw(PyException(optuna.TrialPruned()))\n        end\n        cur_i += 1\n    end\n    return loss\nend","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"In the following benchmarking code you will most likely have to adapt solver parameters. This is designed around quasi_Newton but can be adapted to any solver as needed. The example below performs a small number of trials for quick rendering but in practice you should aim for at least a few thousand trials (the n_trials parameter).","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"# An abstract type in case we want to try different optimization packages.\nabstract type AbstractOptimConfig end\nstruct ManoptQN <: AbstractOptimConfig end\n\n# Benchmark that evaluates hyperparameters. Returns time to reach the solution, number of iterations and final value of the objective.\nfunction benchmark_time_state(\n    ::ManoptQN,\n    M::AbstractManifold,\n    N,\n    f,\n    g!,\n    p0,\n    stepsize::Manopt.Stepsize,\n    mem_len::Int,\n    gtol::Real;\n    kwargs...,\n)\n    manopt_sc = StopWhenGradientNormLess(gtol; norm=norm_inf) | StopAfterIteration(1000)\n    mem_len = min(mem_len, manifold_dimension(M))\n    manopt_state = quasi_Newton(\n        M,\n        f,\n        g!,\n        p0;\n        stepsize=stepsize,\n        evaluation=InplaceEvaluation(),\n        return_state=true,\n        memory_size=mem_len,\n        stopping_criterion=manopt_sc,\n        debug=[],\n        kwargs...,\n    )\n    bench_manopt = @benchmark quasi_Newton(\n        $M,\n        $f,\n        $g!,\n        $p0;\n        stepsize=$(stepsize),\n        evaluation=$(InplaceEvaluation()),\n        memory_size=$mem_len,\n        stopping_criterion=$(manopt_sc),\n        debug=[],\n        $kwargs...,\n    )\n    iters = get_count(manopt_state, :Iterations)\n    final_val = f(M, manopt_state.p)\n    return median(bench_manopt.times) / 1000, iters, final_val\nend\n\n\"\"\"\n    lbfgs_study(; pruning_coeff::Float64=0.95)\n\nSet up the example hyperparameter optimization study.\n\"\"\"\nfunction lbfgs_study(; pruning_coeff::Float64=0.95)\n    Ns = [2^n for n in 1:3:12]\n    ls_hz = LineSearches.HagerZhang()\n    od = ObjectiveData(\n        f_rosenbrock,\n        g_rosenbrock!,\n        Ns,\n        1e-5,\n        AbstractVectorTransportMethod[ParallelTransport(), ProjectionTransport()],\n        [ExponentialRetraction(), ProjectionRetraction()],\n        Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))],\n        zeros(Float64, eachindex(Ns)),\n        Tuple{String,Any}[\n            (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n            #(\"Improved HZ\", (M, sigma) -> HagerZhangLinesearch(M; sigma=sigma)),\n            (\"Wolfe-Powell\", (M, c1, c2) -> Manopt.WolfePowellLinesearch(M, c1, c2)),\n        ],\n        10.0,\n    )\n\n    # Here you need to define baseline values of all hyperparameters\n    baseline_pruning_losses = compute_pruning_losses(\n        od,\n        Dict(\"mem_len\" => 4),\n        Dict(\n            \"Wolfe-Powell c1\" => 1e-4,\n            \"Wolfe-Powell 1-c2\" => 1e-3,\n            \"Improved HZ sigma\" => 0.9,\n        ),\n        Dict(\n            \"vector_transport_method\" => 1,\n            \"retraction_method\" => 1,\n            \"manifold\" => 1,\n            \"manopt_stepsize\" => 1,\n        ),\n    )\n    od.pruning_losses = pruning_coeff * baseline_pruning_losses\n\n    study = optuna.create_study(; study_name=\"L-BFGS\")\n    # Here you can specify number of trials and timeout (in seconds).\n    study.optimize(od; n_trials=1000, timeout=500)\n    println(\"Best params is $(study.best_params) with value $(study.best_value)\")\n    selected_manifold = od.manifold_constructors[pyconvert(Int, study.best_params[\"manifold\"])][1]\n    selected_retraction_method = od.retrs[pyconvert(Int, study.best_params[\"retraction_method\"])]\n    selected_vector_transport = od.vts[pyconvert(Int, study.best_params[\"vector_transport_method\"])]\n    println(\"Selected manifold: $(selected_manifold)\")\n    println(\"Selected retraction method: $(selected_retraction_method)\")\n    println(\"Selected vector transport method: $(selected_vector_transport)\")\n    return study\nend\n\nlbfgs_study()","category":"page"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"Best params is {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006125542888545935, 'Wolfe-Powell 1-c2': 0.0010744467792321093} with value 5510.963227438757\nSelected manifold: Sphere\nSelected retraction method: ExponentialRetraction()\nSelected vector transport method: ProjectionTransport()\n\n[I 2024-03-16 18:04:17,965] A new study created in memory with name: L-BFGS\n[I 2024-03-16 18:04:45,996] Trial 0 finished with value: 5639.789870295856 and parameters: {'mem_len': 26, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00027288064367948073, 'Wolfe-Powell 1-c2': 0.00026503788892114045}. Best is trial 0 with value: 5639.789870295856.\n[I 2024-03-16 18:05:11,860] Trial 1 finished with value: 5635.936370295855 and parameters: {'mem_len': 11, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.002743250060163298, 'Wolfe-Powell 1-c2': 0.00037986521186922096}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:05:39,386] Trial 2 finished with value: 5673.101441724422 and parameters: {'mem_len': 26, 'vector_transport_method': 2, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00043339485784312605, 'Wolfe-Powell 1-c2': 0.0027302649933974173}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:10,279] Trial 3 finished with value: 7410.818084581564 and parameters: {'mem_len': 26, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:37,995] Trial 4 finished with value: 5756.566226449636 and parameters: {'mem_len': 25, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:42,755] Trial 5 pruned. \n[I 2024-03-16 18:06:58,577] Trial 6 pruned. \n[I 2024-03-16 18:07:15,366] Trial 7 pruned. \n[I 2024-03-16 18:07:40,605] Trial 8 finished with value: 5581.7437274386975 and parameters: {'mem_len': 7, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0010567355712112379, 'Wolfe-Powell 1-c2': 0.003948002490203636}. Best is trial 8 with value: 5581.7437274386975.\n[I 2024-03-16 18:07:46,021] Trial 9 pruned. \n[I 2024-03-16 18:08:11,512] Trial 10 finished with value: 5510.963227438757 and parameters: {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006125542888545935, 'Wolfe-Powell 1-c2': 0.0010744467792321093}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:08:35,914] Trial 11 finished with value: 5521.388656010121 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006738829952322474, 'Wolfe-Powell 1-c2': 0.0010639659137420014}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:00,317] Trial 12 finished with value: 5521.36958458155 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00010975606104676191, 'Wolfe-Powell 1-c2': 0.0007663843095951679}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:24,680] Trial 13 finished with value: 5520.7020845815505 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 6.743450835567536e-05, 'Wolfe-Powell 1-c2': 0.0008779759729737719}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:50,268] Trial 14 pruned. \n[I 2024-03-16 18:10:15,494] Trial 15 finished with value: 5595.119584581556 and parameters: {'mem_len': 6, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 8.147444451747575e-05, 'Wolfe-Powell 1-c2': 0.00012268601197923553}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:10:25,264] Trial 16 pruned. \n[I 2024-03-16 18:10:50,209] Trial 17 finished with value: 5572.474513153012 and parameters: {'mem_len': 5, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0015998473664092935, 'Wolfe-Powell 1-c2': 0.0005109172020536229}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:10:54,772] Trial 18 pruned. \n[I 2024-03-16 18:11:04,534] Trial 19 pruned. \n[I 2024-03-16 18:11:28,873] Trial 20 finished with value: 5512.3824417244705 and parameters: {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.1581668103921961e-05, 'Wolfe-Powell 1-c2': 0.0002691056199427656}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:11:53,327] Trial 21 finished with value: 5529.088227438692 and parameters: {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.3645031886009879e-05, 'Wolfe-Powell 1-c2': 0.0001863385753491203}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:12:17,911] Trial 22 finished with value: 5522.041370295835 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.0030173525937465e-05, 'Wolfe-Powell 1-c2': 0.000543991948003312}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:12:27,645] Trial 23 pruned. \n[I 2024-03-16 18:12:52,163] Trial 24 finished with value: 5528.840941724406 and parameters: {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.000245400433292576, 'Wolfe-Powell 1-c2': 0.000133639324295565}. Best is trial 10 with value: 5510.963227438757.\n\nPython: <optuna.study.study.Study object at 0x70dd985d9b50>","category":"page"},{"location":"examples/HyperparameterOptimization/#Summary","page":"Hyperparameter optimziation","title":"Summary","text":"","category":"section"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"We’ve shown how to automatically select the best hyperparameter values for your optimization problem.","category":"page"},{"location":"examples/HyperparameterOptimization/#Literature","page":"Hyperparameter optimziation","title":"Literature","text":"","category":"section"},{"location":"examples/HyperparameterOptimization/","page":"Hyperparameter optimziation","title":"Hyperparameter optimziation","text":"T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2019), arXiv:1907.10902.\n\n\n\n","category":"page"},{"location":"objectives/#List-of-Objectives-defined-for-the-Examples","page":"Objectives","title":"List of Objectives defined for the Examples","text":"","category":"section"},{"location":"objectives/#Rayleigh","page":"Objectives","title":"Rayleigh Quotient on the Sphere","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rayleigh example (TODO) to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RayleighQuotient.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RayleighQuotientCost","page":"Objectives","title":"ManoptExamples.RayleighQuotientCost","text":"RayleighQuotientCost\n\nA functor representing the Rayleigh Quotient cost function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Rayleigh Quotient in two forms. Either\n\nf(p) = p^mathrmTApqquad p  𝕊^n-1\n\nor extended into the embedding as\n\nf(x) = x^mathrmTAx qquad x  ℝ^n\n\nwhich is not the orignal Rayleigh quotient for performance reasons, but useful if you want to use this as the Euclidean cost in the emedding of 𝕊^n-1.\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientCost(A)\n\nCreate the Rayleigh cost function.\n\nSee also\n\nRayleighQuotientGrad!!, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientGrad!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientGrad!!","text":"RayleighQuotientGrad!!\n\nA functor representing the Rayleigh Quotient gradient function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the gradient of the Rayleigh Quotient in two forms. Either\n\noperatornamegrad f(p) = 2 Ap - 2 (p^mathrmTAp)*pqquad p  𝕊^n-1\n\nor taking the Euclidean gradient of the Rayleigh quotient on the sphere as\n\nf(x) = 2Ax qquad x  ℝ^n\n\nFor details, see Example 3.62 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientGrad!!(A)\n\nCreate the Rayleigh quotient gradient function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientHess!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientHess!!","text":"RayleighQuotientHess!!\n\nA functor representing the Rayleigh Quotient Hessian.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Hessian of the Rayleigh Quotient in two forms. Either\n\noperatornameHess f(p)X = 2 bigl(AX - (p^mathrmTAX)p - (p^mathrmTAp)Xbigr)qquad p  𝕊^n-1 X in T_p𝕊^n-1\n\nor taking the Euclidean Hessian of the Rayleigh quotient on the sphere as\n\n^2f(x)V = 2AV qquad x V  ℝ^n\n\nFor details, see Example 5.27 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientHess!!(A)\n\nCreate the Rayleigh quotient Hessian function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientGrad!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#BezierCurves","page":"Objectives","title":"Bézier Curves","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Bezier Curves example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/BezierCurves.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.BezierSegment","page":"Objectives","title":"ManoptExamples.BezierSegment","text":"BezierSegment\n\nA type to capture a Bezier segment. With n points, a Bézier segment of degree n-1 is stored. On the Euclidean manifold, this yields a polynomial of degree n-1.\n\nThis type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an AbstractVector of BezierSegments where each of the points might be a nested array on a PowerManifold already.\n\nNot that this can also be used to represent tangent vectors on the control points of a segment.\n\nSee also: de_Casteljau.\n\nConstructor\n\nBezierSegment(pts::AbstractVector)\n\nGiven an abstract vector of pts generate the corresponding Bézier segment.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}, AbstractFloat, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.L2_acceleration_Bezier","text":"L2_acceleration_Bezier(M,B,pts,λ,d)\n\ncompute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e.\n\nfracλ2sum_i=0^N d_mathcal M(d_i c_B(i))^2+\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i and d_2 refers to the second order absolute difference second_order_Total_Variation (squared), the junction points are denoted by p_i, and to each p_i corresponds one data item in the manifold points given in d. For details on the acceleration approximation, see acceleration_Bezier. Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_L2_acceleration_Bezier, acceleration_Bezier, grad_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}}} where P","page":"Objectives","title":"ManoptExamples.acceleration_Bezier","text":"acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P}\n\ncompute the value of the discrete Acceleration of the composite Bezier curve\n\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i, i=1N, and d_2 refers to the second order absolute difference second_order_Total_Variation (squared). Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nThis acceleration discretization was introduced in Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, grad_L2_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    T::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    T::AbstractVector,\n    X::AbstractVector,\n)\n\nEvaluate the adjoint of the differential with respect to the controlpoints at several times T. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\n\nevaluate the adjoint of the differential of a composite Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t, η)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t,\n    η,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a point t01 on the curve and a tangent vector ηT_β(t)mathcal M. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.de_Casteljau-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any}}","page":"Objectives","title":"ManoptExamples.de_Casteljau","text":"de_Casteljau(M::AbstractManifold, b::BezierSegment NTuple{N,P}) -> Function\n\nreturn the Bézier curve β(b_0b_n) 01  mathcal M defined by the control points b_0b_nmathcal M, nmathbb N, as a BezierSegment. This function implements de Casteljau's algorithm Casteljau, 1959, Casteljau, 1963 generalized to manifolds by Popiel, Noakes, J Approx Theo, 2007: Let γ_ab(t) denote the shortest geodesic connecting abmathcal M. Then the curve is defined by the recursion\n\nbeginaligned\n    β(tb_0b_1) = gamma_b_0b_1(t)\n    β(tb_0b_n) = gamma_β(tb_0b_n-1) β(tb_1b_n)(t)\nendaligned\n\nand P is the type of a point on the Manifold M.\n\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) -> Function\n\nGiven a vector of Bézier segments, i.e. a vector of control points B=bigl( (b_00b_n_00)(b_0m b_n_mm) bigr), where the different segments might be of different degree(s) n_0n_m. The resulting composite Bézier curve c_B0m  mathcal M consists of m segments which are Bézier curves.\n\nc_B(t) =\n    begincases\n        β(t b_00b_n_00)  text if  t 01\n        β(t-i b_0ib_n_ii)  text if \n            t(ii+1 quad i1m-1\n    endcases\n\nde_Casteljau(M::AbstractManifold, b::BezierSegment, t::Real)\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}, t::Real)\nde_Casteljau(M::AbstractManifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\nde_Casteljau(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n) -> AbstractVector\n\nEvaluate the Bézier curve at time t or at times t in T.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Θ::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at the points in T, which are elementwise in 0N, and each depending the corresponding segment(s). Here, N is the length of B. For the mutating variant the result is computed in Θ.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at t0N, which depends only on the corresponding segment. Here, N is the length of B. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at the points T, elementwise in t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t::Float, X::BezierSegment)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    t,\n    X::BezierSegment\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X given in the tangent spaces of the control points. The result is the “change” of the curve at t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degree-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.get_Bezier_degree","text":"get_Bezier_degree(M::AbstractManifold, b::BezierSegment)\n\nreturn the degree of the Bézier curve represented by the tuple b of control points on the manifold M, i.e. the number of points minus 1.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degrees-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_degrees","text":"get_Bezier_degrees(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\n\nreturn the degrees of the components of a composite Bézier curve represented by tuples in B containing points on the manifold M.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_inner_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_inner_points","text":"get_Bezier_inner_points(M::AbstractManifold, B::AbstractVector{<:BezierSegment} )\nget_Bezier_inner_points(M::AbstractManifold, b::BezierSegment)\n\nreturns the inner (i.e. despite start and end) points of the segments of the composite Bézier curve specified by the control points B. For a single segment b, its inner points are returned\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junction_tangent_vectors-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_junction_tangent_vectors","text":"get_Bezier_junction_tangent_vectors(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junction_tangent_vectors(M::AbstractManifold, b::BezierSegment)\n\nreturns the tangent vectors at start and end points of the composite Bézier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points B, either a vector of segments of controlpoints.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junctions","page":"Objectives","title":"ManoptExamples.get_Bezier_junctions","text":"get_Bezier_junctions(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junctions(M::AbstractManifold, b::BezierSegment)\n\nreturns the start and end point(s) of the segments of the composite Bézier curve specified by the control points B. For just one segment b, its start and end points are returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_points","page":"Objectives","title":"ManoptExamples.get_Bezier_points","text":"get_Bezier_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_Bezier_points(M::AbstractManifold, b::BezierSegment, reduce::Symbol=:default)\n\nreturns the control points of the segments of the composite Bézier curve specified by the control points B, either a vector of segments of controlpoints or a.\n\nThis method reduces the points depending on the optional reduce symbol\n\n:default:        no reduction is performed\n:continuous:     for a continuous function, the junction points are doubled at b_0i=b_n_i-1i-1, so only b_0i is in the vector.\n:differentiable: for a differentiable function additionally log_b_0ib_1i = -log_b_n_i-1i-1b_n_i-1-1i-1 holds. hence b_n_i-1-1i-1 is omitted.\n\nIf only one segment is given, all points of b, b.pts, is returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_segments-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any, Symbol}} where P","page":"Objectives","title":"ManoptExamples.get_Bezier_segments","text":"get_Bezier_segments(M::AbstractManifold, c::AbstractArray{P}, d[, s::Symbol=:default])\n\nreturns the array of BezierSegments B of a composite Bézier curve reconstructed from an array c of points on the manifold M and an array of degrees d.\n\nThere are a few (reduced) representations that can get extended; see also get_Bezier_points. For ease of the following, let c=(c_1c_k) and d=(d_1d_m), where m denotes the number of components the composite Bézier curve consists of. Then\n\n:default: k = m + sum_i=1^m d_i since each component requires one point more than its degree. The points are then ordered in tuples, i.e.\nB = bigl c_1c_d_1+1 (c_d_1+2c_d_1+d_2+2 c_k-m+1+d_mc_k bigr\n:continuous: k = 1+ sum_i=1m d_i, since for a continuous curve start and end point of successive components are the same, so the very first start point and the end points are stored.\nB = bigl c_1c_d_1+1 c_d_1+1c_d_1+d_2+1 c_k-1+d_mb_k) bigr\n:differentiable – for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence k = 2 - m + sum_i=1m d_i and at a junction point b_n with its given prior point c_n-1, i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as b = exp_c_n(-log_c_n c_n-1) such that the assumed differentiability holds\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector, Any, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.grad_L2_acceleration_Bezier","text":"grad_L2_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector,\n    λ,\n    d::AbstractVector{P}\n) where {P}\n\ncompute the gradient of the discretized acceleration of a composite Bézier curve on the Manifold M with respect to its control points B together with a data term that relates the junction points p_i to the data d with a weight λ compared to the acceleration. The curve is evaluated at the points given in pts (elementwise in 0N), where N is the number of segments of the Bézier curve. The summands are grad_distance for the data term and grad_acceleration_Bezier for the acceleration with interpolation constrains. Here the get_Bezier_junctions are included in the optimization, i.e. setting λ=0 yields the unconstrained acceleration minimization. Note that this is ill-posed, since any Bézier curve identical to a geodesic is a minimizer.\n\nNote that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_acceleration_Bezier-Tuple{ManifoldsBase.AbstractManifold, AbstractVector, AbstractVector{<:Integer}, AbstractVector}","page":"Objectives","title":"ManoptExamples.grad_acceleration_Bezier","text":"grad_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector,\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector\n)\n\ncompute the gradient of the discretized acceleration of a (composite) Bézier curve c_B(t) on the Manifold M with respect to its control points B given as a point on the PowerManifold assuming C1 conditions and known degrees. The curve is evaluated at the points given in T (elementwise in 0N, where N is the number of segments of the Bézier curve). The get_Bezier_junctions are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see grad_L2_acceleration_Bezier and set λ=0 therein. This gradient is computed using adjoint_Jacobi_fields. For details, see Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018. See de_Casteljau for more details on the curve.\n\nSee also\n\nacceleration_Bezier,  grad_L2_acceleration_Bezier, L2_acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#RiemannianMean","page":"Objectives","title":"Riemannian Mean","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Riemannian mean example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RiemannianMean.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RiemannianMeanCost","page":"Objectives","title":"ManoptExamples.RiemannianMeanCost","text":"RiemannianMeanCost{P}\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\nf(p) = sum_j=i^N d_mathcal M^2(d_i p)\n\nwhere d_mathcal M is the distance on a Riemannian manifold.\n\nConstructor\n\nRiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P}\n\nInitialize the cost function to a data set data of points on a manfiold M, where each point is of type P.\n\nSee also\n\nRiemannianMeanGradient!!, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RiemannianMeanGradient!!","page":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","text":"RiemannianMeanGradient!!{P} where P\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\noperatornamegradf(p) = sum_j=i^N log_p d_i\n\nwhere d_mathcal M is the distance on a Riemannian manifold and we employ grad_distance to compute the single summands.\n\nThis functor provides both the allocating variant grad_f(M,p) as well as the in-place variant grad_f!(M, X, p) which computes the gradient in-place of X.\n\nConstructors\n\nRiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T}\n\nGenerate the Riemannian mean gradient based on some data points data an intial tangent vector initial_vector. If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant.\n\nRiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T}\n\nInitialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold M is provideed.\n\nSee also\n\nRiemannianMeanCost, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.Riemannian_mean_objective","page":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","text":"Riemannian_mean_objective(data, initial_vector=nothing, evaluation=Manopt.AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n)\n\nGenerate the objective for the Riemannian mean task for some given vector of data points on the Riemannian manifold M.\n\nSee also\n\nRiemannianMeanCost, RiemannianMeanGradient!!\n\nnote: Note\nThe first constructor should only be used if an additional storage of the vector is not feasible, since initialising the initial_vector to nothing disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided.\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#RobustPCA","page":"Objectives","title":"Robust PCA","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Robust PCA example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RobustPCA.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RobustPCACost","page":"Objectives","title":"ManoptExamples.RobustPCACost","text":"RobustPCACost{D,F}\n\nA functor representing the Riemannian robust PCA function on the Grassmann manifold. For some given (column) data Dmathbb R^dtimes n the cost function is defined on some operatornameGr(dm), mn as the sum of the distances of the columns D_i to the subspace spanned by pinoperatornameGr(dm) (represented as a point on the Stiefel manifold). The function reads\n\nf(U) = frac1nsum_i=1^n lVert pp^mathrmTD_i - D_irVert\n\nThis cost additionally provides a Huber regularisation of the cost, that is for some ε0 one use ℓ_ε(x) = sqrtx^2+ε^2 - ε in\n\nf_ε(p) = frac1nsum_i=1^n ℓ_εbigl(lVert pp^mathrmTD_i - D_irVertbigr)\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCACost(data::AbstractMatrix, ε=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, ε=1.0)\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RobustPCAGrad!!","page":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","text":"RobustPCAGrad!!{D,F}\n\nA functor representing the Riemannian robust PCA gradient on the Grassmann manifold. For some given (column) data Xmathbb R^ptimes n the gradient of the RobustPCACost can be computed by projecting the Euclidean gradient onto the corresponding tangent space.\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCAGrad!!(data, ε=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, ε=1.0; evaluation=AllocatingEvaluation())\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the evaluation= keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.robust_PCA_objective","page":"Objectives","title":"ManoptExamples.robust_PCA_objective","text":"robust_PCA_objective(data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluton())\n\nGenerate the objective for the robust PCA task for some given data D and Huber regularization parameter ε.\n\nSee also\n\nRobustPCACost, RobustPCAGrad!!\n\nnote: Note\nSince the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the evaluation, indeed the gradient always allows for both the allocating and the in-place variant to be used, though that keyword is used to setup the objective.\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#Rosenbrock","page":"Objectives","title":"Rosenbrock Function","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rosenbrock example  and The Difference of Convex Rosenbrock Example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/Rosenbrock.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RosenbrockCost","page":"Objectives","title":"ManoptExamples.RosenbrockCost","text":"RosenbrockCost\n\nProvide the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nf(mathcal M p) = a(p_1^2-p_2)^2 + (p_1-b)^2\n\nwhich means that for the 2D case, the manifold mathcal M is ignored.\n\nSee also 📖 Rosenbrock (with slightly different parameter naming).\n\nConstructor\n\nf = Rosenbrock(a,b)\n\ngenerates the struct/function of the Rosenbrock cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockGradient!!","page":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","text":"RosenbrockGradient\n\nProvide Eclidean GRadient fo the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nnabla f(mathcal M p) = beginpmatrix\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \n    -2a(p_1^2-p_2)\nendpmatrix\n\ni.e. also here the manifold is ignored.\n\nConstructor\n\nRosenbrockGradient(a,b)\n\nFunctors\n\ngrad_f!!(M,p)\ngrad_f!!(M, X, p)\n\nevaluate the gradient at p the manifoldmathcal M is ignored.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockMetric","page":"Objectives","title":"ManoptExamples.RosenbrockMetric","text":"RosenbrockMetric <: AbstractMetric\n\nA metric related to the Rosenbrock problem, where the metric at a point pmathbb R^2 is given by\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nwhere the mathrmRb stands for Rosenbrock\n\n\n\n\n\n","category":"type"},{"location":"objectives/#Base.exp-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","page":"Objectives","title":"Base.exp","text":"q = exp(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, q, p, X)\n\nCompute the exponential map with respect to the RosenbrockMetric.\n\n    q = beginpmatrix p_1 + X_1  p_2+X_2+X_1^2endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Base.log-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any}","page":"Objectives","title":"Base.log","text":"X = log(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, X, p, q)\n\nCompute the logarithmic map with respect to the RosenbrockMetric. The formula reads for any j  1m\n\nX = beginpmatrix\n  q_1 - p_1 \n  q_2 - p_2 + (q_1 - p_1)^2\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.inverse_local_metric-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.inverse_local_metric","text":"inverse_local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the inverse of the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG^-1_p =\nbeginpmatrix\n    1  2p_1\n    2p_1  1+4p_1^2 \nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.local_metric-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.local_metric","text":"local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.change_representer-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","page":"Objectives","title":"ManifoldsBase.change_representer","text":"Y = change_representer(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, Y, ::EuclideanMetric, p, X)\n\nGiven the Euclidan gradient X at p, this function computes the corresponting Riesz representer Ysuch that⟨X,Z⟩ = ⟨ Y, Z ⟩_{\\mathrm{Rb},p}holds for allZ, in other wordsY = G(p)^{-1}X`.\n\nthis function is used in riemannian_gradient to convert a Euclidean into a Riemannian gradient.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.inner-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","page":"Objectives","title":"ManifoldsBase.inner","text":"inner(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X, Y)\n\nCompute the inner product on mathbb R^2 with respect to the RosenbrockMetric, i.e. for XY in T_pmathcal M we have\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1\n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Rosenbrock_objective","page":"Objectives","title":"ManoptExamples.Rosenbrock_objective","text":"Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation())\n\nReturn the gradient objective of the Rosenbrock example.\n\nSee also RosenbrockCost, RosenbrockGradient!!\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","page":"Objectives","title":"ManoptExamples.minimizer","text":"minimizer(::RosenbrockCost)\n\nReturn the minimizer of the RosenbrockCost, which is given by\n\np^* = beginpmatrix bb^2 endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Total-Variation","page":"Objectives","title":"Total Variation","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Total Variation example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/TotalVariation.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.Intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.Intrinsic_infimal_convolution_TV12","text":"Intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are second_order_Total_Variation and Total_Variation. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( βmathrmTV(v) + (1-β)mathrmTV_2(w) bigr)\n\nfor more details see [BFPS17, BFPS18].\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation-NTuple{4, Any}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation","text":"L2_Total_Variation(M, p_data, α, p)\n\ncompute the ℓ^2-TV functional on the PowerManifold M for given (fixed) data p_data (on M), a nonnegative weight α, and evaluated at p (on M), i.e.\n\nE(p) = d_mathcal M^2(fp) + alpha operatornameTV(p)\n\nSee also\n\nTotal_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation_1_2-Tuple{ManifoldsBase.PowerManifold, Vararg{Any, 4}}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation_1_2","text":"L2_Total_Variation_1_2(M, f, α, β, x)\n\ncompute the ℓ^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + βoperatornameTV_2(x)\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_second_order_Total_Variation-Tuple{ManifoldsBase.PowerManifold, Any, Any, Any}","page":"Objectives","title":"ManoptExamples.L2_second_order_Total_Variation","text":"L2_second_order_Total_Variation(M, f, β, x)\n\ncompute the ℓ^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + βoperatornameTV_2(x)\n\nas used in [BBSW16].\n\nSee also\n\nsecond_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Total_Variation","page":"Objectives","title":"ManoptExamples.Total_Variation","text":"Total_Variation(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1kcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nsee [WDS14] for more details. In long function names, this might be shortened to TV1 and the 1 might be ommitted if only total variation is present.\n\nSee also\n\ngrad_Total_Variation, prox_Total_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\nadjoint_differential_forward_logs!(M, Y, p, X)\n\nCompute the adjoint differential of forward_logs F occurring, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n. The adjoint differential can be computed in place of Y.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOutput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_forward_logs-Tuple{ManifoldsBase.PowerManifold, Any, Any}","page":"Objectives","title":"ManoptExamples.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\ndifferential_forward_logs!(M, Y, p, X)\n\ncompute the differential of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i  mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the   logs, where mathcal N is the power manifold with the number of dimensions added   to size(x). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.forward_logs","text":"Y = forward_logs(M,x)\nforward_logs!(M, Y, x)\n\ncompute the forward logs F (generalizing forward differences) occurring, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i. This can also be done in place of ξ.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is the power manifold with the number of dimensions added to size(x). The computation can be done in place of Y.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, λ, x[, p=1])\ngrad_Total_Variation!(M, X, λ, x[, p=1])\n\nCompute the (sub)gradient f of all forward differences occurring, in the power manifold array, i.e. of the function\n\nf(p) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nX – resulting tangent vector in T_xmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Any}} where T","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, (x,y)[, p=1])\ngrad_Total_Variation!(M, X, (x,y)[, p=1])\n\ncompute the (deterministic) (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.grad_intrinsic_infimal_convolution_TV12","text":"grad_u, grad_v = grad_intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see second_order_Total_Variation, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nβmathrmTV(v) + (1-β)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, grad_Total_Variation and grad_second_order_Total_Variation, respectively.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"Y = grad_second_order_Total_Variation(M, q[, p=1])\ngrad_second_order_Total_Variation!(M, Y, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1 q_2 q_3) with respect to all three components of qmathcal M^3, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\nmathcal C = bigl c  mathcal M   g(tfrac12q_1q_3) text for some geodesic gbigr\n\ndenote the mid points between q_1 and q_3 on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(q_1q_2q_3) = min_c  mathcal C_q_1q_3 d(c q_2)\n\nWhile the (sub)gradient with respect to q_2 is easy, the other two require the evaluation of an adjoint_Jacobi_field.\n\nThe derivation of this gradient can be found in [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation-2","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"grad_second_order_Total_Variation(M::PowerManifold, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1q_2q_3) with respect to all q_1q_2q_3 occurring along any array dimension in the point q, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.project_collaborative_TV","page":"Objectives","title":"ManoptExamples.project_collaborative_TV","text":"project_collaborative_TV(M, λ, x, Ξ[, p=2,q=1])\nproject_collaborative_TV!(M, Θ, λ, x, Ξ[, p=2,q=1])\n\ncompute the projection onto collaborative Norm unit (or α-) ball, i.e. of the function\n\nF^q(x) = sum_imathcal G\n  Bigl( sum_jmathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for xmathcal M and mathcal I_i is the set of its forward neighbors. The computation can also be done in place of Θ.\n\nThis is adopted from the paper Duran, Möller, Sbert, Cremers, SIAM J Imag Sci, 2016, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"ξ = prox_Total_Variation(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all forward differences occurring in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting  point containing with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"[y1,y2] = prox_Total_Variation(M, λ, [x1,x2] [,p=1])\nprox_Total_Variation!(M, [y1,y2] λ, [x1,x2] [,p=1])\n\nCompute the proximal map operatornameprox_λvarphi of φ(xy) = d_mathcal M^p(xy) with parameter λ. A derivation of this closed form solution is given in see [WDS14].\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_λφ((x1,x2)). The result can also be computed in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.prox_parallel_TV","page":"Objectives","title":"ManoptExamples.prox_parallel_TV","text":"y = prox_parallel_TV(M, λ, x [,p=1])\nprox_parallel_TV!(M, y, λ, x [,p=1])\n\ncompute the proximal maps operatornameprox_λφ of all forward differences occurring in the power manifold array, i.e. φ(x_ix_j) = d_mathcal M^p(x_ix_j) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements). The computation can also be done in place.\n\nSee also prox_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"(y1,y2,y3) = prox_second_order_Total_Variation(M,λ,(x1,x2,x3),[p=1], kwargs...)\nprox_second_order_Total_Variation!(M, y, λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_λvarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2. The result can be computed in place of y.\n\nNote that this function does not have a closed form solution but is solbed by a few steps of the subgradient mehtod from manopt.jl by default. See [BBSW16] for a derivation.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map. The computation can also be done in place.\nnote: Note\n\nThis function requires Manopt.jl to be loaded\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation-Union{Tuple{T}, Tuple{N}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any, Int64}} where {N, T}","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"y = prox_second_order_Total_Variation(M, λ, x[, p=1])\nprox_second_order_Total_Variation!(M, y, λ, x[, p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all centered second order differences occurring in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting point with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place.\n\nnote: Note\nThis function requires Manopt.jl to be loaded\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1kcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i() denotes the mid point between its two arguments that is nearest to x_i, see [BBSW16] for a derivation.\n\nIn long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation-Union{Tuple{T}, Tuple{MT}, Tuple{MT, Tuple{T, T, T}}, Tuple{MT, Tuple{T, T, T}, Any}} where {MT<:ManifoldsBase.AbstractManifold, T}","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the function reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nsee [BBSW16] for a derivation. In long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation, Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.subgrad_Total_Variation","page":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","text":"X = subgrad_TV(M, λ, p[, k=1; atol=0])\nsubgrad_TV!(M, X, λ, p[, k=1; atol=0])\n\nCompute the (randomized) subgradient partial F of all forward differences occurring, in the power manifold array, i.e. of the function\n\nF(p) = sum_isum_j  mathcal I_i d^k(p_ip_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\np – a point.\n\nOuput\n\nX – resulting tangent vector in T_pmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.subgrad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","text":"X = subgrad_TV(M, (p,q)[, k=1; atol=0])\nsubgrad_TV!(M, X, (p,q)[, k=1; atol=0])\n\ncompute the (randomized) subgradient of frac1kd^k_mathcal M(pq) with respect to both p and q (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Literature","page":"Objectives","title":"Literature","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"examples/#List-of-Examples","page":"Overview","title":"List of Examples","text":"","category":"section"},{"location":"examples/","page":"Overview","title":"Overview","text":"Name provides Documentation Comment\nA Benchmark for Difference of Convex contains a few simple functions  \nBézier Curves and Minimizing their Acceleration tools Bézier curves and their acceleration 📚 \nSolving Rosenbrock with Difference of Convex DoC split of Rosenbrock 📚 uses a Rosenbrock based metric\nDifference of Convex vs. Frank-Wolfe   closed-form sub solver\nRiemannian Mean f, operatornamegradf (A/I), objective 📚 \nRobust PCA f, operatornamegradf (A/I), objective 📚 \nRosenbrock f, operatornamegradf (A/I), objective, minimizer 📚 \nThe Rayleigh Quotient f, operatornamegradf (A/I), operatornameHessf (A/I), objective 📚 \nTotal Variation Minimization f, operatornameproxf (A/I), objective 📚 ","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"Symbols:","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"A Allocating variant\nI In-place variant\n📚 link to documented functions in the documentation","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","page":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Introduction","page":"Rosenbrock Metric","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"This example illustrates how the 📖 Rosenbrock problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [BFSS23], Section 7.2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Both the Rosenbrock problem","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"    operatorname*argmin_xin ℝ^2 abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where ab0 and usually b=1 and a gg b, we know the minimizer x^* = (bb^2)^mathrmT, and also the (Euclidean) gradient","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"nabla f(x) =\n  beginpmatrix\n  4a(x_1^2-x_2) -2a(x_1^2-x_2)\n  endpmatrix\n  +\n  beginpmatrix\n  2(x_1-b) 0\n  endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"They are even available already here in ManifoldExamples.jl, see RosenbrockCost and RosenbrockGradient!!.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Furthermore, the RosenbrockMetric can be used on ℝ^2, that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"XY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e. using a gradient but not a Hessian.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"The Euclidean Gradient\nThe Riemannian gradient descent with respect to the RosenbrockMetric\nThe Euclidean Difference of Convex Algorithm\nThe Riemannian Difference of Convex Algorithm respect to the RosenbrockMetric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Where we obtain a difference of convex problem by writing","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f(x) = abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 - bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"g(x) = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 quadtext and quad h(x) = bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_manopt_parameter!\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"To emphasize the effect, we choose a quite large value of a.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"a = 2*10^5\nb = 1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and use the starting point and a direction to check gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"p0 = [0.1, 0.2]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Euclidean gradient we can just use the same approach as in the Rosenbrock example","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M = ℝ^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n∇f!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"define a common debug vector","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"debug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|δp|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and call the gradient descent algorithm","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Eucl_GD_state = gradient_descent(M, f, ∇f!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000000 F(x): 8.9937e-06 |δp|: 1.3835e+00 | |grad f|: 8.170355e-03\n# 20000000 F(x): 2.9474e-09 |δp|: 6.5764e-03 | |grad f|: 1.419191e-04\n# 30000000 F(x): 9.8376e-13 |δp|: 1.1918e-04 | |grad f|: 2.526295e-06\n# 40000000 F(x): 3.2830e-16 |δp|: 2.1773e-06 | |grad f|: 4.526313e-08\n# 50000000 F(x): 1.0154e-19 |δp|: 3.9803e-08 | |grad f|: 6.838240e-10\nAt iteration 53073227 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 53073227 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Riemannian case, we define","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"MetricManifold(Euclidean(2; field=ℝ), ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and the gradient is now adopted to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_f!(M, X, p)\n    ∇f!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 1000000  F(x): 1.3571e-09 |δp|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |δp|: 3.6836e-05 | |grad f|: 9.240792e-09\nAt iteration 2443750 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the convex case, we have to first introduce the two parts of the cost.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and their (Euclidan) gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function ∇h!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ∇h(M, p; a=100, b=1)\n    X = zero(p)\n    ∇h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ∇g!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ∇g(M, p; a=100, b=1)\n    X = zero(p)\n    ∇g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and we define for convenience","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_∇h!(M, X, p) = ∇h!(M, X, p; a=a, b=b)\ndocE_∇g!(M, X, p) = ∇g!(M, X, p; a=a, b=b)\nfunction docE_∇f!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_∇g!(M, X, p)\n  docE_∇h!(M, Y, p)\n  X .-= Y\n  return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we call the difference of convex algorithm on Eucldiean space ℝ^2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"E_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_∇h!, p0;\n    gradient=docE_∇f!,\n    grad_g = docE_∇g!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000    F(x): 2.9705e-09 |δp|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |δp|: 1.2173e-04 | |grad f|: 4.541087e-08\nAt iteration 26549 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"We first have to again defined the gradients with respect to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_h!(M, X, p; a=100, b=1)\n    ∇h!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ∇g!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For X in h(p^(k)) the sunproblem top determine p^(k+1) reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatorname*argmin_pinmathcal M g(p) - langle X log_p^(k)prangle","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with X = operatornamegrad h(p^(k)) that","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"phi(p) = g(p) - langle X log_p^(k)prangle\n= abigl( p_1^2-p_2bigr)^2\n        + 2bigl(p_1-bbigr)^2 - 2(p^(k)_1-b)p_1 + 2(p^(k)_1-b)p^(k)_1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"its Euclidean gradient reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatornamegradphi(p) =\n    nabla varphi(p)\n    = beginpmatrix\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^(k)_1-b)\n        -2a(p_1^2-p_2)\n    endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where we can again employ the gradient conversion from before to obtain the Riemannian gradient.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"mutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (ϕ::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    ϕ(M, X, p)\n    return X\nend\nfunction (ϕ::SubGrad)(M, X, p)\n    X .= [\n        4 * ϕ.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - ϕ.b) - 2 * (ϕ.pk[1] - ϕ.b),\n        -2 * ϕ.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And in orer to update the subsolvers gradient correctly, we have to overwrite","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"set_manopt_parameter!(ϕ::SubGrad, ::Val{:p}, p) = (ϕ.pk .= p)\nset_manopt_parameter!(ϕ::SubGrad, ::Val{:X}, X) = (ϕ.Xk .= X)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we again introduce for ease of use","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \nAt iteration 1235 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","page":"Rosenbrock Metric","title":"Comparison in Iterations","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^8),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\")","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Literature","page":"Rosenbrock Metric","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\n","category":"page"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/Changelog.md\"","category":"page"},{"location":"changelog/#Changelog","page":"Changelog","title":"Changelog","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"All notable changes to this Julia package will be documented in this file.","category":"page"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"page"},{"location":"changelog/#[0.1.10]-–-18/10/2024","page":"Changelog","title":"[0.1.10] – 18/10/2024","text":"","category":"section"},{"location":"changelog/#Changed","page":"Changelog","title":"Changed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Bump dependencies","category":"page"},{"location":"changelog/#[0.1.9]-–-28/06/2024","page":"Changelog","title":"[0.1.9] – 28/06/2024","text":"","category":"section"},{"location":"changelog/#Added","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Three numerical experiments from the Riemannian Convex Bundle paper.","category":"page"},{"location":"changelog/#[0.1.8]-–-12/06/2024","page":"Changelog","title":"[0.1.8] – 12/06/2024","text":"","category":"section"},{"location":"changelog/#Changed-2","page":"Changelog","title":"Changed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"use range compatible with Julia 1.6 and hence lower the compatibility entry for Julia again.","category":"page"},{"location":"changelog/#[0.1.7]-–-07/06/2024","page":"Changelog","title":"[0.1.7] – 07/06/2024","text":"","category":"section"},{"location":"changelog/#Changed-3","page":"Changelog","title":"Changed","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"make Manopt.jl a weak dependency and load functions that require parts of it only load as an extension. This makes it easier to use the examples in the tests of Manopt itself.","category":"page"},{"location":"changelog/#[0.1.6]-–-22/03/2024","page":"Changelog","title":"[0.1.6] – 22/03/2024","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Hyperparameter optimization example.","category":"page"},{"location":"changelog/#[0.1.3]-–-11/12/2023","page":"Changelog","title":"[0.1.3] – 11/12/2023","text":"","category":"section"},{"location":"changelog/#Added-3","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Total variation Minimization cost, proxes, and an example\nBézier curve cost, gradients, and an example.","category":"page"},{"location":"changelog/#[0.1.3]-–-16/09/2023","page":"Changelog","title":"[0.1.3] – 16/09/2023","text":"","category":"section"},{"location":"changelog/#Added-4","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Rayleigh Quotient functions added\nan example illustrating Euclidean gradient/HEssian conversion\nAdd Literature with DocumenterCitations","category":"page"},{"location":"changelog/#[0.1.2]-–-13/06/2023","page":"Changelog","title":"[0.1.2] – 13/06/2023","text":"","category":"section"},{"location":"changelog/#Added-5","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Update examples to use Quarto\nAdd DC examples","category":"page"},{"location":"changelog/#[0.1.1]-–-01/03/2023","page":"Changelog","title":"[0.1.1] – 01/03/2023","text":"","category":"section"},{"location":"changelog/#Added-6","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Rosenbrock function and examples","category":"page"},{"location":"changelog/#[0.1.0]-–-18/02/2023","page":"Changelog","title":"[0.1.0] – 18/02/2023","text":"","category":"section"},{"location":"changelog/#Added-7","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Setup the project to collect example objectives for Manopt.jl which are well-documented and well-tested\nSetup Documentation to provide one example Quarto file for every example objective to illustrate how to use them.","category":"page"},{"location":"examples/Bezier-curves/#Minimizing-the-Acceleration-of-Bézier-Curves-on-the-Sphere","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"using Manifolds, Manopt, ManoptExamples","category":"page"},{"location":"examples/Bezier-curves/#Introduction","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Introduction","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Bézier Curves can be generalized to manifolds by generalizing the de Casteljau algorithm 📖 to work with geodesics instead of straight lines. An implementation in just a few lines as we demonstrated in [ABBR23] as","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"function bezier(M::AbstractManifold, t, pts::NTuple)\n    p = bezier(M, t, pts[1:(end - 1)])\n    q = bezier(M, t, pts[2:end])\n    return shortest_geodesic(M, p, q, t)\nend\nfunction bezier(M::AbstractManifold, t, pts::NTuple{2})\n    return shortest_geodesic(M, pts[1], pts[2], t)\nend","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"which is also available within this package as de_Casteljau using a simple BezierSegment struct to make it easier to also discuss the case where we compose a set of segments into a composite Bézier course.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"In the following we will need the following packages and functions. They are documented in the section on Bezier Curves and were derived in [BG18] based on [PN07].","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"using ManoptExamples:\n    artificial_S2_composite_Bezier_curve,\n    BezierSegment,\n    de_Casteljau,\n    get_Bezier_degrees,\n    get_Bezier_inner_points,\n    get_Bezier_junctions,\n    get_Bezier_junction_tangent_vectors,\n    get_Bezier_points,\n    get_Bezier_segments,\n    grad_L2_acceleration_Bezier,\n    L2_acceleration_Bezier","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"This notebook reproduces the example form Section 5.2 in [BG18].","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"The following image illustrates how the de-Casteljau algorithm works for one segment.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: A Bezier segment and illustration of the de-Casteljau algorithm)","category":"page"},{"location":"examples/Bezier-curves/#Approximating-data-by-a-curve-with-minimal-accelartion","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Approximating data by a curve with minimal accelartion","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We first load our example data","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"M = Sphere(2)\nB = artificial_S2_composite_Bezier_curve()\ndata_points = get_Bezier_junctions(M, B)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Which is the following cure, which clearly starts and ends slower than its speed in the middle, which can be seen by the increasing length of the gangent vectors in the middle.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: The original curve)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We continue to recude the points, since we “know” sme points due to the C^1 property: the second to last control point of the first segment b_02, the joint junction point connecting both segments b_03=b_10 and the second control point b_11 of the second segment have to line in the tangent space of the joint junction point. Hence we only have to store one of the control points.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We can use this reduced form as the variable to optimize and the one from the data as our initial point.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"pB = get_Bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB))","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"PowerManifold(Sphere(2, ℝ), NestedPowerRepresentation(), 8)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"And we further define the acceleration of the curve as our cost function, where we discretize the acceleration at a certain set of points and set the λ=10","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"curve_samples = [range(0, 3; length=101)...] # sample curve for the gradient\nλ = 10.0\nfunction f(M, pB)\n    return L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend\nfunction grad_f(M, pB)\n    return grad_L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"grad_f (generic function with 1 method)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Then we can optimize","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"x0 = pB\npB_opt = gradient_descent(\n    N,\n    f,\n    grad_f,\n    x0;\n    stepsize=ArmijoLinesearch(N;\n        initial_stepsize=1.0,\n        retraction_method=ExponentialRetraction(),\n        contraction_factor=0.5,\n        sufficient_decrease=0.001,\n    ),\n    stopping_criterion=StopWhenChangeLess(1e-5) |\n                       StopWhenGradientNormLess(1e-7) |\n                       StopAfterIteration(300),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        25,\n        :Stop,\n    ],\n);","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Initial  | f(x): 10.647244 |  |  | \n# 25     | f(x): 2.667564 | |grad f(p)|:0.890845571434862 | s:0.01326670131422904 | Last Change: 0.763281\n# 50     | f(x): 2.650064 | |grad f(p)|:0.05536989605165708 | s:0.05306680525691616 | Last Change: 0.081780\n# 75     | f(x): 2.649707 | |grad f(p)|:0.02135638585837997 | s:0.01326670131422904 | Last Change: 0.011590\n# 100    | f(x): 2.649700 | |grad f(p)|:0.0012887575647752057 | s:0.05306680525691616 | Last Change: 0.001745\nAt iteration 109 the algorithm performed a step with a change (2.9063044690733034e-7) less than 1.0e-5.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"And we can again look at the result","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"The result looks as","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: The resulting curve)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"where all control points are evenly spaced and we hence have less acceleration as the final cost compared to the initial one indicates. Note that the cost is not zero, since we always have a tradeoff between approximating the initial junctinons (data points) and minimizing the acceleration.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"S. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\n","category":"page"},{"location":"data/#Data-sets","page":"Data","title":"Data sets","text":"","category":"section"},{"location":"data/#Signals-on-manifolds","page":"Data","title":"Signals on manifolds","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"Modules = [ManoptExamples]\nPages = [\"data/artificial_signals.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"data/#ManoptExamples.Lemniscate-Tuple{Number}","page":"Data","title":"ManoptExamples.Lemniscate","text":"Lemniscate(t::Float; kwargs...)\nLemniscate(n::integer; interval=[0.0, 2π], kwargs...)\n\ngenerate the Lemniscate of Bernoulli as a curve on a manifold, by generating the curve emplying the keyword arguments below.\n\nTo be precise on the manifold M we use the tangent space at p and generate the curve\n\nγ(t) fracasin^2(t) + 1 beginpmatrix cos(t)  cos(t)sin(t) endpmatrix\n\nin the plane spanned by X and Y in the tangent space. Note that this curve is 2π-periodic and a is the half-width of the curve.\n\nTo reproduce the first examples from [BBSW16] as a default, on the sphere p defaults to the North pole.\n\nTHe second variant generates n points equispaced in ìnterval` and calls the first variant.\n\nKeywords\n\nmanifold - (Sphere(2)) the manifold to build the lemniscate on\np        - ([0.0, 0.0, 1.0] on the sphere, `rand(M) else) the center point of the Lemniscate\na        – (π/2.0) half-width of the Lemniscate\nX        – ([1.0, 0.0, 0.0] for the 2-sphere with default p, the first DefaultOrthonormalBasis() vector otherwise) first direction for the plane to define the Lemniscate in, unit vector recommended.\nY        – ([0.0, 1.0, 0.0] if p is the default, the second DefaultOrthonormalBasis() vector otherwise) second direction for the plane to define the Lemniscate in, unit vector orthogonal to X recommended.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_signal","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to ``[BLSW14].\n\nOptional\n\npts: (500) number of points to sample the function\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S1_signal-Tuple{Real}","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of  [BLSW14] for values outside that interval, this Signal is missing.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_slope_signal","page":"Data","title":"ManoptExamples.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the Circle with increasing slope.\n\nOptional\n\npts:    (500) number of points to sample the function.\nslope:  (4.0) initial slope that gets increased afterwards\n\nThis data set was introduced for the numerical examples in [BLSW14]\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_composite_Bezier_curve-Tuple{}","page":"Data","title":"ManoptExamples.artificial_S2_composite_Bezier_curve","text":"artificial_S2_composite_Bezier_curve()\n\nGenerate a composite Bézier curve on the [BG18].\n\nIt consists of 4 egments connecting the points\n\nmathbf d_0 = beginpmatrix 001endpmatrixquad\nmathbf d_1 = beginpmatrix 0-10endpmatrixquad\nmathbf d_2 = beginpmatrix -100endpmatrixtext and \nmathbf d_3 = beginpmatrix 00-1endpmatrix\n\nwhere instead of providing the two center control points explicitly we provide them as velocities from the corresponding points, such thtat we can directly define the curve to be C^1.\n\nWe define\n\nX_0 = fracπ8sqrt2beginpmatrix1-10endpmatrixquad\nX_1 = fracπ4sqrt2beginpmatrix101endpmatrixquad\nX_2 = fracπ4sqrt2beginpmatrix01-1endpmatrixtext and \nX_3 = fracπ8sqrt2beginpmatrix-110endpmatrix\n\nwhere we defined each X_i in T_d_imathbb S^2. We defined three BezierSegments\n\nof cubic Bézier curves as follows\n\nbeginalign*\nb_00 = d_0 quad  b_10 = exp_d_0X_0 quad  b_20 = exp_d_1X_1 quad  b_30 = d_1\nb_01 = d_1 quad  b_11 = exp_d_1(-X_1) quad  b_21 = exp_d_2X_2 quad  b_31 = d_2\nb_02 = d_2 quad  b_11 = exp_d_2(-X_2) quad  b_22 = exp_d_3X_3 quad  b_32 = d_3\nendalign*\n\n\n\n\n\n","category":"method"},{"location":"data/#images-on-manifolds","page":"Data","title":"images on manifolds","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"Modules = [ManoptExamples]\nPages = [\"data/artificial_images.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"data/#ManoptExamples.artificialIn_SAR_image-Tuple{Integer}","page":"Data","title":"ManoptExamples.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\nThis data set was introduced for the numerical examples in [BLSW14].\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S2_rotation_image","page":"Data","title":"ManoptExamples.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\nCreate an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts:       (64) number of pixels along one dimension\nrotations: ((.5,.5)) number of total rotations performed on the axes.\n\nThis dataset was used in the numerical example of Section 5.1 of [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_image","page":"Data","title":"ManoptExamples.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts::Int=64])\n\nGenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts: (64) size of the image in pts×pts pixel.\n\nThis example dataset was used in the numerical example in Section 5.5 of [LNPS17]\n\nIt is based on artificial_S2_rotation_image extended by small whirl patches.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_patch","page":"Data","title":"ManoptExamples.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the pts×pts patch of Sphere(@ref)(2)-valued image data.\n\nThese patches are used within artificial_S2_whirl_image.\n\nOptional Parameters\n\npts: (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image","page":"Data","title":"ManoptExamples.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with a jump of size stepsize.\n\nThis dataset was used in the numerical example of Section 5.2 of [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image2","page":"Data","title":"ManoptExamples.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with right hand side fraction is moved upwards.\n\nThis data set was introduced in the numerical examples of Section of [BPS16]\n\n\n\n\n\n","category":"function"},{"location":"data/#Literature","page":"Data","title":"Literature","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\n","category":"page"},{"location":"examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","page":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Ronny BergmannLaura Weigl 2023-07-02","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For this example we first load the necessary packages.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Robust-PCA/#Computing-a-Robust-PCA","page":"Robust PCA","title":"Computing a Robust PCA","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For a given matrix D  ℝ^dn whose columns represent points in ℝ^d, a matrix p  ℝ^dm is computed for a given dimension m  n: p represents an ONB of ℝ^dm such that the column space of p approximates the points (columns of D), i.e. the vectors D_i as well as possible.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We compute p as a minimizer over the Grassmann manifold of the cost function:","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"beginsplit\nf(p)\n = frac1nsum_i=1^noperatornamedist(D_i operatornamespan(p))\n\n = frac1n sum_i=1^nlVert pp^TD_i - D_irVert\nendsplit","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The output cost represents the average distance achieved with the returned p, an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that f is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter ε.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f_ϵ(p) = frac1n sum_i=1^nℓ_ϵ(lVert pp^mathrmTD_i - D_irVert)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"where ℓ_ϵ(x) = sqrtx^2 + ϵ^2 - ϵ.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts).","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"First, we generate random data. For illustration purposes we take points in mathbb R^2 and m=1, that is we aim to find a robust regression line.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"n = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"M = Grassmann(d,m);","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For the initial matrix p_0 we use classical PCA via singular value decomposition. Thus, we use the first d left singular vectors.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Then, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in Manopt.jl.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Furthermore the cost and gradient are implemented in ManoptExamples.jl. Since these are Huber regularized, both functors have the ϵ as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection pp^T - I, which converts the Euclidean to the Riemannian gradient.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The trust-region method also requires the Hessian Matrix. By using ApproxHessianFiniteDifference using a finite difference scheme we get an approximation of the Hessian Matrix.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We run the procedure several times, where the smoothing parameter ε is reduced iteratively.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ε = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m]","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"2×1 Matrix{Float64}:\n -0.7494248652139397\n  0.6620893983436593","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Let’s generate the cost and gradient we aim to use here","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f = ManoptExamples.RobustPCACost(M, data, ε)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, ε)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([9.537606557855465 1.6583418797018163 … 30.833523701909474 30.512999245062304; -45.34339972619071 -1.7120433539256108 … -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"and check the initial cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f(M, p0)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.430690947905521","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Now we iterate the opimization with reducing ε after every iteration, which we update in f and grad_f.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"q = copy(M, p0)\nεi = ε\nfor i in 1:iterations\n    f.ε = εi\n    grad_f.ε = εi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global εi *= reduction\nend","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"When finally setting ε we can investigate the final cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f.ε = 0.0\nf(M, q)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.412962019408864","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Finally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"fig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Total-Variation/#Total-Variation-Minimization","page":"Total Variation","title":"Total Variation Minimization","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Total-Variation/#Introduction","page":"Total Variation","title":"Introduction","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Total Variation denoising is an optimization problem used to denoise signals and images. The corresponding (Euclidean) objective is often called Rudin-Osher-Fatemi (ROF) model based on the paper [ROF92].","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"This was generalized to manifolds in [WDS14]. In this short example we will look at the ROF model for manifold-valued data, its generalizations, and how they can be solved using Manopt.jl.","category":"page"},{"location":"examples/Total-Variation/#The-manifold-valued-ROF-model","page":"Total Variation","title":"The manifold-valued ROF model","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Generalizing the ROF model to manifolds can be phrased as follows: Given a (discrete) signal on a manifold s = (s_i)_i=1^N in mathbb M^n of length n in mathbb N, we usually assume that this signal might be noisy. For the (Euclidean) ROF model we assume that the noise is Gaussian. Then variational models for denoising usually consist of a data term D(ps) to “stay close to” s and a regularizer R(p). For TV regularization the data term is the squared distance and the regularizer models that without noise, neighboring values are close. We obtain","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"operatorname*argmin_pinmathcal M^n\nf(p)\nqquad\nf(p) = D(ps) + α R(p) = sum_i=1^n d_mathcal M^2(s_ip_i) + αsum_i=1^n-1 d_mathcal M(p_ip_i+1)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"where α  0 is a weight parameter.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"The challenge here is that most classical algorithm, like gradient descent or Quasi Newton, assume the cost f(p) to be smooth such that the gradient exists at every point. In our setting that is not the case since the distacen is not differentiable for any p_i=p_i+1. So we have to use another technique.","category":"page"},{"location":"examples/Total-Variation/#The-Cyclic-Proximal-Point-algorithm","page":"Total Variation","title":"The Cyclic Proximal Point algorithm","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"If the cost consists of a sum of functions, where each of the proximal maps is “easy to evaluate”, for best of cases in closed form, we can “apply the proximal maps in a cyclic fashion” and optain the Cyclic Proximal Point Algorithm [Bac14].","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Both for the distance and the squared distance, we have generic implementations; since this happens in a cyclic manner, there is also always one of the arguments involved in the prox and never both. We can improve the performance slightly by computing all proes in parallel that do not interfer. To be precise we can compute first all proxes of distances in the regularizer that start with an odd index in parallel. Afterwards all that start with an even index.","category":"page"},{"location":"examples/Total-Variation/#The-Optimsation","page":"Total Variation","title":"The Optimsation","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"using Manifolds, Manopt, ManoptExamples, ManifoldDiff\nusing ManifoldDiff: prox_distance\nusing ManoptExamples: prox_Total_Variation\nn = 500 #Signal length\nσ = 0.2 # amount of noise\nα = 0.5# in the TV model","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We define a few colors","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"using Colors, NamedColors, ColorSchemes, Plots, Random\ndata_color = RGBA{Float64}(colorant\"black\")\nlight_color = RGBA{Float64}(colorant\"brightgrey\")\nrecon_color = RGBA{Float64}(colorant\"vibrantorange\")\nnoisy_color = RGBA{Float64}(colorant\"vibrantteal\")","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"And we generate our data on the Circle, since that is easy to plot and nice to compare to the Euclidean case of a real-valued signal.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Random.seed!(23)\nM = Circle()\nN = PowerManifold(M, n)\ndata = ManoptExamples.artificial_S1_signal(n)\ns = [exp(M, d, rand(M; vector_at=d, σ=0.2)) for d in data]\nt = range(0.0, 1.0; length=n)\nscene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=noisy_color,\n    markerstrokecolor=noisy_color,\n    lab=\"noisy\",\n)\nyticks!(\n    [-π, -π / 2, 0, π / 2, π],\n    [raw\"$-\\pi$\", raw\"$-\\frac{\\pi}{2}$\", raw\"$0$\", raw\"$\\frac{\\pi}{2}$\", raw\"$\\pi$\"],\n)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"(Image: )","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"As mentioned above, total variation now minimized different neighbors – while keeping jumps if the are large enough. One notable difference between Euclidean and Cyclic data is, that the y-axis is in our case periodic, hence the first jump is actually not a jump but a “linear increase” that “wraps around” and the second large jump –or third overall– is actually only as small as the second jump.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Defining cost and the proximal maps, which are actually 3 proxes to be precise.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"f(N, p) = ManoptExamples.L2_Total_Variation(N, s, α, p)\nproxes_f = ((N, λ, p) -> prox_distance(N, λ, s, p, 2), (N, λ, p) -> prox_Total_Variation(N, α * λ, p))","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We run the algorithm","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"o = cyclic_proximal_point(\n    N,\n    f,\n    proxes_f,\n    s;\n    λ=i -> π / (2 * i),\n    debug=[\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        :Cost,\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    record=[:Iteration, :Cost, :Change, :Iterate],\n    return_state=true,\n);","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Initial  |  | f(x): 59.187445 | \n# 1000   | λ:0.0015707963267948967 | f(x): 13.963912 | Last Change: 1.773283\n# 2000   | λ:0.0007853981633974483 | f(x): 13.947124 | Last Change: 0.011678\n# 3000   | λ:0.0005235987755982988 | f(x): 13.941538 | Last Change: 0.003907\n# 4000   | λ:0.00039269908169872416 | f(x): 13.938748 | Last Change: 0.001957\n# 5000   | λ:0.0003141592653589793 | f(x): 13.937075 | Last Change: 0.001175\nThe algorithm reached its maximal number of iterations (5000).","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We can see that the cost reduces nicely. Let’s extract the result an the recorded values","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"recon = get_solver_result(o)\nrecord = get_record(o)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We get","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"scene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=light_color,\n    markerstrokecolor=light_color,\n    lab=\"noisy\",\n)\nscatter!(\n    scene,\n    t,\n    recon;\n    markersize=2,\n    markercolor=recon_color,\n    markerstrokecolor=recon_color,\n    lab=\"reconstruction\",\n)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"(Image: )","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Which contains the usual stair casing one expects for TV regularization, but here in a “cyclic manner”","category":"page"},{"location":"examples/Total-Variation/#Outlook","page":"Total Variation","title":"Outlook","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We can generalize the total variation also to a second order total variation. Again intuitively, while TV prefers constant areas, the operatornameTV_2 yields a cost 0 for anything linear, which on manifolds can be generalized to equidistant on a geodesic [BBSW16]. Here we can again derive proximal maps, which for the circle again have a closed form solutoin [BLSW14] but on general manifolds these have again to be approximated.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Another extension for both first and second order TV is to apply this for manifold-valued images S = (S_ij)_ij=1^mn in mathcal M^mn, where the distances in the regularizer are then used in both the first dimension i and the second dimension j in the data.","category":"page"},{"location":"examples/Total-Variation/#Technical-details","page":"Total Variation","title":"Technical details","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"This version of the example was generated with the following package versions.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Pkg.status()","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Status `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [35d6a980] ColorSchemes v3.26.0\n  [5ae59095] Colors v0.12.11\n  [7073ff75] IJulia v1.25.0\n  [8ac3fa9e] LRUCache v1.6.1\n  [d3d80556] LineSearches v7.3.0\n  [af67fdf4] ManifoldDiff v0.3.12\n⌃ [1cead3c2] Manifolds v0.9.20\n  [3362f125] ManifoldsBase v0.15.17\n⌅ [0fc0a36d] Manopt v0.4.69\n  [5b8d5e80] ManoptExamples v0.1.10 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.40.8\n  [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.23\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`","category":"page"},{"location":"examples/Total-Variation/#Literature","page":"Total Variation","title":"Literature","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"examples/Rosenbrock/#The-Rosenbrock-Function","page":"Rosenbrock","title":"The Rosenbrock Function","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Ronny Bergmann 2023-01-03","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"After loading the necessary packages","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Manifolds, Manopt, ManoptExamples\nusing Plots","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We fix the parameters for the 📖 Rosenbrock (where the wikipedia page has a slightly different parameter naming).","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"a = 100.0\nb = 1.0\np0 = [1/10, 2/10]","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which is defined on mathbb R^2, so we need","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"M = ℝ^2","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Euclidean(2; field=ℝ)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and can then generate both the cost and the gradient","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"ManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"For comparison, we look at the initial cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, p0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"4.42","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And to illustrate, we run two small solvers with their default settings as a comparison.","category":"page"},{"location":"examples/Rosenbrock/#Gradient-Descent","page":"Rosenbrock","title":"Gradient Descent","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We start with the gradient descent solver.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Since we need the state anyways to access the record, we also get from the return_state=true a short summary of the solver run.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"From the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.10562873187751265","category":"page"},{"location":"examples/Rosenbrock/#Quasi-Newton","page":"Rosenbrock","title":"Quasi Newton","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We can improve this using the quasi Newton algorithm","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 26 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 2), projections, and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector transport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(DefaultManifold(), 0.0001, 0.999) with keyword arguments\n  * retraction_method = ExponentialRetraction()\n  * vector_transport_method = ParallelTransport()\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 1000: not reached\n    |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And we see it stops far earlier, after 45 Iterations. We again collect the recorded values","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"1.4404666436813376e-18","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and see that the final value is close to the one of the minimizer","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, ManoptExamples.minimizer(f))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.0","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which we also see if we plot the recorded cost.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"fig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","page":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Ronny Bergmann 2023-07-02","category":"page"},{"location":"examples/Riemannian-mean/#Preliminary-Notes","page":"Riemannian Mean","title":"Preliminary Notes","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Each of the example objectives or problems stated in this package should be accompanied by a Quarto notebook that illustrates their usage, like this one.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For this first example, the objective is a very common one, for example also used in the Get started: optimize! tutorial of Manopt.jl.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"The second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"There are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in examples/. If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having ManoptExamples.jl in development mode. this requires that your example does not have any (additional) dependencies beyond the ones ManoptExamples.jl has anyways.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For registered versions of ManoptExamples.jl use the environment of examples/ and – under development – add ManoptExamples.jl in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the examples/ environment again.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Riemannian-mean/#Loading-packages-and-defining-data","page":"Riemannian Mean","title":"Loading packages and defining data","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Loading the necessary packages and defining a data set on a manifold","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nσ = π / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];","category":"page"},{"location":"examples/Riemannian-mean/#Variant-1:-Using-the-functions","page":"Riemannian Mean","title":"Variant 1: Using the functions","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"We can define both the cost and gradient, RiemannianMeanCost and RiemannianMeanGradient!!, respectively. For their mathematical derivation and further explanations, we again refer to Get started: optimize!.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"f = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Then we can for example directly call a gradient descent as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"x1 = gradient_descent(M, f, grad_f, first(data))","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285","category":"page"},{"location":"examples/Riemannian-mean/#Variant-2:-Using-the-objective","page":"Riemannian Mean","title":"Variant 2: Using the objective","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"A shorter way to directly obtain the Manifold objective including these two functions. Here, we want to specify that the objective can do in-place-evaluations using the evaluation=-keyword. The objective can be obtained calling Riemannian_mean_objective as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Together with a manifold, this forms a Manopt Problem, which would usually enable to switch manifolds between solver runs. Here we could for example switch to using Euclidean(3) instead for the same data the objective is build upon.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmp = DefaultManoptProblem(M, rmo)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"This enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s1 = GradientDescentState(M, copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"but we can easily use a conjugate gradient instead","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s2 = ConjugateGradientDescentState(\n    M,\n    copy(M, first(data)),\n    StopAfterIteration(100),\n    ArmijoLinesearch(M),\n    FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868393613136017\n 0.006531541407458413\n 0.7267799052788726","category":"page"},{"location":"examples/RayleighQuotient/#The-Rayleigh-Quotient","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Ronny Bergmann 2024-03-09","category":"page"},{"location":"examples/RayleighQuotient/#Introduction","page":"The Rayleigh Quotient","title":"Introduction","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [Bou23] using the Rayleigh quotient and explores several different ways to use the algorithms from Manopt.jl.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For a symmetric matrix A in mathbb R^ntimes n we consider the 📖 Rayleigh Quotient","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"operatorname*argmin_x in mathbb R^n backslash 0\nfracx^mathrmTAxlVert x rVert^2","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"On the sphere we can omit the denominator and obtain","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f(p) = p^mathrmTApqquad p  𝕊^n-1","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"which by itself we can again continue in the embedding as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"tilde f(x) = x^mathrmTAxqquad x in mathbb R^n","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This cost has the nice feature that at the minimizer p^*inmathbb S^n-1 the function falue f(p^*) is the smalles eigenvalue of A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For the embedded function tilde f the gradient and Hessian can be computed with classical methods as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\ntilde f(x) = 2Ax qquad x  ℝ^n\n\n^2tilde f(x)V = 2AV qquad x V  ℝ^n\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Similarly, cf. Examples 3.62 and 5.27 of [Bou23], the Riemannian gradient and Hessian on the manifold mathcal M = mathbb S^n-1 are given by","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\noperatornamegrad f(p) = 2Ap - 2(p^mathrmTAp)*pqquad p  𝕊^n-1\n\noperatornameHess f(p)X =  2AX - 2(p^mathrmTAX)p - 2(p^mathrmTAp)Xqquad p  𝕊^n-1 X in T_p𝕊^n-1\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first generate an example martrx A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random\nRandom.seed!(42)\nn = 500\nA = Symmetric(randn(n, n) / n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And the manifolds","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"M = Sphere(n-1)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Sphere(499, ℝ)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"E = get_embedding(M)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean(500; field=ℝ)","category":"page"},{"location":"examples/RayleighQuotient/#Setup-the-corresponding-functions","page":"The Rayleigh Quotient","title":"Setup the corresponding functions","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Since RayleighQuotientCost, RayleighQuotientGrad!!, and RayleighQuotientHess!! are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute f is called with M as their first argument and tilde f if called with E.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We instantiate","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f = ManoptExamples.RayleighQuotientCost(A)\ngrad_f = ManoptExamples.RayleighQuotientGrad!!(A)\nHess_f = ManoptExamples.RayleighQuotientHess!!(A)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"the suffix !! also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"p0 = [1.0, zeros(n-1)...]\nX = zero_vector(M, p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"we can both call","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Y = grad_f(M, p0)  # Allocates memory\ngrad_f(M, X, p0)    # Computes in place of X and returns the result in X.\nnorm(M, p0, X-Y)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"0.0","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Now we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"First of all let’s construct the actual result – since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"λ = min(eigvals(A)...)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"-0.08967721009388108","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-on-gradient-information","page":"The Rayleigh Quotient","title":"A Solver based on gradient information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient nabla tilde f. We can “tell” the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally, Manopt.jl then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf. e.g. this tutorial section or Section 3.8 or more precisely Example 3.62 in [Bou23].","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"But instead of diving into all the tecnical details, we can just specify objective_type=:Euclidean to trigger the conversion. We start with a simple gradient descent","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"s = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n    return_state=true,\n)\nq1 = get_solver_result(s)\ns","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.000727\n# 50    f(x): -0.088415|grad f(p)|:0.004530500043902619\n# 100   f(x): -0.089097|grad f(p)|:0.004589417101266096\n# 150   f(x): -0.089530|grad f(p)|:0.0026028331895358247\n# 200   f(x): -0.089650|grad f(p)|:0.0012359084298719039\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 50]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"From the final cost we can already see that q1 is an eigenvector to the smallest eigenvalue we obtaines above.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And we can compare this to running with the Riemannian gradient, since the RayleighQuotientGrad!! returns this one as well, when just called with the sphere as first Argument, we just have to remove the objective_type.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q2 = gradient_descent(M, f, grad_f, p0;\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n)\n#Test that both are the same\nisapprox(M, q1,q2)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.000727\n# 50    f(x): -0.088415|grad f(p)|:0.004530500043902567\n# 100   f(x): -0.089097|grad f(p)|:0.004589417101266063\n# 150   f(x): -0.089530|grad f(p)|:0.002602833189535808\n# 200   f(x): -0.089650|grad f(p)|:0.0012359084298719097\n\ntrue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We can also benchmark both","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 23 samples with 1 evaluation.\n Range (min … max):  218.625 ms … 280.105 ms  ┊ GC (min … max): 3.46% … 3.93%\n Time  (median):     220.184 ms               ┊ GC (median):    3.49%\n Time  (mean ± σ):   225.401 ms ±  13.335 ms  ┊ GC (mean ± σ):  3.54% ± 0.27%\n\n  ▆█                                                             \n  ███▄▁▄▁▁▁▁▁▁▁▁▁▁█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄ ▁\n  219 ms           Histogram: frequency by time          280 ms <\n\n Memory estimate: 1.13 GiB, allocs estimate: 3607.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 155 samples with 1 evaluation.\n Range (min … max):  29.784 ms … 165.193 ms  ┊ GC (min … max): 0.00% … 81.19%\n Time  (median):     30.166 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   32.288 ms ±  11.060 ms  ┊ GC (mean ± σ):  3.48% ±  7.06%\n\n  █                                                             \n  █▄▃▆▃▃▃▃▂▂▁▁▁▁▂▁▁▁▁▃▅▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂ ▂\n  29.8 ms         Histogram: frequency by time           46 ms <\n\n Memory estimate: 11.38 MiB, allocs estimate: 3000.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"From these results we see, that the conversion from the Euclidean to the Riemannian gradient does require a small amount of effort and hence reduces the performance slighly. Still, if the Euclidean Gradient is easier to compute or already available, this is in terms of coding the faster way. Finally this is a tradeoff between derivation and implementation efforts for the Riemannian gradient and a slight performance reduction when using the Euclidean one.","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-(also)-on-(approximate)-Hessian-information","page":"The Rayleigh Quotient","title":"A Solver based (also) on (approximate) Hessian information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To also involve the Hessian, we consider the trust regions solver with three cases:","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean, approximating the Hessian\nEuclidean, providing the Hessian\nRiemannian, providing the Hessian but also using in-place evaluations.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.000727\n# 10    f(x): -0.088106|grad f(p)|:0.01903913659588686\n# 20    f(x): -0.089023|grad f(p)|:0.007792334296299116\n# 30    f(x): -0.089501|grad f(p)|:0.008034300330026467\n# 40    f(x): -0.089842|grad f(p)|:0.008125526728200166\n# 50    f(x): -0.089890|grad f(p)|:0.0031244752821335416\n# 60    f(x): -0.089925|grad f(p)|:0.0029682862637714163\n# 70    f(x): -0.089962|grad f(p)|:0.002811722437216778\n# 80    f(x): -0.089997|grad f(p)|:0.0026658493010157363\n# 90    f(x): -0.090032|grad f(p)|:0.0025418974797659266\n# 100   f(x): -0.090067|grad f(p)|:0.0024485809550738955\n# 110   f(x): -0.090108|grad f(p)|:0.0023894008071780747\n# 120   f(x): -0.090155|grad f(p)|:0.002362317662908117\n# 130   f(x): -0.090208|grad f(p)|:0.0023611301647631484\n# 140   f(x): -0.090262|grad f(p)|:0.00237797866404072\n# 150   f(x): -0.090314|grad f(p)|:0.002405563029627607\n# 160   f(x): -0.090362|grad f(p)|:0.002438250821406204\n# 170   f(x): -0.090404|grad f(p)|:0.002472221074327323\n# 180   f(x): -0.090441|grad f(p)|:0.0025051377726827166\n# 190   f(x): -0.090472|grad f(p)|:0.002535721310831389\n# 200   f(x): -0.090498|grad f(p)|:0.0025633813700434637\n# 210   f(x): -0.090513|grad f(p)|:0.0025832821804127513\n# 220   f(x): -0.090513|grad f(p)|:0.0025832821804127513\n# 230   f(x): -0.090513|grad f(p)|:0.0025832821792817493\n# 240   f(x): -0.090513|grad f(p)|:0.0025832821770197098\n# 250   f(x): -0.090513|grad f(p)|:0.0025832821747576924\n# 260   f(x): -0.090513|grad f(p)|:0.002583282172495683\n# 270   f(x): -0.090513|grad f(p)|:0.0025832821702336567\n# 280   f(x): -0.090513|grad f(p)|:0.002583282167971658\n# 290   f(x): -0.090513|grad f(p)|:0.002583282165709656\n# 300   f(x): -0.090513|grad f(p)|:0.002583282163447637\n# 310   f(x): -0.090513|grad f(p)|:0.0025832821611855928\n# 320   f(x): -0.090513|grad f(p)|:0.0025832821589235814\n# 330   f(x): -0.090513|grad f(p)|:0.002583282156661572\n# 340   f(x): -0.090513|grad f(p)|:0.0025832821543995727\n# 350   f(x): -0.090513|grad f(p)|:0.002583282152137569\n# 360   f(x): -0.090513|grad f(p)|:0.0025832821498755487\n# 370   f(x): -0.090513|grad f(p)|:0.0025832821476135036\n# 380   f(x): -0.090513|grad f(p)|:0.0025832821453515035\n# 390   f(x): -0.090513|grad f(p)|:0.0025832821430894675\n# 400   f(x): -0.090513|grad f(p)|:0.0025832821408274405\n# 410   f(x): -0.090513|grad f(p)|:0.002583282138565445\n# 420   f(x): -0.090513|grad f(p)|:0.002583282136303441\n# 430   f(x): -0.090513|grad f(p)|:0.00258328213404143\n# 440   f(x): -0.090513|grad f(p)|:0.002583282131779385\n# 450   f(x): -0.090513|grad f(p)|:0.0025832821295174104\n# 460   f(x): -0.090513|grad f(p)|:0.002583282127255372\n# 470   f(x): -0.090513|grad f(p)|:0.002583282124993372\n# 480   f(x): -0.090513|grad f(p)|:0.0025832821227313313\n# 490   f(x): -0.090513|grad f(p)|:0.0025832821204693065\n# 500   f(x): -0.090513|grad f(p)|:0.002583282118207321\n# 510   f(x): -0.090513|grad f(p)|:0.0025832821159453034\n# 520   f(x): -0.090513|grad f(p)|:0.0025832821136832665\n# 530   f(x): -0.090513|grad f(p)|:0.0025832821114212673\n# 540   f(x): -0.090513|grad f(p)|:0.002583282109159243\n# 550   f(x): -0.090513|grad f(p)|:0.002583282106897217\n# 560   f(x): -0.090513|grad f(p)|:0.002583282104635213\n# 570   f(x): -0.090513|grad f(p)|:0.0025832821023731955\n# 580   f(x): -0.090513|grad f(p)|:0.0025832821001112094\n# 590   f(x): -0.090513|grad f(p)|:0.002583282097849167\n# 600   f(x): -0.090513|grad f(p)|:0.0025832820955871503\n# 610   f(x): -0.090513|grad f(p)|:0.0025832820933251325\n# 620   f(x): -0.090513|grad f(p)|:0.002583282091063122\n# 630   f(x): -0.090513|grad f(p)|:0.0025832820888010873\n# 640   f(x): -0.090513|grad f(p)|:0.0025832820865390785\n# 650   f(x): -0.090513|grad f(p)|:0.0025832820842770442\n# 660   f(x): -0.090513|grad f(p)|:0.0025832820820150576\n# 670   f(x): -0.090513|grad f(p)|:0.0025832820797530767\n# 680   f(x): -0.090513|grad f(p)|:0.0025832820774910523\n# 690   f(x): -0.090513|grad f(p)|:0.0025832820752290362\n# 700   f(x): -0.090513|grad f(p)|:0.002583282072966992\n# 710   f(x): -0.090513|grad f(p)|:0.002583282070704973\n# 720   f(x): -0.090513|grad f(p)|:0.0025832820684429532\n# 730   f(x): -0.090513|grad f(p)|:0.002583282066180946\n# 740   f(x): -0.090513|grad f(p)|:0.0025832820639189306\n# 750   f(x): -0.090513|grad f(p)|:0.0025832820616569214\n# 760   f(x): -0.090513|grad f(p)|:0.00258328205939488\n# 770   f(x): -0.090513|grad f(p)|:0.002583282057132884\n# 780   f(x): -0.090513|grad f(p)|:0.0025832820548708406\n# 790   f(x): -0.090513|grad f(p)|:0.002583282052608873\n# 800   f(x): -0.090513|grad f(p)|:0.002583282050346837\n# 810   f(x): -0.090513|grad f(p)|:0.0025832820480848214\n# 820   f(x): -0.090513|grad f(p)|:0.0025832820458228205\n# 830   f(x): -0.090513|grad f(p)|:0.0025832820435608087\n# 840   f(x): -0.090513|grad f(p)|:0.0025832820412987944\n# 850   f(x): -0.090513|grad f(p)|:0.0025832820390367726\n# 860   f(x): -0.090513|grad f(p)|:0.002583282036774768\n# 870   f(x): -0.090513|grad f(p)|:0.002583282034512706\n# 880   f(x): -0.090513|grad f(p)|:0.002583282032250709\n# 890   f(x): -0.090513|grad f(p)|:0.0025832820299886896\n# 900   f(x): -0.090513|grad f(p)|:0.002583282027726701\n# 910   f(x): -0.090513|grad f(p)|:0.002583282025464682\n# 920   f(x): -0.090513|grad f(p)|:0.0025832820232026517\n# 930   f(x): -0.090513|grad f(p)|:0.002583282020940619\n# 940   f(x): -0.090513|grad f(p)|:0.0025832820186786334\n# 950   f(x): -0.090513|grad f(p)|:0.002583282016416595\n# 960   f(x): -0.090513|grad f(p)|:0.0025832820141545986\n# 970   f(x): -0.090513|grad f(p)|:0.0025832820118925717\n# 980   f(x): -0.090513|grad f(p)|:0.0025832820096305525\n# 990   f(x): -0.090513|grad f(p)|:0.002583282007368543\n# 1000  f(x): -0.090513|grad f(p)|:0.0025832820051065217","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any struct is considered to (eventually) be the also optional starting point. For space reasons, let’s also shorten the debug print to only iterations 7 and 14.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q4 = trust_regions(M, f, grad_f, (E, p, X) -> Hess_f(E, p, X), p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.000727\n# 10    f(x): -0.089673|grad f(p)|:0.0033633987039373655","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q5 = trust_regions(M, f, grad_f, (M, Y, p, X) -> Hess_f(M, Y, p, X), p0;\n    evaluation=InplaceEvaluation(),\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.000727\n# 10    f(x): -0.089673|grad f(p)|:0.00336339870393737","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s also here compare them in benchmarks. Let’s here compare all variants in their (more performant) in-place versions.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $p0;\n  objective_type=:Euclidean,\n  evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 10 samples with 1 evaluation.\n Range (min … max):  491.065 ms … 539.773 ms  ┊ GC (min … max): 3.15% … 3.49%\n Time  (median):     501.502 ms               ┊ GC (median):    3.09%\n Time  (mean ± σ):   502.975 ms ±  14.496 ms  ┊ GC (mean ± σ):  3.15% ± 0.13%\n\n  █▁▁       ▁    █    █                                       ▁  \n  ███▁▁▁▁▁▁▁█▁▁▁▁█▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  491 ms           Histogram: frequency by time          540 ms <\n\n Memory estimate: 1.97 GiB, allocs estimate: 29451.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -> Hess_f(E, Y, p, X)), $p0;\n  evaluation=InplaceEvaluation(),\n  objective_type=:Euclidean\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 329 samples with 1 evaluation.\n Range (min … max):  12.743 ms … 155.346 ms  ┊ GC (min … max): 0.00% … 87.86%\n Time  (median):     14.227 ms               ┊ GC (median):    3.43%\n Time  (mean ± σ):   15.215 ms ±   8.152 ms  ┊ GC (mean ± σ):  6.37% ±  5.79%\n\n  ▇  █▇▇ ▅▅ ▇                                                   \n  █▇▁███▁██▇█▇▄▁▄▁▁▆▁▁▁▁▄▁▆▁▁▁▁▁▆▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▆▄▄ ▇\n  12.7 ms       Histogram: log(frequency) by time      29.9 ms <\n\n Memory estimate: 37.42 MiB, allocs estimate: 2788.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -> Hess_f(M, Y, p, X)), $p0;\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 541 samples with 1 evaluation.\n Range (min … max):  8.376 ms … 146.597 ms  ┊ GC (min … max): 0.00% … 92.39%\n Time  (median):     8.558 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   9.249 ms ±   6.065 ms  ┊ GC (mean ± σ):  4.63% ±  5.52%\n\n  ▅█▆▂     ▄▄▃                                                 \n  ████▅▅▆▄█████▄▄▁▄▄▄▁▄▁▁▄▁▄▄▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▆▅▄▄▁▁▄ ▇\n  8.38 ms      Histogram: log(frequency) by time      13.7 ms <\n\n Memory estimate: 10.75 MiB, allocs estimate: 2767.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Of course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q1, q) for q ∈ [q2,q3] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 4.471485799821605e-15\n 0.048047538209352994","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q3, q) for q ∈ [q4,q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 0.08269488012454579\n 0.08269488012454579","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Which we can also see in the final cost, comparing it to the Eigenvalue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[f(M, q) - λ for q ∈ [q1, q2, q3, q4, q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"5-element Vector{Float64}:\n  2.76900562450888e-5\n  2.769005624428389e-5\n -0.000836208332542443\n  3.191891195797325e-16\n  3.191891195797325e-16","category":"page"},{"location":"examples/RayleighQuotient/#Summary","page":"The Rayleigh Quotient","title":"Summary","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute.","category":"page"},{"location":"examples/RayleighQuotient/#Literature","page":"The Rayleigh Quotient","title":"Literature","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"N. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\n","category":"page"},{"location":"helpers/error_measures/#Error-measures","page":"Error measures","title":"Error measures","text":"","category":"section"},{"location":"helpers/error_measures/","page":"Error measures","title":"Error measures","text":"Modules = [ManoptExamples]\nPages = [\"ErrorMeasures.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"helpers/error_measures/#ManoptExamples.mean_average_error-Tuple{ManifoldsBase.AbstractManifold, Any, Any}","page":"Error measures","title":"ManoptExamples.mean_average_error","text":"mean_average_error(M,x,y)\n\nCompute the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"method"},{"location":"helpers/error_measures/#ManoptExamples.mean_squared_error-Union{Tuple{mT}, Tuple{mT, Any, Any}} where mT<:ManifoldsBase.AbstractManifold","page":"Error measures","title":"ManoptExamples.mean_squared_error","text":"mean_squared_error(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"method"},{"location":"#Welcome-to-ManoptExample.jl","page":"Home","title":"Welcome to ManoptExample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ManoptExamples.ManoptExamples","category":"page"},{"location":"#ManoptExamples.ManoptExamples","page":"Home","title":"ManoptExamples.ManoptExamples","text":"🏔️⛷️ ManoptExamples.jl – A collection of research and tutorial example problems for Manopt.jl\n\n📚 Documentation: juliamanifolds.github.io/ManoptExamples.jl\n📦 Repository: github.com/JuliaManifolds/ManoptExamples.jl\n💬 Discussions: github.com/JuliaManifolds/ManoptExamples.jl/discussions\n🎯 Issues: github.com/JuliaManifolds/ManoptExamples.jl/issues\n\n\n\n\n\n","category":"module"},{"location":"","page":"Home","title":"Home","text":"This package provides a set of example tasks for Manopt.jl based on either generic manifolds from the ManifoldsBase.jl interface or specific manifolds from Manifolds.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each example usually consists of","category":"page"},{"location":"","page":"Home","title":"Home","text":"a cost function and additional objects, like the gradient or proximal maps, see objectives\nan example explaining how to use these, see examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Helping functions that are used in one or more examples can be found in the section of functions in the menu.","category":"page"},{"location":"examples/H2-Signal-TV/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM,-and-the-CPPA-for-denoising-a-signal-on-the-hyperbolic-space","page":"Hyperbolic Signal Denoising","title":"A comparison of the RCBM with the PBA, the SGM, and the CPPA for denoising a signal on the hyperbolic space","text":"","category":"section"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Hajg Jasa 6/27/24","category":"page"},{"location":"examples/H2-Signal-TV/#Introduction","page":"Hyperbolic Signal Denoising","title":"Introduction","text":"","category":"section"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FO98], to denoise an artificial signal on the Hyperbolic space mathcal H^2. This example reproduces the results from [BHJ24], Section 5.2.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"page"},{"location":"examples/H2-Signal-TV/#The-Problem","page":"Hyperbolic Signal Denoising","title":"The Problem","text":"","category":"section"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Let mathcal M = mathcal H^2 be the 2-dimensional hyperbolic space and let p q in mathcal M^n be two manifold-valued signals, for n in mathbb N. Let f colon mathcal M to mathbb R be defined by","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"    f_q (p)\n    =\n    frac1n\n    left(\n    frac12 sum_i = 1^n mathrmdist(p_i q_i)^2\n    +\n    alpha operatornameTV(p)\n    right)\n    ","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"where operatornameTV(p), is the total variation term given by","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"    operatornameTV(p)\n    =\n    sum_i = 1^n-1 mathrmdist(p_i p_i+1)\n    ","category":"page"},{"location":"examples/H2-Signal-TV/#Numerical-Experiment","page":"Hyperbolic Signal Denoising","title":"Numerical Experiment","text":"","category":"section"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"We initialize the experiment parameters, as well as some utility functions.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Random.seed!(33)\nn = 496 # (this is so that n equals the actual length of the artificial signal)\nσ = 0.1 # Noise parameter\nα = 0.05 # TV parameter\natol = 1e-8\nk_max = 0.0\nmax_iters = 15000\n#\n# Colors\ndata_color = RGBA{Float64}(colorant\"#BBBBBB\")\nnoise_color = RGBA{Float64}(colorant\"#33BBEE\") # Tol Vibrant Teal\nresult_color = RGBA{Float64}(colorant\"#EE7733\") # Tol Vibrant Orange","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"function artificial_H2_signal(\n    pts::Integer=100; a::Real=0.0, b::Real=1.0, T::Real=(b - a) / 2\n)\n    t = range(a, b; length=pts)\n    x = [[s, sign(sin(2 * π / T * s))] for s in t]\n    y = [\n        [x[1]]\n        [\n            x[i] for\n            i in 2:(length(x) - 1) if (x[i][2] != x[i + 1][2] || x[i][2] != x[i - 1][2])\n        ]\n        [x[end]]\n    ]\n    y = map(z -> Manifolds._hyperbolize(Hyperbolic(2), z), y)\n    data = []\n    geodesics = []\n    l = Int(round(pts * T / (2 * (b - a))))\n    for i in 1:2:(length(y) - 1)\n        append!(\n            data,\n            shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n        )\n        if i + 2 ≤ length(y) - 1\n            append!(\n                geodesics,\n                shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n            )\n            append!(\n                geodesics,\n                shortest_geodesic(\n                    Hyperbolic(2), y[i + 1], y[i + 2], range(0.0, 1.0; length=l)\n                ),\n            )\n        end\n    end\n    return data, geodesics\nend\nfunction matrixify_Poincare_ball(input)\n    input_x = []\n    input_y = []\n    for p in input\n        push!(input_x, p.value[1])\n        push!(input_y, p.value[2])\n    end\n    return hcat(input_x, input_y)\nend","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"We now fix the data for the experiment…","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"H = Hyperbolic(2)\ndata, geodesics = artificial_H2_signal(n; a=-6.0, b=6.0, T=3)\nHn = PowerManifold(H, NestedPowerRepresentation(), length(data))\nnoise = map(p -> exp(H, p, rand(H; vector_at=p, σ=σ)), data)\np0 = noise\ndiameter = floatmax(Float64)","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"… As well as objective, subdifferential, and proximal map.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"function f(M, p)\n    return 1 / length(data) *\n           (1 / 2 * distance(M, data, p)^2 + α * ManoptExamples.Total_Variation(M, p))\nend\ndomf(M, p) = distance(M, p, p0) < diameter / 2 ? true : false\nfunction ∂f(M, p)\n    return 1 / length(data) * (\n        ManifoldDiff.grad_distance(M, data, p) +\n        α * ManoptExamples.subgrad_Total_Variation(M, p; atol=atol)\n    )\nend\nproxes = (\n    (M, λ, p) -> ManifoldDiff.prox_distance(M, λ, data, p, 2),\n    (M, λ, p) -> ManoptExamples.prox_Total_Variation(M, α * λ, p),\n)","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"We can now plot the initial setting.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"global ball_scene = plot()\nif export_orig\n    ball_data = convert.(PoincareBallPoint, data)\n    ball_noise = convert.(PoincareBallPoint, noise)\n    ball_geodesics = convert.(PoincareBallPoint, geodesics)\n    plot!(ball_scene, H, ball_data; geodesic_interpolation=100, label=\"Geodesics\")\n    plot!(\n        ball_scene,\n        H,\n        ball_data;\n        markercolor=data_color,\n        markerstrokecolor=data_color,\n        label=\"Data\",\n    )\n    plot!(\n        ball_scene,\n        H,\n        ball_noise;\n        markercolor=noise_color,\n        markerstrokecolor=noise_color,\n        label=\"Noise\",\n    )\n    matrix_data = matrixify_Poincare_ball(ball_data)\n    matrix_noise = matrixify_Poincare_ball(ball_noise)\n    matrix_geodesics = matrixify_Poincare_ball(ball_geodesics)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-data.csv\"),\n        DataFrame(matrix_data, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-noise.csv\"),\n        DataFrame(matrix_noise, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-geodesics.csv\"),\n        DataFrame(matrix_geodesics, :auto);\n        header=[\"x\", \"y\"],\n    )\n    display(ball_scene)\nend","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"(Image: )","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"We introduce some keyword arguments for the solvers we will use in this experiment","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"rcbm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.8f \"),\n        (:ξ, \"ξ: %1.16f \"),\n        (:ε, \"ε: %1.16f \"),\n        :WarnBundle,\n        :Stop,\n        1000,\n        \"\\n\",\n        ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) |                                   StopAfterIteration(max_iters),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(max_iters),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) |\n                           StopAfterIteration(max_iters),\n]\ncppa_kwargs = [\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenChangeLess(atol)\n    ),\n    :debug => [\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        (:Cost, \"F(p): %1.16f \"),\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\ncppa_bm_kwargs = [\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenChangeLess(atol)\n    ),\n]","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Finally, we run the optimization algorithms…","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"rcbm = convex_bundle_method(Hn, f, ∂f, p0; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(Hn, f, ∂f, p0; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(Hn, f, ∂f, p0; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm)\n#\ncppa = cyclic_proximal_point(Hn, f, proxes, p0; cppa_kwargs...)\ncppa_result = get_solver_result(cppa)\ncppa_record = get_record(cppa)","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"… And we benchmark their performance.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"if benchmarking\n    pba_bm = @benchmark proximal_bundle_method($Hn, $f, $∂f, $p0; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($Hn, $f, $∂f, $p0; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($Hn, $f, $∂f, $p0; $sgm_bm_kwargs...)\n    cppa_bm = @benchmark cyclic_proximal_point($Hn, $f, $proxes, $p0; $cppa_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\", \"CPPA\"]\n    records = [rcbm_record, pba_record, sgm_record, cppa_record]\n    results = [rcbm_result, pba_result, sgm_result, cppa_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n        median(cppa_bm).time * 1e-9,\n    ]\n    #\n    global B = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records],\n        [distance(Hn, data, result) / length(data) for result in results];\n        dims=2,\n    )\n    #\n    global header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\", \"Error\"]\n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            A = cat(first.(record), [r[2] for r in record]; dims=2)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(A, :auto);\n                header=[\"i\", \"cost\"],\n            )\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(B, :auto);\n            header=header,\n        )\n    end\nend","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"We can take a look at how the algorithms compare to each other in their performance with the following table…","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Algorithm Iterations Time (s) Objective Error\nRCBM 4017 67.1689 0.00179287 0.000331751\nPBA 14807 108.278 0.00181956 0.000440844\nSGM 15000 107.933 0.00179154 0.000330336\nCPPA 15000 97.83 0.00179276 0.000332292","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"Lastly, we plot the results.","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"if export_result\n    # Convert hyperboloid points to Poincaré ball points\n    ball_b = convert.(PoincareBallPoint, rcbm_result)\n    ball_p = convert.(PoincareBallPoint, pba_result)\n    ball_s = convert.(PoincareBallPoint, sgm_result)\n    ball_c = convert.(PoincareBallPoint, cppa_result)\n    #\n    # Plot results\n    plot!(\n        ball_scene,\n        H,\n        ball_b;\n        markercolor=result_color,\n        markerstrokecolor=result_color,\n        label=\"Convex Bundle Method\",\n    )\n    #\n    # Write csv files\n    matrix_b = matrixify_Poincare_ball(ball_b)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-bundle_optimum.csv\"),\n        DataFrame(matrix_b, :auto);\n        header=[\"x\", \"y\"],\n    )\n    #\n    # Suppress some plots for clarity, since they are visually indistinguishable\n    # plot!(ball_scene, H, ball_p; label=\"Proximal Bundle Method\")\n    # plot!(ball_scene, H, ball_s; label=\"Subgradient Method\")\n    # plot!(ball_scene, H, ball_c; label=\"CPPA\")\n    display(ball_scene)\nend","category":"page"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"(Image: )","category":"page"},{"location":"examples/H2-Signal-TV/#Literature","page":"Hyperbolic Signal Denoising","title":"Literature","text":"","category":"section"},{"location":"examples/H2-Signal-TV/","page":"Hyperbolic Signal Denoising","title":"Hyperbolic Signal Denoising","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"page"}]
}
