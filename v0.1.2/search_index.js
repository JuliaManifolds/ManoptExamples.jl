var documenterSearchIndex = {"docs":
[{"location":"examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","page":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Ronny Bergmann 2023-11-06","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Introduction","page":"Frank Wolfe comparison","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"In this example we compare the Difference of Convex Algprithm (DCA) (Bergmann et al., 2023) with the Frank-Wolfe Algorithm, which was introduced in (Weber and Sra, 2022). This example reproduces the results from (Bergmann et al., 2023), Section 7.3.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We consider the collowing constraint maximimization problem of the Fréchet mean on the symmetric positive definite matrices mathcal P(n) with the affine invariant metric. Let q_1ldotsq_m in mathcal P(n) be a set of points and mu_1ldotsmu_m be a set of weights, such that they sum to one. We consider then","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmax_pinmathcal C  h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"h(p) =\nsum_j=1^m mu_j d^2(pq_i)\nquad text where \nd^2(pq_i) = operatornametrbigl(\n  log^2(p^-frac12q_jp^-frac12)\nbig)\nqquadtextandqquad\nmathcal C =  pin mathcal M  bar Lpreceq p preceq bar U ","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"for a lower bound L and an upper bound U for the matrices in the positive definite sense A preceq B Leftrightarrow (B-A) is positive semi-definite","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"When every one of the weights mu_1 ldots mu_m are equal, this function h is known as the of the set q_1 dots q_m.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And for our example we set","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Random.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use as lower and upper bound the arithmetic and geometric mean L and U, respectively.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"L = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As a starting point, the Frank-Wolfe algorithm requires a feasible point. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p0 = (L+U)/2","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can check that it is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","page":"Frank Wolfe comparison","title":"Common Functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Given p in mathcal M, X in T_pmathcal M on the symmetric positive definite matrices M, this method computes the closed form solution to","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_qin  mathcal C langle X log_p qrangle\n  = operatorname*argmin_qin  mathcal C operatornametr(Slog(YqY))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"where mathcal C =  q  L preceq q preceq U , S = p^-12Xp^-12, and Y=p^-12.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The solution is given by Z=X^-1Qbigl( P^mathrmT-operatornamesgn(D)_+P+hatLbigr)Q^mathrmTX^-1,@ where S=QDQ^mathrmT is a diagonalization of S, hatU-hatL=P^mathrmTP with hatL=Q^mathrmTXLXQ and hatU=Q^mathrmTXUXQ, where -mboxsgn(D)_+ is the diagonal matrix","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatornamediagbigl(\n  -operatornamesgn(d_11)_+ ldots -operatornamesgn(d_nn)_+\nbigr)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and D=(d_ij).","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closeed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","page":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use g(p) = iota_mathcal C(p) as the indicator funtion of the set mathcal C. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"So we can first check that p0 is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"g(p0,L,U) == 0.0","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"true","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Now setting","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pinmathcal M g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We look for a maximum of h, where g is minimal, i.e. g(p) is zero or in other words p in mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The gradient of h can also be implemented in closed form as","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"grad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can further define the cost, which will just be +infty outside of mathcal C. We define","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Here we can omit the gradient of g in the definition of operatornamegrad f, since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As the last step, we can provide the closed form solver for the DC sub problem given at iteration k by","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pin mathcal C\n  biglangle -operatornamegrad h(p^(k)) exp^-1_p^(k)pbigrangle","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Which we con compute","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For safety, we might want to avoid ending up at the boundary of mathcal C. That is we reduce the distance we walk towards the solution q a bit.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    α = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= α\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($α)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","page":"Frank Wolfe comparison","title":"The DoC solver run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s compare both methods when they have the same stopping criteria","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial F(p): -0.77661458292831\n# 30       F(p): -0.78442445025558 |Δp|: 0.05499863340233  |grad f(p)|: 0.17698758  |Δgrad f(p)|: 0.17568455\nAt iteration 39 the change of the gradient (1.5841755142063383e-13) was less than 1.0e-9.\n 18.917838 seconds (15.81 M allocations: 1.717 GiB, 4.59% gc time, 83.49% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 39 iterations\n\n## Parameters\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 300:  not reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\n\", 30]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s extract the final point and look at its cost","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844245126697607","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As well as whether (and how well) it is feasible, that is the following values should all be larger than zero.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (4.794825832759285e-13, 0.06692017412921099)\n (7.230668033332052e-6, 0.06701531165157007)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For the statistics we extract the recordings from the state","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","page":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For Frank wolfe, the cost is just defined as -h(p) but the minimisation is constraint to mathcal C, which is enfored by the oracle.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","page":"Frank Wolfe comparison","title":"The FW Solver Run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Similarly we can run the Frank-Wolfe algorithm with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial F(x): -0.776615\n# 2000     F(x): -0.784420 |Δp|: 0.04611942377531  |grad f(p)|: 0.17693408  |Δgrad f(p)|: 0.17555618\n# 4000     F(x): -0.784421 |Δp|: 0.00372201631969  |grad f(p)|: 0.17694619  |Δgrad f(p)|: 0.00749427\n# 6000     F(x): -0.784422 |Δp|: 0.00205683506768  |grad f(p)|: 0.17695204  |Δgrad f(p)|: 0.00414088\n# 8000     F(x): -0.784422 |Δp|: 0.00140675676249  |grad f(p)|: 0.17695565  |Δgrad f(p)|: 0.00283200\n# 10000    F(x): -0.784422 |Δp|: 0.00106177438594  |grad f(p)|: 0.17695815  |Δgrad f(p)|: 0.00213746\nThe algorithm reached its maximal number of iterations (10000).\n306.349726 seconds (55.92 M allocations: 93.626 GiB, 3.79% gc time, 0.36% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stepsize\nDecreasingStepsize(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2)\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 10000:    reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\n\", 2000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we take a look at this result as well","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844220281765067","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And its feasibility","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (4.914485568254976e-10, 0.06659173821656042)\n (3.245654983246501e-5, 0.06713970236151023)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Statistics","page":"Frank Wolfe comparison","title":"Statistics","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We extract the recorded values","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"# DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And let’s plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\")","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"This indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"On the other hand, Frank-Wolfe still has not reached this level function value after 10^4 iterations.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Literature","page":"Frank Wolfe comparison","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Bergmann, R., Ferreira, O. P., Santos, E. M., et al. (2023) The difference of convex algorithm on Hadamard manifolds. arXiv:2112.05250. arXiv. Available at: http://arxiv.org/abs/2112.05250.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Weber, M. and Sra, S. (2022) Riemannian optimization via frank-wolfe methods. Mathematical Programming. Springer Science and Business Media LLC. DOI: 10.1007/s10107-022-01840-5.","category":"page"},{"location":"examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","page":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Ronny Bergmann 2023-07-02","category":"page"},{"location":"examples/Riemannian-mean/#Preliminary-Notes","page":"Riemannian Mean","title":"Preliminary Notes","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Each of the example objectives or problems stated in this package should be accompanied by a Quarto notebook that illustrates their usage, like this one.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For this first example, the objective is a very common one, for example also used in the Get Started: Optimize! tutorial of Manopt.jl.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"The second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"There are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in examples/. If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having ManoptExamples.jl in development mode. this requires that your example does not have any (additional) dependencies beyond the ones ManoptExamples.jl has anyways.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For registered versions of ManoptExamples.jl use the environment of examples/ and – under development – add ManoptExamples.jl in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the examples/ environment again.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Riemannian-mean/#Loading-packages-and-defining-data","page":"Riemannian Mean","title":"Loading packages and defining data","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Loading the necessary packages and defining a data set on a manifold","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nσ = π / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];","category":"page"},{"location":"examples/Riemannian-mean/#Variant-1:-Using-the-functions","page":"Riemannian Mean","title":"Variant 1: Using the functions","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"We can define both the cost and gradient, RiemannianMeanCost and RiemannianMeanGradient!!, respectively. For their mathematical derivation and further explanations, we again refer to Get Started: Optimize!.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"f = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Then we can for example directly call a gradient descent as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"x1 = gradient_descent(M, f, grad_f, first(data))","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794750908\n 0.00653160068349042\n 0.7267799820871861","category":"page"},{"location":"examples/Riemannian-mean/#Variant-2:-Using-the-objective","page":"Riemannian Mean","title":"Variant 2: Using the objective","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"A shorter way to directly obtain the Manifold objective including these two functions. Here, we want to specify that the objective can do inplace-evaluations using the evaluation=-keyword. The objective can be obtained calling Riemannian_mean_objective as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Together with a manifold, this forms a Manopt Problem, which would usually enable to switch manifolds between solver runs. Here we could for example switch to using Euclidean(3) instead for the same data the objective is build upon.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmp = DefaultManoptProblem(M, rmo)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"This enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s1 = GradientDescentState(M, copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794750908\n 0.00653160068349042\n 0.7267799820871861","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"but we can easily use a conjugate gradient instead","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s2 = ConjugateGradientDescentState(\n    M,\n    copy(M, first(data)),\n    StopAfterIteration(100),\n    ArmijoLinesearch(M),\n    FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868393613136017\n 0.006531541407458413\n 0.7267799052788726","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"EditURL = \"https://github.com/JuliaManifolds/ManoptExamples.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing/#Contributing-to-Manopt.jl","page":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The following is a set of guidelines to ManoptExamples.jl.","category":"page"},{"location":"contributing/#Table-of-Contents","page":"Contributing to ManoptExamples.jl","title":"Table of Contents","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Contributing to Manopt.jl\nTable of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd an objective\nCode style","category":"page"},{"location":"contributing/#I-just-have-a-question","page":"Contributing to ManoptExamples.jl","title":"I just have a question","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on our GitHub discussion.","category":"page"},{"location":"contributing/#How-can-I-file-an-issue?","page":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"page"},{"location":"contributing/#How-can-I-contribute?","page":"Contributing to ManoptExamples.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing/#Add-an-objective","page":"Contributing to ManoptExamples.jl","title":"Add an objective","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The objective in Manopt.jl represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a Problem.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have a specific objective you would like to provide here, feel free to start a new file in the src/objectives/ folder in your own fork and propose it later as a Pull Request.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in src/functions/ and follows the naming scheme:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"cost functions are always of the form cost_ and a fitting name\ngradient functions are always of the the gradient_ and a fitting name, followed by an !","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"for in-place gradients and by !! if it is a struct that can provide both.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"It would be great if you could also add a small test for the functions and the problem you defined in the test/ section.","category":"page"},{"location":"contributing/#Add-an-example","page":"Contributing to ManoptExamples.jl","title":"Add an example","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding Quarto Markdown file to the examples/ folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the bib/literature.yaml file to add references (in CSL_YAML, which can for example be exported e.g. from Zotero).","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Add any packages you need to the examples/ environment (see the containting Project.toml). The examples will not be run on CI, but their rendered CommonMark outpout should be included in the list of examples in the documentation of this package.","category":"page"},{"location":"contributing/#Code-style","page":"Contributing to ManoptExamples.jl","title":"Code style","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We also follow a few internal conventions:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Any implemented function should be accompanied by its mathematical formulae if a closed form exists.\nwithin a file the structs should come first and functions second. The only exception are constructors for the structs\nwithin both blocks an alphabetical order is preferable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including in-place/non-mutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature.\nAll import/using/include should be in the main module file.\nThere should only be a minimum of exports within this file, all problems should usually be later addressed as ManoptExamples.[...]\nthe Quarto Markdown files are excluded from this formatting.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","page":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Introduction","page":"A Benchmark","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"In this Benchmark we compare the Difference of Convex Algprithm (DCA) (Bergmann et al., 2023) and the Difference of Convex Proximal Point Algorithm (DCPPA) (Souza and Oliveira, 2015) which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from (Bergmann et al., 2023), Section 7.1.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"operatorname*argmin_pinmathcal M   g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where ghcolon mathcal M to mathbb R are geodesically convex function on the Riemannian manifold mathcal M.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#The-DC-Problem","page":"A Benchmark","title":"The DC Problem","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We start with defining the two convex functions gh and their gradients as well as the DC problem f and its gradient for the problem","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"    operatorname*argmin_pinmathcal M  bigl( logbigr(det(p)bigr)bigr)^4 - bigl(log det(p) bigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where the critical points obtain a functional value of -frac14.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where mathcal M is the manifold of symmetric positive definite (SPD) matrices with the affine invariant metric, which is the default.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We first define the corresponding functions","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"g(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and their gradients","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"grad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which we can use to verify that the gradients of g and h are correct. We use for that","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"n = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"to tall both checks","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, g, grad_g, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, h, grad_h, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which both pass the test. We continue to define their inplace variants","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"function grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And compare times for both algorithms, with a bit of debug output.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |δp|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_state=TrustRegionsState(M, copy(M, p0)),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |δp|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |δp|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |δp|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000003 |δp|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (3.9079528504063575e-11) is less than 1.0e-10.\n  7.392207 seconds (7.90 M allocations: 531.878 MiB, 4.32% gc time, 99.24% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The cost is","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dca)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Similarly the DCPPA performs","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    λ=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|δp|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantStepsize(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_state=TrustRegionsState(M, copy(M, p0)),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470 \n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|δp|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|δp|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|δp|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|δp|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|δp|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|δp|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|δp|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.791221 seconds (1.62 M allocations: 114.000 MiB, 2.17% gc time, 95.35% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"It needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dcppa)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","page":"A Benchmark","title":"Benchmark I: Time comparison","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and run a benchmark for both algorithms","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"for n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state=TrustRegionsState($Mn, copy($Mn, $pn)),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        λ=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantStepsize(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state=TrustRegionsState($Mn, copy($Mn, $pn)),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Since we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","page":"A Benchmark","title":"Benchmark II: Iterations and cost.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"As a second benchmark, let’s collect the number of iterations needed and the development of the cost over dimensions.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"N2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}()","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state = TrustRegionsState(Mn, copy(Mn, pn)),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        λ = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantStepsize(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_state = TrustRegionsState(Mn, copy(Mn, pn)),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The iterations are like","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And for the developtment of the cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where we can see that the DCA needs less iterations than the DCPPA.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Literature","page":"A Benchmark","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Bergmann, R., Ferreira, O. P., Santos, E. M., et al. (2023) The difference of convex algorithm on Hadamard manifolds. arXiv:2112.05250. arXiv. Available at: http://arxiv.org/abs/2112.05250.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Souza, J. C. de O. and Oliveira, P. R. (2015) A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization, 63, 797–810. Springer Science and Business Media LLC. DOI: 10.1007/s10898-015-0282-7.","category":"page"},{"location":"objectives/#List-of-Objectives-defined-for-the-Examples","page":"Objectives","title":"List of Objectives defined for the Examples","text":"","category":"section"},{"location":"objectives/#RiemannianMean","page":"Objectives","title":"Riemannian Mean","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Riemannian mean example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RiemannianMean.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RiemannianMeanCost","page":"Objectives","title":"ManoptExamples.RiemannianMeanCost","text":"RiemannianMeanCost{P}\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\nf(p) = sum_j=i^N d_mathcal M^2(d_i p)\n\nwhere d_mathcal M is the distance on a Riemannian manifold.\n\nConstructor\n\nRiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P}\n\nInitialize the cost function to a data set data of points on a manfiold M, where each point is of type P.\n\nSee also\n\nRiemannianMeanGradient!!, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RiemannianMeanGradient!!","page":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","text":"RiemannianMeanGradient!!{P} where P\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\noperatornamegradf(p) = sum_j=i^N log_p d_i\n\nwhere d_mathcal M is the distance on a Riemannian manifold and we employ grad_distance to compute the single summands.\n\nThis functor provides both the allocating variant grad_f(M,p) as well as the in-place variant grad_f!(M, X, p) which computes the gradient in-place of X.\n\nConstructors\n\nRiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T}\n\nGenerate the Riemannian mean gradient based on some data points data an intial tangent vector initial_vector. If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant.\n\nRiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T}\n\nInitialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold M is provideed.\n\nSee also\n\nRiemannianMeanCost, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.Riemannian_mean_objective-Tuple{AbstractVector}","page":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","text":"Riemannian_mean_objective(data, initial_vector=nothing, evaluation=AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n)\n\nGenerate the objective for the Riemannian mean task for some given vector of data points on the Riemannian manifold M.\n\nSee also\n\nRiemannianMeanCost, RiemannianMeanGradient!!\n\nnote: Note\nThe first constructor should only be used if an additional storage of the vector is not feasible, since initialising the initial_vector to nothing disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#RobustPCA","page":"Objectives","title":"Robust PCA","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Robust PCA example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RobustPCA.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RobustPCACost","page":"Objectives","title":"ManoptExamples.RobustPCACost","text":"RobustPCACost{D,F}\n\nA functor representing the Riemannian robust PCA function on the Grassmann manifold. For some given (column) data Dmathbb R^dtimes n the cost function is defined on some operatornameGr(dm), mn as the sum of the distances of the columns D_i to the subspace spanned by pinoperatornameGr(dm) (represented as a point on the Stiefel manifold). The function reads\n\nf(U) = frac1nsum_i=1^n lVert pp^mathrmTD_i - D_irVert\n\nThis cost additionally provides a Huber regularisation of the cost, that is for some ε0 one use ℓ_ε(x) = sqrtx^2+ε^2 - ε in\n\nf_ε(p) = frac1nsum_i=1^n ℓ_εbigl(lVert pp^mathrmTD_i - D_irVertbigr)\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCACost(data::AbstractMatrix, ε=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, ε=1.0)\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RobustPCAGrad!!","page":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","text":"RobustPCAGrad!!{D,F}\n\nA functor representing the Riemannian robust PCA gradient on the Grassmann manifold. For some given (column) data Xmathbb R^ptimes n the gradient of the RobustPCACost can be computed by projecting the Euclidean gradient onto the corresponding tangent space.\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCAGrad!!(data, ε=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, ε=1.0; evaluation=AllocatingEvaluation())\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the evaluation= keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.robust_PCA_objective","page":"Objectives","title":"ManoptExamples.robust_PCA_objective","text":"robust_PCA_objective(data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluton())\n\nGenerate the objective for the robust PCA task for some given data D and Huber regularization parameter ε.\n\nSee also\n\nRobustPCACost, RobustPCAGrad!!\n\nnote: Note\nSince the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the evaluation, indeed the gradient always allows for both the allocating and the inplace variant to be used, though that keyword is used to setup the objective.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#Rosenbrock","page":"Objectives","title":"Rosenbrock Function","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rosenbrock example  and The Difference of Convex Rosenbrock Example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/Rosenbrock.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RosenbrockCost","page":"Objectives","title":"ManoptExamples.RosenbrockCost","text":"RosenbrockCost\n\nProvide the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nf(mathcal M p) = a(p_1^2-p_2)^2 + (p_1-b)^2\n\nwhich means that for the 2D case, the manifold mathcal M is ignored.\n\nSee also 📖 Rosenbrock (with slightly different parameter naming).\n\nConstructor\n\nf = Rosenbrock(a,b)\n\ngenerates the struct/function of the Rosenbrock cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockGradient!!","page":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","text":"RosenbrockGradient\n\nProvide Eclidean GRadient fo the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nnabla f(mathcal M p) = beginpmatrix\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \n    -2a(p_1^2-p_2)\nendpmatrix\n\ni.e. also here the manifold is ignored.\n\nConstructor\n\nRosenbrockGradient(a,b)\n\nFunctors\n\ngrad_f!!(M,p)\ngrad_f!!(M, X, p)\n\nevaluate the gradient at p the manifoldmathcal M is ignored.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockMetric","page":"Objectives","title":"ManoptExamples.RosenbrockMetric","text":"RosenbrockMetric <: AbstractMetric\n\nA metric related to the Rosenbrock problem, where the metric at a point pmathbb R^2 is given by\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nwhere the mathrmRb stands for Rosenbrock\n\n\n\n\n\n","category":"type"},{"location":"objectives/#Base.exp-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","page":"Objectives","title":"Base.exp","text":"q = exp(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, q, p, X)\n\nCompute the exponential map with respect to the RosenbrockMetric.\n\n    q = beginpmatrix p_1 + X_1  p_2+X_2+X_1^2endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Base.log-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any}","page":"Objectives","title":"Base.log","text":"X = log(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, X, p, q)\n\nCompute the logarithmic map with respect to the RosenbrockMetric. The formula reads for any j  1m\n\nX = beginpmatrix\n  q_1 - p_1 \n  q_2 - p_2 + (q_1 - p_1)^2\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.inverse_local_metric-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.inverse_local_metric","text":"inverse_local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the inverse of the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG^-1_p =\nbeginpmatrix\n    1  2p_1\n    2p_1  1+4p_1^2 \nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.local_metric-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.local_metric","text":"local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.change_representer-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","page":"Objectives","title":"ManifoldsBase.change_representer","text":"Y = change_representer(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, Y, ::EuclideanMetric, p, X)\n\nGiven the Euclidan gradient X at p, this function computes the corresponting Riesz representer Ysuch that⟨X,Z⟩ = ⟨ Y, Z ⟩_{\\mathrm{Rb},p}holds for allZ, in other wordsY = G(p)^{-1}X`.\n\nthis function is used in riemannian_gradient to convert a Euclidean into a Riemannian gradient.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.inner-Tuple{Manifolds.MetricManifold{ℝ, Manifolds.Euclidean{Tuple{2}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","page":"Objectives","title":"ManifoldsBase.inner","text":"inner(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X, Y)\n\nCompute the inner product on mathbb R^2 with respect to the RosenbrockMetric, i.e. for XY in T_pmathcal M we have\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1\n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Rosenbrock_objective","page":"Objectives","title":"ManoptExamples.Rosenbrock_objective","text":"Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation())\n\nReturn the gradient objective of the Rosenbrock example.\n\nSee also RosenbrockCost, RosenbrockGradient!!\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","page":"Objectives","title":"ManoptExamples.minimizer","text":"minimizer(::RosenbrockCost)\n\nReturn the minimizer of the RosenbrockCost, which is given by\n\np^* = beginpmatrix bb^2 endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","page":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Ronny BergmannLaura Weigl 2023-07-02","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For this example we first load the necessary packages.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Robust-PCA/#Computing-a-Robust-PCA","page":"Robust PCA","title":"Computing a Robust PCA","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For a given matrix D  ℝ^dn whose columns represent points in ℝ^d, a matrix p  ℝ^dm is computed for a given dimension m  n: p represents an ONB of ℝ^dm such that the column space of p approximates the points (columns of D), i.e. the vectors D_i as well as possible.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We compute p as a minimizer over the Grassmann manifold of the cost function:","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"beginsplit\nf(p)\n = frac1nsum_i=1^noperatornamedist(D_i operatornamespan(p))\n\n = frac1n sum_i=1^nlVert pp^TD_i - D_irVert\nendsplit","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The output cost represents the average distance achieved with the returned p, an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that f is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter ε.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f_ϵ(p) = frac1n sum_i=1^nℓ_ϵ(lVert pp^mathrmTD_i - D_irVert)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"where ℓ_ϵ(x) = sqrtx^2 + ϵ^2 - ϵ.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts).","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"First, we generate random data. For illustration purposes we take points in mathbb R^2 and m=1, that is we aim to find a robust regression line.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"n = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"M = Grassmann(d,m);","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For the initial matrix p_0 we use classical PCA via singular value decomposition. Thus, we use the first d left singular vectors.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Then, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in Manopt.jl.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Furthermore the cost and gradient are implemented in ManoptExamples.jl. Since these are Huber regularized, both functors have the ϵ as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection pp^T - I, which converts the Euclidean to the Riemannian gradient.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The trust-region method also requires the Hessian Matrix. By using ApproxHessianFiniteDifference using a finite difference scheme we get an approximation of the Hessian Matrix.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We run the procedure several times, where the smoothing parameter ε is reduced iteratively.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ε = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m]","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"2×1 Matrix{Float64}:\n -0.7494248652139397\n  0.6620893983436593","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Let’s generate the cost and gradient we aim to use here","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f = ManoptExamples.RobustPCACost(M, data, ε)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, ε)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([9.537606557855465 1.6583418797018163 … 30.833523701909474 30.512999245062304; -45.34339972619071 -1.7120433539256108 … -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"and check the initial cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f(M, p0)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.430690947905521","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Now we iterate the opimization with reducing ε after every iteration, which we update in f and grad_f.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"q = copy(M, p0)\nεi = ε\nfor i in 1:iterations\n    f.ε = εi\n    grad_f.ε = εi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global εi *= reduction\nend","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"When finally setting ε we can investigate the final cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f.ε = 0.0\nf(M, q)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.412973804873698","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Finally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"fig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/#List-of-Examples","page":"Overview","title":"List of Examples","text":"","category":"section"},{"location":"examples/","page":"Overview","title":"Overview","text":"Name provides Documentation\nA Benchmark for Difference of Convex contains a few simple functions \nSolving Rosenbrock with Difference of Convex DoC split of Rosenbrock uses a Rosenbrock based metric 📚\nDifference of Convex vs. Frank-Wolfe closed-form sub solver \nRiemannian Mean f, operatornamegradf (A/I), objective 📚\nRobust PCA f, operatornamegradf (A/I), objective 📚\nRosenbrock f, operatornamegradf (A/I), objective, minimizer 📚","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"Symbols:","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"A Allocating variant\nI In-place variant\n📚 link to documented functions in the documentation","category":"page"},{"location":"#Welcome-to-ManoptExample.jl","page":"Home","title":"Welcome to ManoptExample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ManoptExamples","category":"page"},{"location":"","page":"Home","title":"Home","text":"ManoptExamples.ManoptExamples","category":"page"},{"location":"#ManoptExamples.ManoptExamples","page":"Home","title":"ManoptExamples.ManoptExamples","text":"🏔️⛷️ ManoptExamples.jl – A collection of research and tutorial example problems for Manopt.jl\n\n📚 Documentation: juliamanifolds.github.io/ManoptExamples.jl\n📦 Repository: github.com/JuliaManifolds/ManoptExamples.jl\n💬 Discussions: github.com/JuliaManifolds/ManoptExamples.jl/discussions\n🎯 Issues: github.com/JuliaManifolds/ManoptExamples.jl/issues\n\n\n\n\n\n","category":"module"},{"location":"","page":"Home","title":"Home","text":"This package provides a set of example tasks for Manopt.jl based on either generic manifolds from the ManifoldsBase.jl interface or specific manifolds from Manifolds.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each example usually consists of","category":"page"},{"location":"","page":"Home","title":"Home","text":"a cost function and additional objects, like the gradient or proximal maps, see objectives\nan example explaining how to use these, see examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Helping functions that are used in one or more examples can be found in the section of functions in the menu.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","page":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Introduction","page":"Rosenbrock Metric","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"This example illustrates how the 📖 Rosenbrock problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in (Bergmann et al., 2023), Section 7.2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Both the Rosenbrock problem","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"    operatorname*argmin_xinmathbb R^2 abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where ab0 and usually b=1 and a gg b, we know the minimizer x^* = (bb^2)^mathrmT, and also the (Euclidean) gradient","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"nabla f(x) =\n  beginpmatrix\n  4a(x_1^2-x_2) -2a(x_1^2-x_2)\n  endpmatrix\n  +\n  beginpmatrix\n  2(x_1-b) 0\n  endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"They are even available already here in ManifoldExamples.jl, see RosenbrockCost and RosenbrockGradient!!.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Furthermore, the RosenbrockMetric can be used on mathbb R^2, that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"XY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e. using a gradient but not a Hessian.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"The Euclidean Gradient\nThe Riemannian gradient descent with respect to the RosenbrockMetric\nThe Euclidean Difference of Convex Algorithm\nThe Riemannian Difference of Convex Algorithm respect to the RosenbrockMetric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Where we obtain a difference of convex problem by writing","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f(x) = abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 - bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"g(x) = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 quadtext and quad h(x) = bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_manopt_parameter!\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"To emphasize the effect, we choose a quite large value of a.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"a = 2*10^5\nb = 1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and use the starting point and a direction to check gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"p0 = [0.1, 0.2]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Euclidean gradient we can just use the same approach as in the Rosenbrock example","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M = ℝ^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n∇f!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"define a common debug vector","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"debug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|δp|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and call the gradient descent algorithm","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Eucl_GD_state = gradient_descent(M, f, ∇f!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000000 F(x): 8.9937e-06 |δp|: 1.3835e+00 | |grad f|: 8.170355e-03\n# 20000000 F(x): 2.9474e-09 |δp|: 6.5764e-03 | |grad f|: 1.419191e-04\n# 30000000 F(x): 9.8376e-13 |δp|: 1.1918e-04 | |grad f|: 2.526295e-06\n# 40000000 F(x): 3.2830e-16 |δp|: 2.1773e-06 | |grad f|: 4.526313e-08\n# 50000000 F(x): 1.0154e-19 |δp|: 3.9803e-08 | |grad f|: 6.838240e-10\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 53073227 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 10000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Riemannian case, we define","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"MetricManifold(Euclidean(2; field = ℝ), ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and the gradient is now adopted to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_f!(M, X, p)\n    ∇f!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 1000000  F(x): 1.3571e-09 |δp|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |δp|: 3.6836e-05 | |grad f|: 9.240792e-09\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the convex case, we have to first introduce the two parts of the cost.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and their (Euclidan) gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function ∇h!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ∇h(M, p; a=100, b=1)\n    X = zero(p)\n    ∇h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ∇g!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ∇g(M, p; a=100, b=1)\n    X = zero(p)\n    ∇g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and we define for convenience","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_∇h!(M, X, p) = ∇h!(M, X, p; a=a, b=b)\ndocE_∇g!(M, X, p) = ∇g!(M, X, p; a=a, b=b)\nfunction docE_∇f!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_∇g!(M, X, p)\n  docE_∇h!(M, Y, p)\n  X .-= Y\n  return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we call the difference of convex algorithm on Eucldiean space mathbb R^2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"E_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_∇h!, p0;\n    gradient=docE_∇f!,\n    grad_g = docE_∇g!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000    F(x): 2.9705e-09 |δp|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |δp|: 1.2173e-04 | |grad f|: 4.541087e-08\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLineseach() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 10000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"We first have to again defined the gradients with respect to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_h!(M, X, p; a=100, b=1)\n    ∇h!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ∇g!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For X in partial h(p^(k)) the sunproblem top determine p^(k+1) reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatorname*argmin_pinmathcal M g(p) - langle X log_p^(k)prangle","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with X = operatornamegrad h(p^(k)) that","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"phi(p) = g(p) - langle X log_p^(k)prangle\n= abigl( p_1^2-p_2bigr)^2\n        + 2bigl(p_1-bbigr)^2 - 2(p^(k)_1-b)p_1 + 2(p^(k)_1-b)p^(k)_1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"its Euclidean gradient reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatornamegradphi(p) =\n    nabla varphi(p)\n    = beginpmatrix\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^(k)_1-b)\n        -2a(p_1^2-p_2)\n    endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where we can again employ the gradient conversion from before to obtain the Riemannian gradient.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"mutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (ϕ::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    ϕ(M, X, p)\n    return X\nend\nfunction (ϕ::SubGrad)(M, X, p)\n    X .= [\n        4 * ϕ.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - ϕ.b) - 2 * (ϕ.pk[1] - ϕ.b),\n        -2 * ϕ.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And in orer to update the subsolvers gradient correctly, we have to overwrite","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"set_manopt_parameter!(ϕ::SubGrad, ::Val{:p}, p) = (ϕ.pk .= p)\nset_manopt_parameter!(ϕ::SubGrad, ::Val{:X}, X) = (ϕ.Xk .= X)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we again introduce for ease of use","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLineseach() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","page":"Rosenbrock Metric","title":"Comparison in Iterations","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^8),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\")","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Literature","page":"Rosenbrock Metric","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Bergmann, R., Ferreira, O. P., Santos, E. M., et al. (2023) The difference of convex algorithm on Hadamard manifolds. arXiv:2112.05250. arXiv. Available at: http://arxiv.org/abs/2112.05250.","category":"page"},{"location":"examples/Rosenbrock/#The-Rosenbrock-Function","page":"Rosenbrock","title":"The Rosenbrock Function","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Ronny Bergmann 2023-01-03","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"After loading the necessary packages","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Manifolds, Manopt, ManoptExamples\nusing Plots","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We fix the parameters for the 📖 Rosenbrock (where the wikipedia page has a slightly different parameter naming).","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"a = 100.0\nb = 1.0\np0 = [1/10, 2/10]","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which is defined on mathbb R^2, so we need","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"M = ℝ^2","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Euclidean(2; field = ℝ)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and can then generate both the cost and the gradient","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"ManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"For comparison, we look at the initial cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, p0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"4.42","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And to illustrate, we run two small solvers with their default settings as a comparison.","category":"page"},{"location":"examples/Rosenbrock/#Gradient-Descent","page":"Rosenbrock","title":"Gradient Descent","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We start with the gradient descent solver.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Since we need the state anyways to access the record, we also get from the return_state=true a short summary of the solver run.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLineseach() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"From the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.10562873187751265","category":"page"},{"location":"examples/Rosenbrock/#Quasi-Newton","page":"Rosenbrock","title":"Quasi Newton","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We can improve this using the quasi Newton algorithm","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 44 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 2), projections, and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector trnasport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(DefaultManifold(), 0.0001, 0.999) with keyword arguments\n  * retraction_method = ExponentialRetraction()\n  * vector_transport_method = ParallelTransport()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 1000: not reached\n    |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And we see it stops far earlier, after 45 Iterations. We again collect the recorded values","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"2.359559352025148e-14","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and see that the final value is close to the one of the minimizer","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, ManoptExamples.minimizer(f))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.0","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which we also see if we plot the recorded cost.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"fig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"}]
}
