<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Rosenbrock Metric ¬∑ ManoptExamples.jl</title><meta name="title" content="Rosenbrock Metric ¬∑ ManoptExamples.jl"/><meta property="og:title" content="Rosenbrock Metric ¬∑ ManoptExamples.jl"/><meta property="twitter:title" content="Rosenbrock Metric ¬∑ ManoptExamples.jl"/><meta name="description" content="Documentation for ManoptExamples.jl."/><meta property="og:description" content="Documentation for ManoptExamples.jl."/><meta property="twitter:description" content="Documentation for ManoptExamples.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ManoptExamples.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../">Overview</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Difference of Convex</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Difference-of-Convex-Benchmark/">A Benchmark</a></li><li class="is-active"><a class="tocitem" href>Rosenbrock Metric</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#The-Euclidean-Gradient-Descent."><span>The Euclidean Gradient Descent.</span></a></li><li><a class="tocitem" href="#The-Riemannian-Gradient-Descent."><span>The Riemannian Gradient Descent.</span></a></li><li><a class="tocitem" href="#The-Euclidean-Difference-of-Convex"><span>The Euclidean Difference of Convex</span></a></li><li><a class="tocitem" href="#The-Riemannian-Difference-of-Convex"><span>The Riemannian Difference of Convex</span></a></li><li><a class="tocitem" href="#Comparison-in-Iterations"><span>Comparison in Iterations</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../Difference-of-Convex-Frank-Wolfe/">Frank Wolfe comparison</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Convex Bundle Method</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../RCBM-Median/">Riemannian Median</a></li><li><a class="tocitem" href="../H2-Signal-TV/">Hyperbolic Signal Denoising</a></li><li><a class="tocitem" href="../Spectral-Procrustes/">Spectral Procrustes</a></li></ul></li><li><a class="tocitem" href="../HyperparameterOptimization/">Hyperparameter optimziation</a></li><li><a class="tocitem" href="../RayleighQuotient/">The Rayleigh Quotient</a></li><li><a class="tocitem" href="../Riemannian-mean/">Riemannian Mean</a></li><li><a class="tocitem" href="../Robust-PCA/">Robust PCA</a></li><li><a class="tocitem" href="../Rosenbrock/">Rosenbrock</a></li><li><a class="tocitem" href="../Total-Variation/">Total Variation</a></li></ul></li><li><a class="tocitem" href="../../objectives/">Objectives</a></li><li><a class="tocitem" href="../../data/">Data</a></li><li><a class="tocitem" href="../../contributing/">Contributing to ManoptExamples.jl</a></li><li><a class="tocitem" href="../../changelog/">Changelog</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li><a class="is-disabled">Difference of Convex</a></li><li class="is-active"><a href>Rosenbrock Metric</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Rosenbrock Metric</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/ManoptExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/ManoptExamples.jl/blob/main/docs/src/examples/Difference-of-Convex-Rosenbrock.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm"><a class="docs-heading-anchor" href="#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm">Solving Rosenbrock with the Difference of Convex Algorithm</a><a id="Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm" title="Permalink"></a></h1><p>Ronny Bergmann 2023-06-06</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>This example illustrates how the <a href="https://en.wikipedia.org/wiki/Rosenbrock_function">üìñ Rosenbrock</a> problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [<a href="../../references/#BergmannFerreiraSantosSouza:2023">BFSS23</a>], Section 7.2.</p><p>Both the Rosenbrock problem</p><p class="math-container">\[    \operatorname*{argmin}_{x\in ‚Ñù^2} a\bigl( x_1^2-x_2\bigr)^2 + \bigl(x_1-b\bigr)^2,\]</p><p>where <span>$a,b&gt;0$</span> and usually <span>$b=1$</span> and <span>$a \gg b$</span>, we know the minimizer <span>$x^* = (b,b^2)^\mathrm{T}$</span>, and also the (Euclidean) gradient</p><p class="math-container">\[\nabla f(x) =
  \begin{pmatrix}
  4a(x_1^2-x_2)\\ -2a(x_1^2-x_2)
  \end{pmatrix}
  +
  \begin{pmatrix}
  2(x_1-b)\\ 0
  \end{pmatrix}.\]</p><p>They are even available already here in <code>ManifoldExamples.jl</code>, see <a href="../../objectives/#ManoptExamples.RosenbrockCost"><code>RosenbrockCost</code></a> and <a href="../../objectives/#ManoptExamples.RosenbrockGradient!!"><code>RosenbrockGradient!!</code></a>.</p><p>Furthermore, the <a href="../../objectives/#ManoptExamples.RosenbrockMetric"><code>RosenbrockMetric</code></a> can be used on <span>$‚Ñù^2$</span>, that is</p><p class="math-container">\[‚ü®X,Y‚ü©_{\mathrm{Rb},p} = X^\mathrm{T}G_pY, \qquad
G_p = \begin{pmatrix}
  1+4p_1^2 &amp; -2p_1 \\
  -2p_1 &amp; 1
\end{pmatrix},\]</p><p>In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e.¬†using a gradient but not a Hessian.</p><ol><li>The Euclidean Gradient</li><li>The Riemannian gradient descent with respect to the <a href="../../objectives/#ManoptExamples.RosenbrockMetric"><code>RosenbrockMetric</code></a></li><li>The Euclidean Difference of Convex Algorithm</li><li>The Riemannian Difference of Convex Algorithm respect to the <a href="../../objectives/#ManoptExamples.RosenbrockMetric"><code>RosenbrockMetric</code></a></li></ol><p>Where we obtain a difference of convex problem by writing</p><p class="math-container">\[f(x) = a\bigl( x_1^2-x_2\bigr)^2 + \bigl(x_1-b\bigr)^2
 = a\bigl( x_1^2-x_2\bigr)^2 + 2\bigl(x_1-b\bigr)^2 - \bigl(x_1-b\bigr)^2\]</p><p>that is</p><p class="math-container">\[g(x) = a\bigl( x_1^2-x_2\bigr)^2 + 2\bigl(x_1-b\bigr)^2 \quad\text{ and }\quad h(x) = \bigl(x_1-b\bigr)^2\]</p><pre><code class="language-julia hljs">using LinearAlgebra, Random, Statistics
using Manifolds, Manopt, ManoptExamples
using NamedColors, Plots
import Manopt: set_parameter!
Random.seed!(42)</code></pre><pre><code class="language-julia hljs">paul_tol = load_paul_tol()
indigo = paul_tol[&quot;mutedindigo&quot;]
green = paul_tol[&quot;mutedgreen&quot;]
sand = paul_tol[&quot;mutedsand&quot;]
teal = paul_tol[&quot;mutedteal&quot;]
grey = paul_tol[&quot;mutedgrey&quot;]</code></pre><p>To emphasize the effect, we choose a quite large value of <code>a</code>.</p><pre><code class="language-julia hljs">a = 2*10^5
b = 1</code></pre><p>and use the starting point and a direction to check gradients</p><pre><code class="language-julia hljs">p0 = [0.1, 0.2]</code></pre><h2 id="The-Euclidean-Gradient-Descent."><a class="docs-heading-anchor" href="#The-Euclidean-Gradient-Descent.">The Euclidean Gradient Descent.</a><a id="The-Euclidean-Gradient-Descent.-1"></a><a class="docs-heading-anchor-permalink" href="#The-Euclidean-Gradient-Descent." title="Permalink"></a></h2><p>For the Euclidean gradient we can just use the same approach as in the <a href="../Rosenbrock/">Rosenbrock example</a></p><pre><code class="language-julia hljs">M = ‚Ñù^2
f = ManoptExamples.RosenbrockCost(M; a=a, b=b)
‚àáf!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)</code></pre><p>define a common debug vector</p><pre><code class="language-julia hljs">debug_vec = [
        (:Iteration, &quot;# %-8d &quot;),
        (:Cost, &quot;F(x): %1.4e&quot;),
        &quot; &quot;,
        (:Change, &quot;|Œ¥p|: %1.4e | &quot;),
        (:GradientNorm, &quot;|grad f|: %1.6e&quot;),
        :Stop,
        &quot;\n&quot;,
    ]</code></pre><p>and call the <a href="https://manoptjl.org/stable/solvers/gradient_descent/#Manopt.gradient_descent">gradient descent algorithm</a></p><pre><code class="language-julia hljs">Eucl_GD_state = gradient_descent(M, f, ‚àáf!!, p0;
    evaluation=InplaceEvaluation(),
    debug=[debug_vec...,10^7],
    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),
    record=[:Iteration, :Cost],
    return_state=true,
)</code></pre><pre><code class="nohighlight hljs">Initial F(x): 7.2208e+03 
# 10000000 F(x): 8.9937e-06 |Œ¥p|: 1.3835e+00 | |grad f|: 8.170355e-03
The algorithm reached its maximal number of iterations (10000000).

# Solver state for `Manopt.jl`s Gradient Descent
After 10000000 iterations

## Parameters
* retraction method: ExponentialRetraction()

## Stepsize
ArmijoLinesearch(;
    initial_stepsize=1.0
    retraction_method=ExponentialRetraction()
    contraction_factor=0.95
    sufficient_decrease=0.1
)

## Stopping criterion

Stop When _one_ of the following are fulfilled:
    Max Iteration 10000000: reached
    |Œîp| &lt; 1.0e-16: not reached
Overall: reached
This indicates convergence: No

## Debug
    :Iteration = [(:Iteration, &quot;# %-8d &quot;), (:Cost, &quot;F(x): %1.4e&quot;), &quot; &quot;, (:Change, &quot;|Œ¥p|: %1.4e | &quot;), (:GradientNorm, &quot;|grad f|: %1.6e&quot;), &quot;\n&quot;, 10000000]
    :Stop = :Stop

## Record
(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)</code></pre><h2 id="The-Riemannian-Gradient-Descent."><a class="docs-heading-anchor" href="#The-Riemannian-Gradient-Descent.">The Riemannian Gradient Descent.</a><a id="The-Riemannian-Gradient-Descent.-1"></a><a class="docs-heading-anchor-permalink" href="#The-Riemannian-Gradient-Descent." title="Permalink"></a></h2><p>For the Riemannian case, we define</p><pre><code class="language-julia hljs">M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())</code></pre><pre><code class="nohighlight hljs">MetricManifold(Euclidean(2; field=‚Ñù), ManoptExamples.RosenbrockMetric())</code></pre><p>and the gradient is now adopted to the new metric</p><pre><code class="language-julia hljs">function grad_f!(M, X, p)
    ‚àáf!!(M, X, p)
    riemannian_gradient!(M, X, p, X)
    return X
end
function grad_f(M, p)
    X = zero_vector(M, p)
    return grad_f!(M, X, p)
end</code></pre><pre><code class="language-julia hljs">R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;
    evaluation=InplaceEvaluation(),
    debug=[debug_vec...,10^6],
    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),
    record=[:Iteration, :Cost],
    return_state=true,
)</code></pre><pre><code class="nohighlight hljs">Initial F(x): 7.2208e+03 
# 1000000  F(x): 1.3571e-09 |Œ¥p|: 9.1006e-01 | |grad f|: 1.974939e-04
# 2000000  F(x): 2.7921e-18 |Œ¥p|: 3.6836e-05 | |grad f|: 9.240792e-09
At iteration 2443750 the algorithm performed a step with a change (0.0) less than 1.0e-16.

# Solver state for `Manopt.jl`s Gradient Descent
After 2443750 iterations

## Parameters
* retraction method: ExponentialRetraction()

## Stepsize
ArmijoLinesearch(;
    initial_stepsize=1.0
    retraction_method=ExponentialRetraction()
    contraction_factor=0.95
    sufficient_decrease=0.1
)

## Stopping criterion

Stop When _one_ of the following are fulfilled:
    Max Iteration 10000000: not reached
    |Œîp| &lt; 1.0e-16: reached
Overall: reached
This indicates convergence: Yes

## Debug
    :Iteration = [(:Iteration, &quot;# %-8d &quot;), (:Cost, &quot;F(x): %1.4e&quot;), &quot; &quot;, (:Change, &quot;|Œ¥p|: %1.4e | &quot;), (:GradientNorm, &quot;|grad f|: %1.6e&quot;), &quot;\n&quot;, 1000000]
    :Stop = :Stop

## Record
(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)</code></pre><h2 id="The-Euclidean-Difference-of-Convex"><a class="docs-heading-anchor" href="#The-Euclidean-Difference-of-Convex">The Euclidean Difference of Convex</a><a id="The-Euclidean-Difference-of-Convex-1"></a><a class="docs-heading-anchor-permalink" href="#The-Euclidean-Difference-of-Convex" title="Permalink"></a></h2><p>For the convex case, we have to first introduce the two parts of the cost.</p><pre><code class="language-julia hljs">f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;
f2(M, p; a=100, b=1) = (p[1] - b[1])^2;
g(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)
h(M, p; a=100, b=1) = f2(M, p; a=a, b=b)</code></pre><p>and their (Euclidan) gradients</p><pre><code class="language-julia hljs">function ‚àáh!(M, X, p; a=100, b=1)
    X[1] = 2*(p[1]-b)
    X[2] = 0
    return X
end
function ‚àáh(M, p; a=100, b=1)
    X = zero(p)
    ‚àáh!(M, X, p; a=a, b=b)
    return X
end
function ‚àág!(M, X, p; a=100, b=1)
    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)
    X[2] = -2*a*(p[1]^2-p[2])
    return X
end
function ‚àág(M, p; a=100, b=1)
    X = zero(p)
    ‚àág!(M, X, p; a=a, b=b)
    return X
end</code></pre><p>and we define for convenience</p><pre><code class="language-julia hljs">docE_g(M, p) = g(M, p; a=a, b=b)
docE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)
docE_‚àáh!(M, X, p) = ‚àáh!(M, X, p; a=a, b=b)
docE_‚àág!(M, X, p) = ‚àág!(M, X, p; a=a, b=b)
function docE_‚àáf!(M, X, p)
  Y = zero_vector(M, p)
  docE_‚àág!(M, X, p)
  docE_‚àáh!(M, Y, p)
  X .-= Y
  return X
end</code></pre><p>Then we call the <a href="https://manoptjl.org/stable/solvers/difference_of_convex/#Manopt.difference_of_convex_algorithm">difference of convex algorithm</a> on Eucldiean space <span>$‚Ñù^2$</span>.</p><pre><code class="language-julia hljs">E_doc_state = difference_of_convex_algorithm(
    M, docE_f, docE_g, docE_‚àáh!, p0;
    gradient=docE_‚àáf!,
    grad_g = docE_‚àág!,
    debug=[debug_vec..., 10^4],
    evaluation=InplaceEvaluation(),
    record=[:Iteration, :Cost],
    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),
    sub_hess=nothing, # Use gradient descent
    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),
    return_state=true,
)</code></pre><pre><code class="nohighlight hljs">Initial F(x): 7.2208e+03 
# 10000    F(x): 2.9705e-09 |Œ¥p|: 1.3270e+00 | |grad f|: 1.388203e-04
# 20000    F(x): 3.3302e-16 |Œ¥p|: 1.2173e-04 | |grad f|: 4.541087e-08
At iteration 26549 the algorithm performed a step with a change (0.0) less than 1.0e-16.

# Solver state for `Manopt.jl`s Difference of Convex Algorithm
After 26549 iterations

## Parameters
* sub solver state:
    | # Solver state for `Manopt.jl`s Gradient Descent
    | After 2000 iterations
    | 
    | ## Parameters
    | * retraction method: ExponentialRetraction()
    | 
    | ## Stepsize
    | ArmijoLinesearch(;
    |     initial_stepsize=1.0
    |     retraction_method=ExponentialRetraction()
    |     contraction_factor=0.95
    |     sufficient_decrease=0.1
    | )
    | 
    | ## Stopping criterion
    | 
    | Stop When _one_ of the following are fulfilled:
    |     Max Iteration 2000:   reached
    |     |grad f| &lt; 1.0e-16: not reached
    | Overall: reached
    | This indicates convergence: No

## Stopping criterion

Stop When _one_ of the following are fulfilled:
    Max Iteration 10000000: not reached
    |Œîp| &lt; 1.0e-16: reached
Overall: reached
This indicates convergence: Yes

## Debug
    :Iteration = [(:Iteration, &quot;# %-8d &quot;), (:Cost, &quot;F(x): %1.4e&quot;), &quot; &quot;, (:Change, &quot;|Œ¥p|: %1.4e | &quot;), (:GradientNorm, &quot;|grad f|: %1.6e&quot;), &quot;\n&quot;, 10000]
    :Stop = :Stop

## Record
(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)</code></pre><h2 id="The-Riemannian-Difference-of-Convex"><a class="docs-heading-anchor" href="#The-Riemannian-Difference-of-Convex">The Riemannian Difference of Convex</a><a id="The-Riemannian-Difference-of-Convex-1"></a><a class="docs-heading-anchor-permalink" href="#The-Riemannian-Difference-of-Convex" title="Permalink"></a></h2><p>We first have to again defined the gradients with respect to the new metric</p><pre><code class="language-julia hljs">function grad_h!(M, X, p; a=100, b=1)
    ‚àáh!(M, X, p; a=a, b=b)
    riemannian_gradient!(M, X, p, X)
    return X
end
function grad_h(M, p; a=100, b=1)
    X = zero(p)
    grad_h!(M, X, p; a=a, b=b)
    return X
end
function grad_g!(M, X, p; a=100, b=1)
    ‚àág!(M, X, p; a=a,b=b)
    riemannian_gradient!(M, X, p, X)
    return X
end
function grad_g(M, p; a=100, b=1)
    X = zero(p)
    grad_g!(M, X, p; a=a, b=b)
    return X
end</code></pre><p>While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For <span>$X \in   ‚àÇh(p^{(k)})$</span> the sunproblem top determine <span>$p^{(k+1)}$</span> reads</p><p class="math-container">\[\operatorname*{argmin}_{p\in\mathcal M} g(p) - \langle X, \log_{p^{(k)}}p\rangle\]</p><p>for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with <span>$X = \operatorname{grad} h(p^{(k)})$</span> that</p><p class="math-container">\[\phi(p) = g(p) - \langle X, \log_{p^{(k)}}p\rangle
= a\bigl( p_{1}^2-p_{2}\bigr)^2
        + 2\bigl(p_{1}-b\bigr)^2 - 2(p^{(k)}_1-b)p_1 + 2(p^{(k)}_1-b)p^{(k)}_1,\]</p><p>its Euclidean gradient reads</p><p class="math-container">\[\operatorname{grad}\phi(p) =
    \nabla \varphi(p)
    = \begin{pmatrix}
        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^{(k)}_1-b)\\
        -2a(p_1^2-p_2)
    \end{pmatrix}\]</p><p>where we can again employ the gradient conversion from before to obtain the Riemannian gradient.</p><pre><code class="language-julia hljs">mutable struct SubGrad{P,T,V}
    pk::P
    Xk::T
    a::V
    b::V
end
function (œï::SubGrad)(M, p)
    X = zero_vector(M, p)
    œï(M, X, p)
    return X
end
function (œï::SubGrad)(M, X, p)
    X .= [
        4 * œï.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - œï.b) - 2 * (œï.pk[1] - œï.b),
        -2 * œï.a * (p[1]^2 - p[2]),
    ]
    riemannian_gradient!(M, X, p, X) # convert
    return X
end</code></pre><p>And in orer to update the sub solvers gradient correctly, we have to overwrite</p><pre><code class="language-julia hljs">set_parameter!(œï::SubGrad, ::Val{:p}, p) = (œï.pk .= p)
set_parameter!(œï::SubGrad, ::Val{:X}, X) = (œï.Xk .= X)</code></pre><p>And we again introduce for ease of use</p><pre><code class="language-julia hljs">docR_g(M, p) = g(M, p; a=a, b=b)
docR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)
docR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)
docR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)
function docR_grad_f!(M, X, p)
    Y = zero_vector(M, p)
    docR_grad_g!(M, X, p)
    docR_grad_h!(M, Y, p)
    X .-= Y
    return X
end
docR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)</code></pre><p>Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.</p><pre><code class="language-julia hljs">R_doc_state = difference_of_convex_algorithm(
    M_rb, docR_f, docR_g, docR_grad_h!, p0;
    gradient=docR_grad_f!,
    grad_g = docR_grad_g!,
    debug=[debug_vec..., 10^6],
    evaluation=InplaceEvaluation(),
    record=[:Iteration, :Cost],
    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),
    sub_grad=docR_sub_grad,
    sub_hess = nothing, # Use gradient descent
    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),
    return_state=true,
)</code></pre><pre><code class="nohighlight hljs">Initial F(x): 7.2208e+03 
At iteration 1235 the algorithm performed a step with a change (0.0) less than 1.0e-16.

# Solver state for `Manopt.jl`s Difference of Convex Algorithm
After 1235 iterations

## Parameters
* sub solver state:
    | # Solver state for `Manopt.jl`s Gradient Descent
    | After 2000 iterations
    | 
    | ## Parameters
    | * retraction method: ExponentialRetraction()
    | 
    | ## Stepsize
    | ArmijoLinesearch(;
    |     initial_stepsize=1.0
    |     retraction_method=ExponentialRetraction()
    |     contraction_factor=0.95
    |     sufficient_decrease=0.1
    | )
    | 
    | ## Stopping criterion
    | 
    | Stop When _one_ of the following are fulfilled:
    |     Max Iteration 2000:   reached
    |     |grad f| &lt; 1.0e-16: not reached
    | Overall: reached
    | This indicates convergence: No

## Stopping criterion

Stop When _one_ of the following are fulfilled:
    Max Iteration 10000000: not reached
    |Œîp| &lt; 1.0e-16: reached
Overall: reached
This indicates convergence: Yes

## Debug
    :Iteration = [(:Iteration, &quot;# %-8d &quot;), (:Cost, &quot;F(x): %1.4e&quot;), &quot; &quot;, (:Change, &quot;|Œ¥p|: %1.4e | &quot;), (:GradientNorm, &quot;|grad f|: %1.6e&quot;), &quot;\n&quot;, 1000000]
    :Stop = :Stop

## Record
(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)</code></pre><h2 id="Comparison-in-Iterations"><a class="docs-heading-anchor" href="#Comparison-in-Iterations">Comparison in Iterations</a><a id="Comparison-in-Iterations-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-in-Iterations" title="Permalink"></a></h2><pre><code class="language-julia hljs">fig = plot(;
    legend=:topright,
    xlabel=raw&quot;Iterations $k$ (log. scale)&quot;, ylabel=raw&quot;Cost $f(x)$ (log. scale)&quot;,
    yaxis=:log,
    ylims=(1e-16, 5*1e5),
    xaxis=:log,
    xlims=(1,10^7),
)
scatter!(fig, [1,], [f(M,p0),], label=raw&quot;$f(p_0)$&quot;, markercolor=grey)
egi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries
egc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries
plot!(fig, egi, egc, color=teal, label=&quot;Euclidean GD&quot;)
#
rgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries
rgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries
plot!(fig, rgi, rgc, color=indigo, label=&quot;Riemannian GD&quot;)
#
edi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries
edc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries
plot!(fig, edi, edc, color=sand, label=&quot;Euclidean DoC&quot;)
#
rdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries
rdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries
plot!(fig, rdi, rdc, color=green, label=&quot;Riemannian DoC&quot;)</code></pre><p><img src="../Difference-of-Convex-Rosenbrock_files/figure-commonmark/cell-22-output-1.svg" alt/></p><p>And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.</p><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[BFSS23]</dt><dd><div>R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza. <em>The difference of convex algorithm on Hadamard manifolds</em>. Preprint (2023), <a href="https://arxiv.org/abs/2112.05250">arXiv:2112.05250</a>.</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Difference-of-Convex-Benchmark/">¬´ A Benchmark</a><a class="docs-footer-nextpage" href="../Difference-of-Convex-Frank-Wolfe/">Frank Wolfe comparison ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 11 February 2025 16:09">Tuesday 11 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
