<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>The Rayleigh Quotient Â· ManoptExamples.jl</title><meta name="title" content="The Rayleigh Quotient Â· ManoptExamples.jl"/><meta property="og:title" content="The Rayleigh Quotient Â· ManoptExamples.jl"/><meta property="twitter:title" content="The Rayleigh Quotient Â· ManoptExamples.jl"/><meta name="description" content="Documentation for ManoptExamples.jl."/><meta property="og:description" content="Documentation for ManoptExamples.jl."/><meta property="twitter:description" content="Documentation for ManoptExamples.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ManoptExamples.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../">Overview</a></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Difference of Convex</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Difference-of-Convex-Benchmark/">A Benchmark</a></li><li><a class="tocitem" href="../Difference-of-Convex-Rosenbrock/">Rosenbrock Metric</a></li><li><a class="tocitem" href="../Difference-of-Convex-Frank-Wolfe/">Frank Wolfe comparison</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Convex Bundle Method</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../RCBM-Median/">Riemannian Median</a></li><li><a class="tocitem" href="../H2-Signal-TV/">Hyperbolic Signal Denoising</a></li><li><a class="tocitem" href="../Spectral-Procrustes/">Spectral Procrustes</a></li></ul></li><li><a class="tocitem" href="../HyperparameterOptimization/">Hyperparameter optimziation</a></li><li class="is-active"><a class="tocitem" href>The Rayleigh Quotient</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../Riemannian-mean/">Riemannian Mean</a></li><li><a class="tocitem" href="../Robust-PCA/">Robust PCA</a></li><li><a class="tocitem" href="../Rosenbrock/">Rosenbrock</a></li><li><a class="tocitem" href="../Total-Variation/">Total Variation</a></li></ul></li><li><a class="tocitem" href="../../objectives/">Objectives</a></li><li><a class="tocitem" href="../../data/">Data</a></li><li><a class="tocitem" href="../../contributing/">Contributing to ManoptExamples.jl</a></li><li><a class="tocitem" href="../../changelog/">Changelog</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>The Rayleigh Quotient</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>The Rayleigh Quotient</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/ManoptExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ï‚›</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/ManoptExamples.jl/blob/main/docs/src/examples/RayleighQuotient.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ï„</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-Rayleigh-Quotient"><a class="docs-heading-anchor" href="#The-Rayleigh-Quotient">The Rayleigh Quotient</a><a id="The-Rayleigh-Quotient-1"></a><a class="docs-heading-anchor-permalink" href="#The-Rayleigh-Quotient" title="Permalink"></a></h1><p>Ronny Bergmann 2024-03-09</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [<a href="../../references/#Boumal:2023">Bou23</a>] using the Rayleigh quotient and explores several different ways to use the algorithms from <a href="https://manoptjl.org"><code>Manopt.jl</code></a>.</p><p>For a symmetric matrix <span>$A \in \mathbb R^{n\times n}$</span> we consider the <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">ğŸ“– Rayleigh Quotient</a></p><p class="math-container">\[\operatorname*{arg\,min}_{x \in \mathbb R^n \backslash \{0\}}
\frac{x^{\mathrm{T}}Ax}{\lVert xÂ \rVert^2}.\]</p><p>On the sphere we can omit the denominator and obtain</p><p class="math-container">\[f(p) = p^{\mathrm{T}}Ap,\qquad p âˆˆ ğ•Š^{n-1},\]</p><p>which by itself we can again continue in the embedding as</p><p class="math-container">\[\tilde f(x) = x^{\mathrm{T}}Ax,\qquad x \in \mathbb R^n.\]</p><p>This cost has the nice feature that at the minimizer <span>$p^*\in\mathbb S^{n-1}$</span> the function falue <span>$f(p^*)$</span> is the smalles eigenvalue of <span>$A$</span>.</p><p>For the embedded function <span>$\tilde f$</span> the gradient and Hessian can be computed with classical methods as</p><p class="math-container">\[\begin{align*}
âˆ‡\tilde f(x) &amp;= 2Ax, \qquad x âˆˆ â„^n,
\\
âˆ‡^2\tilde f(x)[V] &amp;= 2AV, \qquad x, V âˆˆ â„^n.
\end{align*}\]</p><p>Similarly, cf.Â Examples 3.62 and 5.27 of [<a href="../../references/#Boumal:2023">Bou23</a>], the Riemannian gradient and Hessian on the manifold <span>$\mathcal M = \mathbb S^{n-1}$</span> are given by</p><p class="math-container">\[\begin{align*}
\operatorname{grad} f(p) &amp;= 2Ap - 2(p^{\mathrm{T}}Ap)*p,\qquad p âˆˆ ğ•Š^{n-1},
\\
\operatorname{Hess} f(p)[X] &amp;=  2AX - 2(p^{\mathrm{T}}AX)p - 2(p^{\mathrm{T}}Ap)X,\qquad p âˆˆ ğ•Š^{n-1}, X \in T_pğ•Š^{n-1}
\end{align*}\]</p><p>Letâ€™s first generate an example martrx <span>$A$</span>.</p><pre><code class="language-julia hljs">using Pkg;
cd(@__DIR__)
Pkg.activate(&quot;.&quot;); # use the example environment,</code></pre><pre><code class="language-julia hljs">using LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random
Random.seed!(42)
n = 500
A = Symmetric(randn(n, n) / n)</code></pre><p>And the manifolds</p><pre><code class="language-julia hljs">M = Sphere(n-1)</code></pre><pre><code class="nohighlight hljs">Sphere(499, â„)</code></pre><pre><code class="language-julia hljs">E = get_embedding(M)</code></pre><pre><code class="nohighlight hljs">Euclidean(500; field=â„)</code></pre><h3 id="Setup-the-corresponding-functions"><a class="docs-heading-anchor" href="#Setup-the-corresponding-functions">Setup the corresponding functions</a><a id="Setup-the-corresponding-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Setup-the-corresponding-functions" title="Permalink"></a></h3><p>Since <a href="../../objectives/#ManoptExamples.RayleighQuotientCost"><code>RayleighQuotientCost</code></a>, <a href="../../objectives/#ManoptExamples.RayleighQuotientGrad!!"><code>RayleighQuotientGrad!!</code></a>, and <a href="../../objectives/#ManoptExamples.RayleighQuotientHess!!"><code>RayleighQuotientHess!!</code></a> are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute <span>$f$</span> is called with <code>M</code> as their first argument and <span>$\tilde f$</span> if called with <code>E</code>.</p><p>We instantiate</p><pre><code class="language-julia hljs">f = ManoptExamples.RayleighQuotientCost(A)
grad_f = ManoptExamples.RayleighQuotientGrad!!(A)
Hess_f = ManoptExamples.RayleighQuotientHess!!(A)</code></pre><p>the suffix <code>!!</code> also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory</p><pre><code class="language-julia hljs">p0 = [1.0, zeros(n-1)...]
X = zero_vector(M, p0)</code></pre><p>we can both call</p><pre><code class="language-julia hljs">Y = grad_f(M, p0)  # Allocates memory
grad_f(M, X, p0)    # Computes in place of X and returns the result in X.
norm(M, p0, X-Y)</code></pre><pre><code class="nohighlight hljs">0.0</code></pre><p>Now we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them.</p><p>First of all letâ€™s construct the actual result â€“Â since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute</p><pre><code class="language-julia hljs">Î» = min(eigvals(A)...)</code></pre><pre><code class="nohighlight hljs">-0.08924035897103727</code></pre><h3 id="A-Solver-based-on-gradient-information"><a class="docs-heading-anchor" href="#A-Solver-based-on-gradient-information">A Solver based on gradient information</a><a id="A-Solver-based-on-gradient-information-1"></a><a class="docs-heading-anchor-permalink" href="#A-Solver-based-on-gradient-information" title="Permalink"></a></h3><p>Letâ€™s first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient <span>$\nabla \tilde f$</span>. We can â€œtellâ€ the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally, <code>Manopt.jl</code> then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf.Â e.g.Â <a href="https://manoptjl.org/stable/tutorials/AutomaticDifferentiation/#EmbeddedGradient">this tutorial section</a> or Section 3.8 or more precisely Example 3.62 in [<a href="../../references/#Boumal:2023">Bou23</a>].</p><p>But instead of diving into all the tecnical details, we can just specify <code>objective_type=:Euclidean</code> to trigger the conversion. We start with a simple <a href="https://manoptjl.org/stable/solvers/gradient_descent/">gradient descent</a></p><pre><code class="language-julia hljs">s = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,
    debug = [:Iteration, :Cost, :GradientNorm, 50, &quot;\n&quot;],
    return_state=true,
)
q1 = get_solver_result(s)
s</code></pre><pre><code class="nohighlight hljs">Initial f(x): -0.000727
# 50    f(x): -0.088242|grad f(p)|:0.003870474326981599
# 100   f(x): -0.088680|grad f(p)|:0.0034956568288634616
# 150   f(x): -0.089026|grad f(p)|:0.0026514781676923237
# 200   f(x): -0.089178|grad f(p)|:0.001531160335922979

# Solver state for `Manopt.jl`s Gradient Descent
After 200 iterations

## Parameters
* retraction method: ExponentialRetraction()

## Stepsize
ArmijoLinesearch(;
    initial_stepsize=1.0
    retraction_method=ExponentialRetraction()
    contraction_factor=0.95
    sufficient_decrease=0.1
)

## Stopping criterion

Stop When _one_ of the following are fulfilled:
    Max Iteration 200:  reached
    |grad f| &lt; 1.0e-8: not reached
Overall: reached
This indicates convergence: No

## Debug
    :Iteration = [(:Iteration, &quot;# %-6d&quot;), (:Cost, &quot;f(x): %f&quot;), (:GradientNorm, &quot;|grad f(p)|:%s&quot;), &quot;\n&quot;, 50]</code></pre><p>From the final cost we can already see that <code>q1</code> is an eigenvector to the smallest eigenvalue we obtaines above.</p><p>And we can compare this to running with the Riemannian gradient, since the <a href="../../objectives/#ManoptExamples.RayleighQuotientGrad!!"><code>RayleighQuotientGrad!!</code></a> returns this one as well, when just called with the sphere as first Argument, we just have to remove the <code>objective_type</code>.</p><pre><code class="language-julia hljs">q2 = gradient_descent(M, f, grad_f, p0;
    debug = [:Iteration, :Cost, :GradientNorm, 50, &quot;\n&quot;],
)
#Test that both are the same
isapprox(M, q1,q2)</code></pre><pre><code class="nohighlight hljs">Initial f(x): -0.000727
# 50    f(x): -0.088242|grad f(p)|:0.0038704743269815734
# 100   f(x): -0.088680|grad f(p)|:0.0034956568288633714
# 150   f(x): -0.089026|grad f(p)|:0.002651478167692353
# 200   f(x): -0.089178|grad f(p)|:0.0015311603359229782

true</code></pre><p>We can also benchmark both</p><pre><code class="language-julia hljs">@benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean)</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 15 samples with 1 evaluation per sample.
 Range (min â€¦ max):  338.265 ms â€¦ 362.217 ms  â”Š GC (min â€¦ max): 9.61% â€¦ 9.80%
 Time  (median):     347.848 ms               â”Š GC (median):    9.79%
 Time  (mean Â± Ïƒ):   348.597 ms Â±   7.903 ms  â”Š GC (mean Â± Ïƒ):  9.58% Â± 0.81%

  â–    â–ˆ â–ˆ  â–        â–    â–    â–     â–       â–ˆâ–           â–   â–  
  â–ˆâ–â–â–â–â–ˆâ–â–ˆâ–â–â–ˆâ–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–ˆâ–â–â–â–â–ˆâ–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–ˆ â–
  338 ms           Histogram: frequency by time          362 ms &lt;

 Memory estimate: 1.13 GiB, allocs estimate: 7964.</code></pre><pre><code class="language-julia hljs">@benchmark gradient_descent($M, $f, $grad_f, $p0)</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 160 samples with 1 evaluation per sample.
 Range (min â€¦ max):  29.575 ms â€¦ 44.511 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 2.26%
 Time  (median):     30.583 ms              â”Š GC (median):    1.89%
 Time  (mean Â± Ïƒ):   31.379 ms Â±  2.418 ms  â”Š GC (mean Â± Ïƒ):  2.40% Â± 2.53%

     â–ƒâ–ˆ                                                        
  â–‡â–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–ƒâ–„â–„â–ƒâ–‚â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–ƒâ–‚â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚ â–‚
  29.6 ms         Histogram: frequency by time        41.5 ms &lt;

 Memory estimate: 11.20 MiB, allocs estimate: 8750.</code></pre><p>From these results we see, that the conversion from the Euclidean to the Riemannian gradient does require a small amount of effort and hence reduces the performance slighly. Still, if the Euclidean Gradient is easier to compute or already available, this is in terms of coding the faster way. Finally this is a tradeoff between derivation and implementation efforts for the Riemannian gradient and a slight performance reduction when using the Euclidean one.</p><h3 id="A-Solver-based-(also)-on-(approximate)-Hessian-information"><a class="docs-heading-anchor" href="#A-Solver-based-(also)-on-(approximate)-Hessian-information">A Solver based (also) on (approximate) Hessian information</a><a id="A-Solver-based-(also)-on-(approximate)-Hessian-information-1"></a><a class="docs-heading-anchor-permalink" href="#A-Solver-based-(also)-on-(approximate)-Hessian-information" title="Permalink"></a></h3><p>To also involve the Hessian, we consider the <a href="https://manoptjl.org/stable/solvers/trust_regions/">trust regions</a> solver with three cases:</p><ol><li>Euclidean, approximating the Hessian</li><li>Euclidean, providing the Hessian</li><li>Riemannian, providing the Hessian but also using in-place evaluations.</li></ol><pre><code class="language-julia hljs">q3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,
    debug = [:Iteration, :Cost, :GradientNorm, 10, &quot;\n&quot;],
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): -0.000727
# 10    f(x): -0.088412|grad f(p)|:0.020989167846023046
# 20    f(x): -0.089079|grad f(p)|:0.007420373217153763
# 30    f(x): -0.089095|grad f(p)|:0.0022962557538450884
# 40    f(x): -0.089095|grad f(p)|:0.0022962557538450884
# 50    f(x): -0.089095|grad f(p)|:0.002296255752372694
# 60    f(x): -0.089095|grad f(p)|:0.002296255750900326
# 70    f(x): -0.089095|grad f(p)|:0.002296255749427973
# 80    f(x): -0.089095|grad f(p)|:0.0022962557479555895
# 90    f(x): -0.089095|grad f(p)|:0.002296255746483256
# 100   f(x): -0.089095|grad f(p)|:0.002296255745010824
# 110   f(x): -0.089095|grad f(p)|:0.0022962557435384336
# 120   f(x): -0.089095|grad f(p)|:0.0022962557420660814
# 130   f(x): -0.089095|grad f(p)|:0.0022962557405937175
# 140   f(x): -0.089095|grad f(p)|:0.0022962557391213406
# 150   f(x): -0.089095|grad f(p)|:0.0022962557376489472
# 160   f(x): -0.089095|grad f(p)|:0.0022962557361765603
# 170   f(x): -0.089095|grad f(p)|:0.0022962557347042086
# 180   f(x): -0.089095|grad f(p)|:0.0022962557332318594
# 190   f(x): -0.089095|grad f(p)|:0.002296255731759444
# 200   f(x): -0.089095|grad f(p)|:0.0022962557302870605
# 210   f(x): -0.089095|grad f(p)|:0.002296255728814736
# 220   f(x): -0.089095|grad f(p)|:0.0022962557273423162
# 230   f(x): -0.089095|grad f(p)|:0.002296255725869944
# 240   f(x): -0.089095|grad f(p)|:0.002296255724397584
# 250   f(x): -0.089095|grad f(p)|:0.0022962557229252085
# 260   f(x): -0.089095|grad f(p)|:0.0022962557214528346
# 270   f(x): -0.089095|grad f(p)|:0.0022962557199804855
# 280   f(x): -0.089095|grad f(p)|:0.0022962557185080726
# 290   f(x): -0.089095|grad f(p)|:0.002296255717035685
# 300   f(x): -0.089095|grad f(p)|:0.0022962557155633144
# 310   f(x): -0.089095|grad f(p)|:0.0022962557140909605
# 320   f(x): -0.089095|grad f(p)|:0.002296255712618571
# 330   f(x): -0.089095|grad f(p)|:0.0022962557111461876
# 340   f(x): -0.089095|grad f(p)|:0.002296255709673808
# 350   f(x): -0.089095|grad f(p)|:0.0022962557082014416
# 360   f(x): -0.089095|grad f(p)|:0.00229625570672907
# 370   f(x): -0.089095|grad f(p)|:0.002296255705256684
# 380   f(x): -0.089095|grad f(p)|:0.0022962557037843165
# 390   f(x): -0.089095|grad f(p)|:0.002296255702311948
# 400   f(x): -0.089095|grad f(p)|:0.0022962557008395722
# 410   f(x): -0.089095|grad f(p)|:0.002296255699367198
# 420   f(x): -0.089095|grad f(p)|:0.0022962556978947854
# 430   f(x): -0.089095|grad f(p)|:0.002296255696422434
# 440   f(x): -0.089095|grad f(p)|:0.0022962556949500715
# 450   f(x): -0.089095|grad f(p)|:0.002296255693477701
# 460   f(x): -0.089095|grad f(p)|:0.0022962556920053264
# 470   f(x): -0.089095|grad f(p)|:0.002296255690532953
# 480   f(x): -0.089095|grad f(p)|:0.0022962556890605674
# 490   f(x): -0.089095|grad f(p)|:0.002296255687588196
# 500   f(x): -0.089095|grad f(p)|:0.002296255686115812
# 510   f(x): -0.089095|grad f(p)|:0.0022962556846434557
# 520   f(x): -0.089095|grad f(p)|:0.00229625568317109
# 530   f(x): -0.089095|grad f(p)|:0.0022962556816987227
# 540   f(x): -0.089095|grad f(p)|:0.002296255680226299
# 550   f(x): -0.089095|grad f(p)|:0.0022962556787539364
# 560   f(x): -0.089095|grad f(p)|:0.0022962556772815833
# 570   f(x): -0.089095|grad f(p)|:0.0022962556758091917
# 580   f(x): -0.089095|grad f(p)|:0.0022962556743368473
# 590   f(x): -0.089095|grad f(p)|:0.0022962556728644322
# 600   f(x): -0.089095|grad f(p)|:0.002296255671392082
# 610   f(x): -0.089095|grad f(p)|:0.0022962556699197066
# 620   f(x): -0.089095|grad f(p)|:0.002296255668447314
# 630   f(x): -0.089095|grad f(p)|:0.002296255666974941
# 640   f(x): -0.089095|grad f(p)|:0.002296255665502539
# 650   f(x): -0.089095|grad f(p)|:0.002296255664030171
# 660   f(x): -0.089095|grad f(p)|:0.002296255662557798
# 670   f(x): -0.089095|grad f(p)|:0.0022962556610854248
# 680   f(x): -0.089095|grad f(p)|:0.002296255659613032
# 690   f(x): -0.089095|grad f(p)|:0.0022962556581407082
# 700   f(x): -0.089095|grad f(p)|:0.002296255656668318
# 710   f(x): -0.089095|grad f(p)|:0.0022962556551958985
# 720   f(x): -0.089095|grad f(p)|:0.002296255653723546
# 730   f(x): -0.089095|grad f(p)|:0.002296255652251183
# 740   f(x): -0.089095|grad f(p)|:0.0022962556507788168
# 750   f(x): -0.089095|grad f(p)|:0.0022962556493064516
# 760   f(x): -0.089095|grad f(p)|:0.002296255647834051
# 770   f(x): -0.089095|grad f(p)|:0.002296255646361663
# 780   f(x): -0.089095|grad f(p)|:0.0022962556448893287
# 790   f(x): -0.089095|grad f(p)|:0.0022962556434169014
# 800   f(x): -0.089095|grad f(p)|:0.0022962556419445497
# 810   f(x): -0.089095|grad f(p)|:0.0022962556404721797
# 820   f(x): -0.089095|grad f(p)|:0.0022962556389998293
# 830   f(x): -0.089095|grad f(p)|:0.002296255637527417
# 840   f(x): -0.089095|grad f(p)|:0.0022962556360550538
# 850   f(x): -0.089095|grad f(p)|:0.00229625563458268
# 860   f(x): -0.089095|grad f(p)|:0.002296255633110347
# 870   f(x): -0.089095|grad f(p)|:0.0022962556316379126
# 880   f(x): -0.089095|grad f(p)|:0.0022962556301655314
# 890   f(x): -0.089095|grad f(p)|:0.0022962556286931957
# 900   f(x): -0.089095|grad f(p)|:0.002296255627220805
# 910   f(x): -0.089095|grad f(p)|:0.0022962556257484328
# 920   f(x): -0.089095|grad f(p)|:0.0022962556242760554
# 930   f(x): -0.089095|grad f(p)|:0.0022962556228036954
# 940   f(x): -0.089095|grad f(p)|:0.002296255621331309
# 950   f(x): -0.089095|grad f(p)|:0.0022962556198589204
# 960   f(x): -0.089095|grad f(p)|:0.0022962556183865473
# 970   f(x): -0.089095|grad f(p)|:0.002296255616914149
# 980   f(x): -0.089095|grad f(p)|:0.0022962556154418005
# 990   f(x): -0.089095|grad f(p)|:0.0022962556139694153
# 1000  f(x): -0.089095|grad f(p)|:0.0022962556124970193</code></pre><p>To provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any <code>struct</code> is considered to (eventually) be the also optional starting point. For space reasons, letâ€™s also shorten the debug print to only iterations 7 and 14.</p><pre><code class="language-julia hljs">q4 = trust_regions(M, f, grad_f, (E, p, X) -&gt; Hess_f(E, p, X), p0; objective_type=:Euclidean,
    debug = [:Iteration, :Cost, :GradientNorm, 10, &quot;\n&quot;],
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): -0.000727
# 10    f(x): -0.089234|grad f(p)|:0.0013561755210368023</code></pre><pre><code class="language-julia hljs">q5 = trust_regions(M, f, grad_f, (M, Y, p, X) -&gt; Hess_f(M, Y, p, X), p0;
    evaluation=InplaceEvaluation(),
    debug = [:Iteration, :Cost, :GradientNorm, 10, &quot;\n&quot;],
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): -0.000727
# 10    f(x): -0.089234|grad f(p)|:0.0013561755210368012</code></pre><p>Letâ€™s also here compare them in benchmarks. Letâ€™s here compare all variants in their (more performant) in-place versions.</p><pre><code class="language-julia hljs">@benchmark trust_regions($M, $f, $grad_f, $p0;
  objective_type=:Euclidean,
  evaluation=InplaceEvaluation(),
)</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 8 samples with 1 evaluation per sample.
 Range (min â€¦ max):  626.868 ms â€¦ 723.111 ms  â”Š GC (min â€¦ max): 7.44% â€¦ 6.05%
 Time  (median):     635.973 ms               â”Š GC (median):    8.17%
 Time  (mean Â± Ïƒ):   645.791 ms Â±  31.647 ms  â”Š GC (mean Â± Ïƒ):  7.80% Â± 0.77%

  â–ˆ â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ                                                  â–ˆ  
  â–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ â–
  627 ms           Histogram: frequency by time          723 ms &lt;

 Memory estimate: 1.96 GiB, allocs estimate: 75417.</code></pre><pre><code class="language-julia hljs">@benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -&gt; Hess_f(E, Y, p, X)), $p0;
  evaluation=InplaceEvaluation(),
  objective_type=:Euclidean
)</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 225 samples with 1 evaluation per sample.
 Range (min â€¦ max):  18.936 ms â€¦ 38.344 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 0.00%
 Time  (median):     21.878 ms              â”Š GC (median):    9.72%
 Time  (mean Â± Ïƒ):   22.276 ms Â±  2.463 ms  â”Š GC (mean Â± Ïƒ):  9.19% Â± 3.05%

  â–‚    â–„â–„â–„â–‡â–ˆâ–ˆâ–‡â–…â–„                                               
  â–ˆâ–…â–â–…â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–â–â–…â–…â–…â–â–…â–…â–â–…â–â–â–â–…â–â–â–â–â–…â–â–â–â–â–…â–â–â–â–…â–â–â–…â–â–â–â–â–â–â–â–â–â–… â–†
  18.9 ms      Histogram: log(frequency) by time      36.2 ms &lt;

 Memory estimate: 44.92 MiB, allocs estimate: 12651.</code></pre><pre><code class="language-julia hljs">@benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -&gt; Hess_f(M, Y, p, X)), $p0;
    evaluation=InplaceEvaluation(),
)</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 339 samples with 1 evaluation per sample.
 Range (min â€¦ max):  12.741 ms â€¦ 27.930 ms  â”Š GC (min â€¦ max): 0.00% â€¦ 27.68%
 Time  (median):     14.477 ms              â”Š GC (median):    7.50%
 Time  (mean Â± Ïƒ):   14.761 ms Â±  1.996 ms  â”Š GC (mean Â± Ïƒ):  7.25% Â±  5.24%

  â–â–‚ â–‚â–„â–…â–ƒ  â–‡â–ˆ â–â–ƒâ–ƒ                                              
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–„â–„â–ƒâ–„â–…â–â–â–â–„â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–‚â–‚â–‚ â–ƒ
  12.7 ms         Histogram: frequency by time        23.3 ms &lt;

 Memory estimate: 16.35 MiB, allocs estimate: 12633.</code></pre><p>We see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians.</p><p>Of course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged.</p><pre><code class="language-julia hljs">[distance(M, q1, q) for q âˆˆ [q2,q3] ]</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 8.835276992173579e-16
 0.8953986994946139</code></pre><pre><code class="language-julia hljs">[distance(M, q3, q) for q âˆˆ [q4,q5] ]</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 1.1159334410358788
 1.1159334410358788</code></pre><p>Which we can also see in the final cost, comparing it to the Eigenvalue</p><pre><code class="language-julia hljs">[f(M, q) - Î» for q âˆˆ [q1, q2, q3, q4, q5] ]</code></pre><pre><code class="nohighlight hljs">5-element Vector{Float64}:
  6.211293387550776e-5
  6.211293387559103e-5
  0.0001451688142172225
 -5.551115123125783e-17
 -5.551115123125783e-17</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute.</p><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[Bou23]</dt><dd><div>N.Â Boumal. <a href="https://www.nicolasboumal.net/#book"><em>An Introduction to Optimization on Smooth Manifolds</em></a>. FirstÂ Edition (<a href="https://doi.org/10.1017/9781009166164">Cambridge University Press, 2023</a>).</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../HyperparameterOptimization/">Â« Hyperparameter optimziation</a><a class="docs-footer-nextpage" href="../Riemannian-mean/">Riemannian Mean Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 11 February 2025 16:09">Tuesday 11 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
