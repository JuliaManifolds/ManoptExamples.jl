var documenterSearchIndex = {"docs":
[{"location":"examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","page":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","text":"Ronny Bergmann 2023-11-06","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Introduction","page":"Frank Wolfe comparison","title":"Introduction","text":"In this example we compare the Difference of Convex Algorithm (DCA) [BFSS24] with the Frank-Wolfe Algorithm, which was introduced in [WS22]. This example reproduces the results from [BFSS24], Section 7.3.\n\nusing LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\n\nand we load a few nice colors\n\npaul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]\n\nWe consider the following constraint maximization problem of the Fréchet mean on the symmetric positive definite matrices mathcal P(n) with the affine invariant metric. Let q_1ldotsq_m in mathcal P(n) be a set of points and mu_1ldotsmu_m be a set of weights, such that they sum to one. We consider then\n\noperatorname*argmax_pinmathcal C  h(p)\n\nwith\n\nh(p) =\nsum_j=1^m mu_j d^2(pq_i)\nquad text where \nd^2(pq_i) = operatornametrbigl(\n  log^2(p^-frac12q_jp^-frac12)\nbig)\nqquadtextandqquad\nmathcal C =  pin mathcal M  bar Lpreceq p preceq bar U \n\nfor a lower bound L and an upper bound U for the matrices in the positive definite sense A preceq B Leftrightarrow (B-A) is positive semi-definite\n\nWhen every one of the weights mu_1 ldots mu_m are equal, this function h is known as the of the set q_1 dots q_m.\n\nAnd for our example we set\n\nRandom.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w)\n\nWe use as lower and upper bound the arithmetic and geometric mean L and U, respectively.\n\nL = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) )\n\nAs a starting point, the Frank-Wolfe algorithm requires a feasible point. We use\n\np0 = (L+U)/2\n\nAnd we can check that it is feasible","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","page":"Frank Wolfe comparison","title":"Common Functions","text":"Given p in mathcal M, X in T_pmathcal M on the symmetric positive definite matrices M, this method computes the closed form solution to\n\noperatorname*argmin_qin  mathcal C langle X log_p qrangle\n  = operatorname*argmin_qin  mathcal C operatornametr(Slog(YqY))\n\nwhere mathcal C =  q  L preceq q preceq U , S = p^-12Xp^-12, and Y=p^-12.\n\nThe solution is given by Z=X^-1Qbigl( P^mathrmT-operatornamesgn(D)_+P+hatLbigr)Q^mathrmTX^-1,@ where S=QDQ^mathrmT is a diagonalization of S, hatU-hatL=P^mathrmTP with hatL=Q^mathrmTXLXQ and hatU=Q^mathrmTXUXQ, where -mboxsgn(D)_+ is the diagonal matrix\n\noperatornamediagbigl(\n  -operatornamesgn(d_11)_+ ldots -operatornamesgn(d_nn)_+\nbigr)\n\nand D=(d_ij).\n\n@doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","page":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","text":"We use g(p) = iota_mathcal C(p) as the indicator function of the set mathcal C. We use\n\nfunction is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) )\n\nSo we can first check that p0 is feasible\n\ng(p0,L,U) == 0.0\n\ntrue\n\nNow setting\n\noperatorname*argmin_pinmathcal M g(p) - h(p)\n\nWe look for a maximum of h, where g is minimal, i.e. g(p) is zero or in other words p in mathcal C.\n\nThe gradient of h can also be implemented in closed form as\n\ngrad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend\n\nAnd we can further define the cost, which will just be +infty outside of mathcal C. We define\n\nf_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\n\nHere we can omit the gradient of g in the definition of operatornamegrad f, since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of mathcal C.\n\nAs the last step, we can provide the closed form solver for the DC sub problem given at iteration k by\n\noperatorname*argmin_pin mathcal C\n  biglangle -operatornamegrad h(p^(k)) exp^-1_p^(k)pbigrangle\n\nWhich we con compute\n\nfunction dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend\n\nFor safety, we might want to avoid ending up at the boundary of mathcal C. That is we reduce the distance we walk towards the solution q a bit.\n\nfunction dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    α = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= α\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($α)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","page":"Frank Wolfe comparison","title":"The DoC solver run","text":"Let’s compare both methods when they have the same stopping criteria\n\n@time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(M, 1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)\n\nInitial F(p): -0.81628444040735 |grad f(p)|: 0.00000000 \n# 30       F(p): -0.82476426867975 |Δp|: 0.08995893096792  |grad f(p)|: 0.18471629  |Δgrad f(p)|: 0.18414762\n# 60       F(p): -0.82474701550902 |Δp|: 0.02777382004997  |grad f(p)|: 0.18453262  |Δgrad f(p)|: 0.05679999\n# 90       F(p): -0.82473955142600 |Δp|: 0.01323514050663  |grad f(p)|: 0.18444796  |Δgrad f(p)|: 0.02766167\n# 120      F(p): -0.82473728355230 |Δp|: 0.00759483209486  |grad f(p)|: 0.18442329  |Δgrad f(p)|: 0.01510815\nAt iteration 129 the change of the gradient (2.3583539765525604e-13) was less than 1.0e-9.\n 19.169905 seconds (43.63 M allocations: 3.878 GiB, 4.17% gc time, 82.68% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 129 iterations\n\n## Parameters\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{Manopt.InplaceEvaluation}()\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * |Δp| < 1.0e-14: not reached\n  * |Δgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 30]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=ManifoldsBase.LogarithmicInverseRetraction())]),)\n\nLet’s extract the final point and look at its cost\n\np1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc)\n\n-0.8247367071753671\n\nAs well as whether (and how well) it is feasible, that is the following values should all be larger than zero.\n\n[ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)]\n\n2-element Vector{Tuple{Float64, Float64}}:\n (4.968516688516737e-14, 0.07131706912126361)\n (1.7139967154158035e-9, 0.06356415609937403)\n\nFor the statistics we extract the recordings from the state","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","page":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","text":"For Frank wolfe, the cost is just defined as -h(p) but the minimisation is constraint to mathcal C, which is enforced by the oracle.\n\nf_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","page":"Frank Wolfe comparison","title":"The FW Solver Run","text":"Similarly we can run the Frank-Wolfe algorithm with\n\n@time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(M, 1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)\n\nInitial f(x): -0.816284 |grad f(p)|: 0.00135537 \n# 2000     f(x): -0.824757 |Δp|: 0.06259164233805  |grad f(p)|: 0.18462554  |Δgrad f(p)|: 0.18383392\n# 4000     f(x): -0.824760 |Δp|: 0.00533792248763  |grad f(p)|: 0.18465855  |Δgrad f(p)|: 0.01075022\n# 6000     f(x): -0.824761 |Δp|: 0.00292669259433  |grad f(p)|: 0.18467288  |Δgrad f(p)|: 0.00589304\n# 8000     f(x): -0.824762 |Δp|: 0.00199428568529  |grad f(p)|: 0.18468099  |Δgrad f(p)|: 0.00401528\n# 10000    f(x): -0.824762 |Δp|: 0.00150207512138  |grad f(p)|: 0.18468619  |Δgrad f(p)|: 0.00302414\nAt iteration 10000 the algorithm reached its maximal number of iterations (10000).\n149.255815 seconds (122.04 M allocations: 92.575 GiB, 6.12% gc time, 1.08% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: ManifoldsBase.LogarithmicInverseRetraction()\n* retraction method: ManifoldsBase.ExponentialRetraction()\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{Manopt.InplaceEvaluation}()\n\n## Stepsize\nDecreasingLength(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2.0, type=relative)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000:    reached\n  * |Δp| < 1.0e-14: not reached\n  * |Δgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"f(x): %f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 2000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=ManifoldsBase.LogarithmicInverseRetraction())]),)\n\nAnd we take a look at this result as well\n\np1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw)\n\n-0.8247621344833183\n\nAnd its feasibility\n\n[extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)]\n\n2-element Vector{Tuple{Float64, Float64}}:\n (5.358772865048796e-10, 0.07048653710020873)\n (5.6534370618234825e-6, 0.06727144535674022)","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Statistics","page":"Frank Wolfe comparison","title":"Statistics","text":"We extract the recorded values\n\n# DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q))\n\nAnd let’s plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains.\n\nfig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\")\n\n(Image: )\n\nThis indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again.\n\nOn the other hand, Frank-Wolfe still has not reached this level function value after 10^4 iterations.","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Technical-details","page":"Frank Wolfe comparison","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:4:25.","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Literature","page":"Frank Wolfe comparison","title":"Literature","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Journal of Optimization Theory and Applications (2024).\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\n","category":"section"},{"location":"references/#Literature","page":"References","title":"Literature","text":"T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2019), arXiv:1907.10902.\n\n\n\nS. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nM. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, O. P. Ferreira, S. Z. Németh and J. Zhu. On projection mappings and the gradient projection method                on hyperbolic space forms, arXiv preprint (2025).\n\n\n\nR. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Journal of Optimization Theory and Applications (2024).\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization, preprint (2025), arXiv:2507.16055.\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Nononvex Optimization, preprint (2025), arXiv:2506.09775.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\nW. Huang and K. Wei. Riemannian proximal gradient methods. Mathematical Programming 194, 371–413 (2021).\n\n\n\nH. Jasa, R. Bergmann, C. Kümmerle, A. Athreya and Z. Lubberts. Procrustes Problems on Random Matrices, preprint (2025), arXiv:2510.05182.\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\nJ. Li, S. Ma and T. Srivastava. A Riemannian ADMM (2022).\n\n\n\nC. Liu and N. Boumal. Simple algorithms for optimization on Riemannian manifolds with constraints. Applied Mathematics & Optimization (2019), arXiv:1091.10000.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\nL. Weigl, R. Bergmann and A. Schiela. Newton's method into vector bundles Part II: : Application to Variational Problems on Manifolds, arXiv Preprint (2025).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"section"},{"location":"examples/Spectral-Procrustes/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM-for-solving-the-spectral-Procrustes-problem","page":"Spectral Procrustes","title":"A comparison of the RCBM with the PBA, the SGM for solving the spectral Procrustes problem","text":"Hajg Jasa 2024-06-27","category":"section"},{"location":"examples/Spectral-Procrustes/#Introduction","page":"Spectral Procrustes","title":"Introduction","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FO98], to solve the spectral Procrustes problem on mathrmSO(250). This example reproduces the results from [BHJ24], Section 5.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/Spectral-Procrustes/#The-Problem","page":"Spectral Procrustes","title":"The Problem","text":"Given two matrices A B in mathbb R^n times d we aim to solve the Procrustes problem\n\n    argmin_p in mathrmSO(d) Vert A - B  p Vert_2\n    \n\nwhere mathrmSO(d) is equipped with the standard bi-invariant metric, and where Vert cdot Vert_2 denotes the spectral norm of a matrix, , its largest singular value. We aim to find the best matrix p in mathbb R^d times d such that p^top p = mathrmid is the identity matrix, or in other words p is the best rotation. Note that the spectral norm is convex in the Euclidean sense, but not geodesically convex on mathrmSO(d). Let us define the objective as\n\n    f (p)\n    =\n    Vert A - B  p Vert_2\n    \n\nTo obtain subdifferential information, we use\n\n    mathrmproj_p(-B^top UV^top)\n\nas a substitute for partial f(p), where U and V are some left and right singular vectors, respectively, corresponding to the largest singular value of A - B  p, and mathrmproj_p is the projection onto\n\n    mathcal T_p mathrmSO(d)\n    =\n    \n    A in mathbb R^dd  vert  pA^top + Ap^top = 0  mathrmtrace(p^-1A)=0\n    \n    ","category":"section"},{"location":"examples/Spectral-Procrustes/#Numerical-Experiment","page":"Spectral Procrustes","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\nRandom.seed!(33)\nn = 1000\nd = 250\nA = rand(n, d)\nB = randn(n, d)\ntol = 1e-8\nmax_iters = 5000\n#\n# Compute the orthogonal Procrustes minimizer given A and B\nfunction orthogonal_procrustes(A, B)\n    s =  svd((A'*B)')\n    R = s.U* s.Vt\n    return R\nend\n#\n# Algorithm parameters\nk_max = 1/4\nk_min = 0.0\ndiameter = π/(3 * √k_max)\n#\n# Manifolds and data\nM = SpecialOrthogonal(d)\np0 = orthogonal_procrustes(A, B)\nproject!(M, p0, p0)\n\nWe now define objective and subdifferential (first the Euclidean one, then the projected one).\n\nf(M, p) = opnorm(A - B*p)\nfunction ∂ₑf(M, p)\n    cost_svd = svd(A - B*p)\n    # Find all maxima in S – since S is sorted, these are the first n ones\n    indices = [i for (i, v) in enumerate(cost_svd.S) if abs(v - cost_svd.S[1]) < eps()]\n    ind = rand(indices)\n    return -B'*(cost_svd.U[:,ind]*cost_svd.Vt[ind,:]')\nend\nrpb = Manifolds.RiemannianProjectionBackend(Manifolds.ExplicitEmbeddedBackend(M; gradient=∂ₑf))\n∂f(M, p) = Manifolds.gradient(M, f, p, rpb)\ndomf(M, p) = distance(M, p, p0) < diameter/2 ? true : false\n\nWe introduce some keyword arguments for the solvers we will use in this experiment\n\nrcbm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ξ, \"ξ: %1.8f \"),\n        (:ε, \"ε: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :WarnBundle,\n        :Stop,\n        10,\n        \"\\n\",\n    ],\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug =>[\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        :WarnBundle,\n        10,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :p_star],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√tol) | StopAfterIteration(max_iters),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√tol) | StopAfterIteration(max_iters),\n]\nglobal header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\"]\n\nWe run the optimization algorithms…\n\nrcbm = convex_bundle_method(M, f, ∂f, p0; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(M, f, ∂f, p0; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(M, f, ∂f, p0; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm)\n\n… And we benchmark their performance.\n\nif benchmarking\n    pba_bm = @benchmark proximal_bundle_method($M, $f, $∂f, $p0; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($M, $f, $∂f, $p0; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($M, $f, $∂f, $p0; $sgm_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\"]\n    records = [rcbm_record, pba_record, sgm_record]\n    results = [rcbm_result, pba_result, sgm_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n    ]\n    if show_plot\n        global fig = plot(xscale=:log10)\n    end\n    #\n    global D = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records];\n        dims=2,\n    )\n    # \n    \n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            C1 = [0.5 f(M, p0)]\n            C = cat(first.(record), [r[2] for r in record]; dims=2)\n            bm_data = vcat(C1, C)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(bm_data, :auto);\n                header=[\"i\", \"cost\"],\n            )\n            if show_plot\n                plot!(fig, bm_data[:,1], bm_data[:,2]; label=experiment)\n            end\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(D, :auto);\n            header=header,\n        )\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table…\n\nAlgorithm Iterations Time (s) Objective\nRCBM 99 102.036 235.46\nPBA 31 5.8049 235.46\nSGM 5000 402.739 235.46\n\n… and this cost versus iterations plot\n\n(Image: )","category":"section"},{"location":"examples/Spectral-Procrustes/#Technical-details","page":"Spectral Procrustes","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nusing Pkg\nPkg.status()\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [336ed68f] CSV v0.10.15\n  [35d6a980] ColorSchemes v3.27.1\n⌅ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.26.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.1\n  [d3d80556] LineSearches v7.3.0\n  [af67fdf4] ManifoldDiff v0.3.13\n  [1cead3c2] Manifolds v0.10.7\n  [3362f125] ManifoldsBase v0.15.22\n  [0fc0a36d] Manopt v0.5.3 `../../Manopt.jl`\n  [5b8d5e80] ManoptExamples v0.1.10 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.40.9\n⌃ [08abe8d2] PrettyTables v2.3.2\n  [6099a3de] PythonCall v0.9.23\n  [f468eda6] QuadraticModels v0.9.7\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n\nusing Dates\nnow()\n\n2024-11-29T09:36:53.667","category":"section"},{"location":"examples/Spectral-Procrustes/#Literature","page":"Spectral Procrustes","title":"Literature","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"section"},{"location":"contributing/#Contributing-to-Manopt.jl","page":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.\n\nThe following is a set of guidelines to ManoptExamples.jl.","category":"section"},{"location":"contributing/#Table-of-Contents","page":"Contributing to ManoptExamples.jl","title":"Table of Contents","text":"Contributing to Manopt.jl\nTable of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd an objective\nCode style","category":"section"},{"location":"contributing/#I-just-have-a-question","page":"Contributing to ManoptExamples.jl","title":"I just have a question","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on our GitHub discussion.","category":"section"},{"location":"contributing/#How-can-I-file-an-issue?","page":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"section"},{"location":"contributing/#How-can-I-contribute?","page":"Contributing to ManoptExamples.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing/#Add-an-objective","page":"Contributing to ManoptExamples.jl","title":"Add an objective","text":"The objective in Manopt.jl represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a Problem.\n\nIf you have a specific objective you would like to provide here, feel free to start a new file in the src/objectives/ folder in your own fork and propose it later as a Pull Request.\n\nIf you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in src/functions/ and follows the naming scheme:\n\ncost functions are always of the form cost_ and a fitting name\ngradient functions are always of the gradient_ and a fitting name, followed by an !\n\nfor in-place gradients and by !! if it is a struct that can provide both.\n\nIt would be great if you could also add a small test for the functions and the problem you defined in the test/ section.","category":"section"},{"location":"contributing/#Add-an-example","page":"Contributing to ManoptExamples.jl","title":"Add an example","text":"If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding Quarto Markdown file to the examples/ folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the bib/literature.yaml file to add references (in CSL_YAML, which can for example be exported e.g. from Zotero).\n\nAdd any packages you need to the examples/ environment (see the containting Project.toml). The examples will not be run on CI, but their rendered CommonMark outpout should be included in the list of examples in the documentation of this package.","category":"section"},{"location":"contributing/#Code-style","page":"Contributing to ManoptExamples.jl","title":"Code style","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.\n\nWe also follow a few internal conventions:\n\nAny implemented function should be accompanied by its mathematical formulae if a closed form exists.\nwithin a file the structs should come first and functions second. The only exception are constructors for the structs\nwithin both blocks an alphabetical order is preferable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including in-place/non-mutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature.\nAll import/using/include should be in the main module file.\nThere should only be a minimum of exports within this file, all problems should usually be later addressed as ManoptExamples.[...]\nthe Quarto Markdown files are excluded from this formatting.","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","page":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","text":"Ronny Bergmann 2023-06-06","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Introduction","page":"A Benchmark","title":"Introduction","text":"In this Benchmark we compare the Difference of Convex Algprithm (DCA) [BFSS24] and the Difference of Convex Proximal Point Algorithm (DCPPA) [SO15] which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from [BFSS24], Section 7.1.\n\noperatorname*argmin_pmathcal M   g(p) - h(p)\n\nwhere ghcolon mathcal M  mathbb R are geodesically convex function on the Riemannian manifold mathcal M.\n\nusing LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42)\n\nand we load a few nice colors\n\npaul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#The-DC-Problem","page":"A Benchmark","title":"The DC Problem","text":"We start with defining the two convex functions gh and their gradients as well as the DC problem f and its gradient for the problem\n\n    operatorname*argmin_pmathcal M  bigl( logbigr(det(p)bigr)bigr)^4 - bigl(log det(p) bigr)^2\n\nwhere the critical points obtain a functional value of -frac14.\n\nwhere mathcal M is the manifold of symmetric positive definite (SPD) matrices with the affine invariant metric, which is the default.\n\nWe first define the corresponding functions\n\ng(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p)\n\nand their gradients\n\ngrad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p)\n\nwhich we can use to verify that the gradients of g and h are correct. We use for that\n\nn = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n);\n\nto tall both checks\n\ncheck_gradient(M, g, grad_g, p0, X0; plot=true)\n\n(Image: )\n\nand\n\ncheck_gradient(M, h, grad_h, p0, X0; plot=true)\n\n(Image: )\n\nwhich both pass the test. We continue to define their in-place variants\n\nfunction grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\n\nAnd compare times for both algorithms, with a bit of debug output.\n\n@time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |δp|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);\n\nInitial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |δp|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |δp|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |δp|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000005 |δp|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (7.619584706652928e-11) is less than 1.0e-10.\n  3.711502 seconds (22.45 M allocations: 1.173 GiB, 7.83% gc time, 99.46% compilation time)\n\nThe cost is\n\nf(M, p_min_dca)\n\n-0.25000000000000006\n\nSimilarly the DCPPA performs\n\n@time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    λ=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|δp|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantLength(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);\n\nInitial f(p): 137.679053470 \n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|δp|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|δp|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|δp|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|δp|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|δp|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|δp|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|δp|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.005187 seconds (5.32 M allocations: 306.021 MiB, 2.72% gc time, 98.51% compilation time)\n\nIt needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost\n\nf(M, p_min_dcppa)\n\n-0.25","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","page":"A Benchmark","title":"Benchmark I: Time comparison","text":"We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test\n\ndca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max\n\nand run a benchmark for both algorithms\n\nfor n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        λ=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantLength(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend\n\nSince we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds\n\ndims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N]\n\nplot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)\n\n(Image: )","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","page":"A Benchmark","title":"Benchmark II: Iterations and cost.","text":"As a second benchmark, let’s collect the number of iterations needed and the development of the cost over dimensions.\n\nN2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}()\n\n@time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        λ = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantLength(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend\n\nThe iterations are like\n\nplot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2)\n\n(Image: )\n\nAnd for the developtment of the cost\n\n(Image: )\n\nwhere we can see that the DCA needs less iterations than the DCPPA.","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Technical-details","page":"A Benchmark","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n⌅ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n  [7073ff75] IJulia v1.30.6\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n⌅ [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 11, 2025, 17:12:28.","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/#Literature","page":"A Benchmark","title":"Literature","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Journal of Optimization Theory and Applications (2024).\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\n","category":"section"},{"location":"examples/RCBM-Median/#A-comparison-of-the-RCBM-with-the-PBA-and-the-SGM-for-the-Riemannian-median","page":"Riemannian Median","title":"A comparison of the RCBM with the PBA and the SGM for the Riemannian median","text":"Hajg Jasa 2024-06-27","category":"section"},{"location":"examples/RCBM-Median/#Introduction","page":"Riemannian Median","title":"Introduction","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FO98], to find the Riemannian median. This example reproduces the results from [BHJ24], Section 5. The runtimes reported in the tables are measured in seconds.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing LinearAlgebra, LRUCache, Random\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples\n\nLet mathcal M be a Hadamard manifold and q_1ldotsq_N in mathcal M denote N = 1000 Gaussian random data points. Let f colon mathcal M to mathbb R be defined by\n\nf(p) = sum_j = 1^N w_j  mathrmdist(p q_j)\n\nwhere w_j, j = 1 ldots N are positive weights such that sum_j = 1^N w_j = 1.\n\nThe Riemannian geometric median p^* of the dataset\n\nmathcal D = \n    q_1ldotsq_N  vert  q_j in mathcal Mtext for all  j = 1ldotsN\n\n\nis then defined as\n\n    p^* coloneqq operatorname*argmin_p in mathcal M f(p)\n\nwhere equality is justified since p^* is uniquely determined on Hadamard manifolds. In our experiments, we choose the weights w_j = frac1N.\n\nWe initialize the experiment parameters, as well as utility functions.\n\nexperiment_name = \"RCBM-Median\"\nresults_folder = joinpath(@__DIR__, experiment_name)\n!isdir(results_folder) && mkdir(results_folder)\nseed_argument = 57\n\natol = 1e-8\nN = 1000 # number of data points\nspd_dims = [2, 5, 10, 15]\nhn_sn_dims = [1, 2, 5, 10, 15]\n\n# Generate a point that is at most `tol` close to the point `p` on `M`\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend\n\n# Objective and subdifferential\nf(M, p, data) = sum(1 / length(data) * distance.(Ref(M), Ref(p), data))\ndomf(M, p, centroid, diameter) = distance(M, p, centroid) < diameter / 2 ? true : false\nfunction ∂f(M, p, data, atol=atol)\n    return sum(\n        1 / length(data) *\n        ManifoldDiff.subgrad_distance.(Ref(M), data, Ref(p), 1; atol=atol),\n    )\nend\n\nmaxiter = 5000\nrcbm_kwargs(diameter, domf, k_max, k_min) = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :domain => domf,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ξ, \"ξ: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :diameter => diameter,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs(diameter, domf, k_max, k_min) = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(maxiter),\n]\npba_bm_kwargs = [:cache => (:LRU, [:Cost, :SubGradient], 50),]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(maxiter),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(maxiter),\n]\n\nBefore running the experiments, we initialize data collection functions that we will use later\n\nglobal col_names_1 = [\n    :Dimension,\n    :Iterations_1,\n    :Time_1,\n    :Objective_1,\n    :Iterations_2,\n    :Time_2,\n    :Objective_2,\n]\ncol_types_1 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)\nglobal col_names_2 = [\n    :Dimension,\n    :Iterations,\n    :Time,\n    :Objective,\n]\ncol_types_2 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_2 = (; zip(col_names_2, type[] for type in col_types_2 )...)\nfunction initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1, named_tuple_2)\n    A1 = DataFrame(named_tuple_1)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Convex-Prox.csv\",\n        ),\n        A1;\n        header=false,\n    )\n    A2 = DataFrame(named_tuple_2)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Subgrad.csv\",\n        ),\n        A2;\n        header=false,\n    )\n    return A1, A2\nend\n\nfunction export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1, col_names_2)\n    B1 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations_1=maximum(first.(records[1])),\n        Time_1=times[1],\n        Objective_1=minimum([r[2] for r in records[1]]),\n        Iterations_2=maximum(first.(records[2])),\n        Time_2=times[2],\n        Objective_2=minimum([r[2] for r in records[2]]),\n    )\n    B2 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations=maximum(first.(records[3])),\n        Time=times[3],\n        Objective=minimum([r[2] for r in records[3]]),\n    )\n    return B1, B2\nend\nfunction write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Convex-Prox.csv\",\n        ),\n        B1;\n        append=true,\n    )\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Subgrad.csv\",\n        ),\n        B2;\n        append=true,\n    )\nend","category":"section"},{"location":"examples/RCBM-Median/#The-Median-on-the-Hyperboloid-Model","page":"Riemannian Median","title":"The Median on the Hyperboloid Model","text":"subexperiment_name = \"Hn\"\nk_max_hn = -1.0\nk_min_hn = -1.0\n\nglobal A1, A2 = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n    Random.seed!(seed_argument)\n\n    M = Hyperbolic(Int(2^n))\n    data_hn = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data_hn, y in data_hn]\n    diameter_hn = 2 * maximum(dists)\n    p0 = data_hn[minimum(Tuple(findmax(dists)[2]))]\n\n    f_hn(M, p) = f(M, p, data_hn)\n    domf_hn(M, p) = domf(M, p, p0, diameter_hn)\n    ∂f_hn(M, p) = ∂f(M, p, data_hn, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_hn, ∂f_hn, p0; rcbm_kwargs(diameter_hn, domf_hn, k_max_hn, k_min_hn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_hn, ∂f_hn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_hn, ∂f_hn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_hn, $∂f_hn, $p0; rcbm_bm_kwargs($diameter_hn, $domf_hn, $k_max_hn, $k_min_hn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_hn, $∂f_hn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_hn, $∂f_hn, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1, B2 = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1, B1)\n        append!(A2, B2)\n        (export_table) && (write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name))\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…\n\nDimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n2 9 0.00374917 1.05192 251 0.0876468 1.05192\n4 8 0.00349267 1.07516 230 0.0974517 1.07516\n32 15 0.0113675 1.08403 234 0.141856 1.08403\n1024 16 0.170142 1.09479 234 2.65338 1.09479\n32768 23 8.611 1.08865 225 86.0427 1.08865\n\n… Whereas the following table refers to the SGM\n\nDimension Iterations Time Objective\n2 18 0.00559008 1.04748\n4 19 0.00665521 1.02366\n32 23 0.0143063 1.08229\n1024 23 0.24613 1.09479\n32768 19 6.84741 1.08865","category":"section"},{"location":"examples/RCBM-Median/#The-Median-on-the-Symmetric-Positive-Definite-Matrix-Space","page":"Riemannian Median","title":"The Median on the Symmetric Positive Definite Matrix Space","text":"subexperiment_name = \"SPD\"\nk_max_spd = 0.0\nk_min_spd = -1/2\n\nglobal A1_SPD, A2_SPD = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in spd_dims\n    Random.seed!(seed_argument)\n\n    M = SymmetricPositiveDefinite(Int(n))\n    data_spd = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data_spd, y in data_spd]\n    diameter_spd = 2 * maximum(dists)\n    p0 = data_spd[minimum(Tuple(findmax(dists)[2]))]\n\n    f_spd(M, p) = f(M, p, data_spd)\n    domf_spd(M, p) = domf(M, p, p0, diameter_spd)\n    ∂f_spd(M, p) = ∂f(M, p, data_spd, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_spd, ∂f_spd, p0; rcbm_kwargs(diameter_spd, domf_spd, k_max_spd, k_min_spd)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_spd, ∂f_spd, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_spd, ∂f_spd, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_spd, $∂f_spd, $p0; rcbm_bm_kwargs($diameter_spd, $domf_spd, $k_max_spd, $k_min_spd)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_spd, $∂f_spd, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_spd, $∂f_spd, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_SPD, B2_SPD = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_SPD, B1_SPD)\n        append!(A2_SPD, B2_SPD)\n        (export_table) && (write_dataframes(B1_SPD, B2_SPD, results_folder, experiment_name, subexperiment_name))\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…\n\nDimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n3 38 0.249011 0.260846 57 0.346319 0.260846\n15 57 1.27281 0.436536 75 1.34131 0.436536\n55 14 46.8956 0.618086 89 3.99824 0.618086\n120 12 0.946472 0.761396 123 10.8884 0.761396\n\n… Whereas the following table refers to the SGM\n\nDimension Iterations Time Objective\n3 4709 25.4934 0.260846\n15 1727 29.7775 0.436536\n55 776 34.2142 0.618086\n120 440 38.0669 0.761396","category":"section"},{"location":"examples/RCBM-Median/#The-Median-on-the-Sphere","page":"Riemannian Median","title":"The Median on the Sphere","text":"For the last experiment, note that a major difference here is that the sphere has constant positive sectional curvature equal to 1. In this case, we lose the global convexity of the Riemannian distance and thus of the objective. Minimizers still exist, but they may, in general, be non-unique.\n\nsubexperiment_name = \"Sn\"\nk_max_sn = 1.0\nk_min_sn = 1.0\ndiameter_sn = π / 3\n\nglobal A1_Sn, A2_Sn = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n    Random.seed!(seed_argument)\n\n    M = Sphere(Int(2^n))\n    north = [0.0 for _ in 1:manifold_dimension(M)]\n    push!(north, 1.0)\n    data_sn = [close_point(M, north, diameter_sn / 2)]\n    distance(M, data_sn[1], north) < diameter_sn / 2 ? pop!(data_sn) : nothing\n    while length(data_sn) < N\n        q = close_point(M, north, diameter_sn / 2)\n        distance(M, q, north) < diameter_sn / 2 ? push!(data_sn, q) : nothing\n    end\n    dists = [distance(M, z, y) for z in data_sn, y in data_sn]\n    p0 = data_sn[minimum(Tuple(findmax(dists)[2]))]\n\n    f_sn(M, p) = f(M, p, data_sn)\n    domf_sn(M, p) = domf(M, p, north, diameter_sn)\n    ∂f_sn(M, p) = ∂f(M, p, data_sn, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_sn, ∂f_sn, p0; rcbm_kwargs(diameter_sn, domf_sn, k_max_sn, k_min_sn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_sn, ∂f_sn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_sn, ∂f_sn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_sn, $∂f_sn, $p0; rcbm_bm_kwargs($diameter_sn, $domf_sn, $k_max_sn, $k_min_sn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $∂f_sn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_sn, $∂f_sn, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_Sn, B2_Sn = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_Sn, B1_Sn)\n        append!(A2_Sn, B2_Sn)\n        (export_table) && (write_dataframes(B1_Sn, B2_Sn, results_folder, experiment_name, subexperiment_name))\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA…\n\nDimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2\n2 53 0.0146394 0.258898 71 0.0157284 0.258898\n4 76 0.0203875 0.253525 62 0.0148905 0.253525\n32 92 0.0398844 0.260417 67 0.0259528 0.260417\n1024 100 43.7537 0.26468 73 0.646818 0.26468\n32768 5000 1143.88 0.278525 69 13.112 0.264932\n\n… Whereas the following table refers to the SGM\n\nDimension Iterations Time Objective\n2 401 0.0936217 0.258898\n4 5000 1.19094 0.253525\n32 241 0.0916558 0.260417\n1024 169 1.24458 0.26468\n32768 209 27.4642 0.264932","category":"section"},{"location":"examples/RCBM-Median/#Technical-details","page":"Riemannian Median","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 16, 2025, 13:16:19.","category":"section"},{"location":"examples/RCBM-Median/#Literature","page":"Riemannian Median","title":"Literature","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"section"},{"location":"examples/HyperparameterOptimization/#Hyperparameter-optimization","page":"Hyperparameter optimziation","title":"Hyperparameter optimization","text":"Mateusz Baran 2024-08-03","category":"section"},{"location":"examples/HyperparameterOptimization/#Introduction","page":"Hyperparameter optimziation","title":"Introduction","text":"This example shows how to automatically select the best values of hyperparameters of optimization procedures such as retraction, vector transport, size of memory in L-BFGS or line search coefficients. Hyperparameter optimization relies on the Optuna [ASY+19] Python library because it is much more advanced than similar Julia projects, offering Bayesian optimization with conditional hyperparameters and early stopping.","category":"section"},{"location":"examples/HyperparameterOptimization/#General-definitions","page":"Hyperparameter optimziation","title":"General definitions","text":"Here are some general definitions that you will most likely be able to directly use for your problem without any changes. Just remember to install optuna, for example using CondaPkg Julia library.\n\nusing Manifolds, Manopt\nusing PythonCall\nusing BenchmarkTools\nusing LineSearches\n\n# This script requires optuna to be available through PythonCall\n# You can install it for example using\n# using CondaPkg\n# ]conda add optuna\n# this has to be done in the Julia environment in which the notebook is rendered\n# *before* running the notebook\n\noptuna = pyimport(\"optuna\")\n\nnorm_inf(M::AbstractManifold, p, X) = norm(X, Inf)\n\n# TTsuggest_ structs collect data from a calibrating optimization run\n# that is handled by compute_pruning_losses function\n\nstruct TTsuggest_int\n    suggestions::Dict{String,Int}\nend\nfunction (s::TTsuggest_int)(name::String, a, b)\n    return s.suggestions[name]\nend\nstruct TTsuggest_float\n    suggestions::Dict{String,Float64}\nend\nfunction (s::TTsuggest_float)(name::String, a, b; log::Bool=false)\n    return s.suggestions[name]\nend\nstruct TTsuggest_categorical\n    suggestions::Dict{String,Any}\nend\nfunction (s::TTsuggest_categorical)(name::String, vals)\n    return s.suggestions[name]\nend\nstruct TTreport\n    reported_vals::Vector{Float64}\nend\nfunction (r::TTreport)(val, i)\n    return push!(r.reported_vals, val)\nend\nstruct TTshould_prune end\n(::TTshould_prune)() = Py(false)\nstruct TracingTrial\n    suggest_int::TTsuggest_int\n    suggest_float::TTsuggest_float\n    suggest_categorical::TTsuggest_categorical\n    report::TTreport\n    should_prune::TTshould_prune\nend\n\nfunction compute_pruning_losses(\n    od,\n    int_suggestions::Dict{String,Int},\n    float_suggestions::Dict{String,Float64},\n    categorical_suggestions::Dict{String,Int},\n)\n    tt = TracingTrial(\n        TTsuggest_int(int_suggestions),\n        TTsuggest_float(float_suggestions),\n        TTsuggest_categorical(categorical_suggestions),\n        TTreport(Float64[]),\n        TTshould_prune(),\n    )\n    od(tt)\n    return tt.report.reported_vals\nend\n\nThe next part is your hyperparameter optimization objective. The ObjectiveData struct contains all relevant information about the sequence of specific problems. The outermost key part is the N_range field. Early stopping requires a series of progressively more complex problems. They will be attempted from the most simple one to the most complex one, and are specified by the values of N in that vector.\n\nmutable struct ObjectiveData{TObj,TGrad}\n    obj::TObj\n    grad::TGrad\n    N_range::Vector{Int}\n    gtol::Float64\n    vts::Vector{AbstractVectorTransportMethod}\n    retrs::Vector{AbstractRetractionMethod}\n    manifold_constructors::Vector{Tuple{String,Any}}\n    pruning_losses::Vector{Float64}\n    manopt_stepsize::Vector{Tuple{String,Any}}\n    obj_loss_coeff::Float64\nend\n\nIn the example below we optimize hyperparameters on a sequence of Rosenbrock-type problems restricted to spheres:\n\nargmin_p in S^N-1 sum_i=1^N2 (1-p_2i)^2 + 100 (p_2i+1 - p_2i^2)^2\n\nwhere N in 2 16 128 1024 8192 65536.\n\nobj and grad are the objective and gradient, here defined as below. Note that gradient works in-place and variants without manifolds are also provided for easier comparison with other libraries like Optim.jl. It is easiest when problems for different values N can be distinguished by being defined on successively larger manifolds but the script could be modified so that it’s not necessary.\n\npruning_losses and compute_pruning_losses are related to early pruning used in Optuna and you shouldn’t have to modify them.\n\nfunction f_rosenbrock(x)\n    result = 0.0\n    for i in 1:2:length(x)\n        result += (1.0 - x[i])^2 + 100.0 * (x[i + 1] - x[i]^2)^2\n    end\n    return result\nend\nfunction f_rosenbrock(::AbstractManifold, x)\n    return f_rosenbrock(x)\nend\n\nfunction g_rosenbrock!(storage, x)\n    for i in 1:2:length(x)\n        storage[i] = -2.0 * (1.0 - x[i]) - 400.0 * (x[i + 1] - x[i]^2) * x[i]\n        storage[i + 1] = 200.0 * (x[i + 1] - x[i]^2)\n    end\n    return storage\nend\nfunction g_rosenbrock!(M::AbstractManifold, storage, x)\n    g_rosenbrock!(storage, x)\n    riemannian_gradient!(M, storage, x, storage)\n    return storage\nend\n\nNext, gtol is the tolerance used for the stopping criterion in optimization. vts and retrs are, respectively, vector transports and retraction methods selected through hyperparameter optimization. Some items need to be different for different values of N, for example the manifold over which the problem is defined. This is handled by manifold_constructors which is then defined as Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))], where the string \"Sphere\" is used to identify the manifold family and the next element is a function that transforms the value of N to the manifold for the problem of size N.\n\nSimilarly, different stepsize selection methods may be considered. This is handled by the field manopt_stepsize. It will be easiest to see how it works by looking at how it is initialized:\n\nTuple{String,Any}[\n    (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n    (\"Wolfe-Powell\", (M, sufficient_decrease, sufficient_curvature) -> Manopt.WolfePowellLinesearch(; sufficient_decrease = sufficient_decrease, sufficient_curvature = sufficient_curvature)),\n]\n\nWe have a string that identifies the line search method name and a constructor of the line search which takes relevant arguments like the manifold or a numerical parameter.\n\nThe next part is the trial evaluation procedure. This is one of the more important places which need to be customized to your problem. This is the point where we tell Optuna about the relevant optimization hyperparameters and use them to define specific problems. The hyperparameter optimization is a multiobjective problem: we want as good problem objectives as possible and as low times as possible. As Optuna doesn’t currently support multicriteria pruning, which is important for obtaining a solution in a reasonable amount of time, we use a linear combination of sub-objectives to turn the problem into a single-criterion optimization. The hyperparameter optimization objective is a linear combination of achieved objectives the relative weight is controlled by objective.obj_loss_coeff.\n\n\nfunction (objective::ObjectiveData)(trial)\n    # Here we use optuna to select memory length for L-BFGS -- an integer in the range between 2 and 30, referenced by name \"mem_len\"\n    mem_len = trial.suggest_int(\"mem_len\", 2, 30)\n\n    # Here we select a vector transport and retraction methods, one of those specified in the `ObjectiveData`.\n    vt = objective.vts[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"vector_transport_method\", Vector(eachindex(objective.vts))\n        ),\n    )]\n    retr = objective.retrs[pyconvert(\n        Int,\n        trial.suggest_categorical(\"retraction_method\", Vector(eachindex(objective.retrs))),\n    )]\n\n    # Here we select the manifold constructor, in case we want to try different manifolds for our problem. For example one could try defining a problem with orthogonality constraints on Stiefel, Grassmann or flag manifold.\n    manifold_name, manifold_constructor = objective.manifold_constructors[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manifold\", Vector(eachindex(objective.manifold_constructors))\n        ),\n    )]\n\n    # Here the stepsize selection method type is selected.\n    manopt_stepsize_name, manopt_stepsize_constructor = objective.manopt_stepsize[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manopt_stepsize\", Vector(eachindex(objective.manopt_stepsize))\n        ),\n    )]\n\n    # This parametrizes stepsize selection methods with relevant numerical parameters.\n    local sufficient_decrease_val, sufficient_curvature_val, hz_sigma\n    if manopt_stepsize_name == \"Wolfe-Powell\"\n        sufficient_decrease_val = pyconvert(\n            Float64, trial.suggest_float(\"Wolfe-Powell sufficient_decrease\", 1e-5, 1e-2; log=true)\n        )\n        sufficient_curvature_val =\n            1.0 - pyconvert(\n                Float64, trial.suggest_float(\"Wolfe-Powell 1-sufficient_curvature\", 1e-4, 1e-2; log=true)\n            )\n    elseif manopt_stepsize_name == \"Improved HZ\"\n        hz_sigma = pyconvert(Float64, trial.suggest_float(\"Improved HZ sigma\", 0.1, 0.9))\n    end\n\n    # The current loss estimate, taking into account estimated loss values for larger, not-yet-evaluated values of `N`.\n    loss = sum(objective.pruning_losses)\n\n    # Here iterate over problems we want to optimize for\n    # from smallest to largest; pruning should stop the iteration early\n    # if the hyperparameter set is not promising\n    cur_i = 0\n    for N in objective.N_range\n        # Here we define the initial point for the optimization procedure\n        p0 = zeros(N)\n        p0[1] = 1\n        M = manifold_constructor(N)\n        # Here we construct the specific line search to be used\n        local ls\n        if manopt_stepsize_name == \"Wolfe-Powell\"\n            ls = manopt_stepsize_constructor(M, sufficient_decrease_val, sufficient_curvature_val)\n        elseif manopt_stepsize_name == \"Improved HZ\"\n            ls = manopt_stepsize_constructor(M, hz_sigma)\n        else\n            ls = manopt_stepsize_constructor(M)\n        end\n        manopt_time, manopt_iters, manopt_obj = benchmark_time_state(\n            ManoptQN(),\n            M,\n            N,\n            objective.obj,\n            objective.grad,\n            p0,\n            ls,\n            pyconvert(Int, mem_len),\n            objective.gtol;\n            vector_transport_method=vt,\n            retraction_method=retr,\n        )\n        # TODO: turn this into multi-criteria optimization when Optuna starts supporting\n        # pruning in such problems\n        loss -= objective.pruning_losses[cur_i + 1]\n        loss += manopt_time + objective.obj_loss_coeff * manopt_obj\n        trial.report(loss, cur_i)\n        if pyconvert(Bool, trial.should_prune().__bool__())\n            throw(PyException(optuna.TrialPruned()))\n        end\n        cur_i += 1\n    end\n    return loss\nend\n\nIn the following benchmarking code you will most likely have to adapt solver parameters. This is designed around quasi_Newton but can be adapted to any solver as needed. The example below performs a small number of trials for quick rendering but in practice you should aim for at least a few thousand trials (the n_trials parameter).\n\n\n# An abstract type in case we want to try different optimization packages.\nabstract type AbstractOptimConfig end\nstruct ManoptQN <: AbstractOptimConfig end\n\n# Benchmark that evaluates hyperparameters. Returns time to reach the solution, number of iterations and final value of the objective.\nfunction benchmark_time_state(\n    ::ManoptQN,\n    M::AbstractManifold,\n    N,\n    f,\n    g!,\n    p0,\n    stepsize::Union{Manopt.Stepsize,Manopt.ManifoldDefaultsFactory{<:Manopt.Stepsize}},\n    mem_len::Int,\n    gtol::Real;\n    kwargs...,\n)\n    manopt_sc = StopWhenGradientNormLess(gtol; norm=norm_inf) | StopAfterIteration(1000)\n    mem_len = min(mem_len, manifold_dimension(M))\n    manopt_state = quasi_Newton(\n        M,\n        f,\n        g!,\n        p0;\n        stepsize=stepsize,\n        evaluation=InplaceEvaluation(),\n        return_state=true,\n        memory_size=mem_len,\n        stopping_criterion=manopt_sc,\n        debug=[],\n        kwargs...,\n    )\n    bench_manopt = @benchmark quasi_Newton(\n        $M,\n        $f,\n        $g!,\n        $p0;\n        stepsize=$(stepsize),\n        evaluation=$(InplaceEvaluation()),\n        memory_size=$mem_len,\n        stopping_criterion=$(manopt_sc),\n        debug=[],\n        $kwargs...,\n    )\n    iters = get_count(manopt_state, :Iterations)\n    final_val = f(M, manopt_state.p)\n    return median(bench_manopt.times) / 1000, iters, final_val\nend\n\n\"\"\"\n    lbfgs_study(; pruning_coeff::Float64=0.95)\n\nSet up the example hyperparameter optimization study.\n\"\"\"\nfunction lbfgs_study(; pruning_coeff::Float64=0.95)\n    Ns = [2^n for n in 1:3:12]\n    ls_hz = LineSearches.HagerZhang()\n    od = ObjectiveData(\n        f_rosenbrock,\n        g_rosenbrock!,\n        Ns,\n        1e-5,\n        AbstractVectorTransportMethod[ParallelTransport(), ProjectionTransport()],\n        [ExponentialRetraction(), ProjectionRetraction()],\n        Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))],\n        zeros(Float64, eachindex(Ns)),\n        Tuple{String,Any}[\n            (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n            #(\"Improved HZ\", (M, sigma) -> HagerZhangLinesearch(M; sigma=sigma)),\n            (\"Wolfe-Powell\", (M, sufficient_decrease, sufficient_curvature) -> Manopt.WolfePowellLinesearch(; sufficient_decrease = sufficient_decrease, sufficient_curvature = sufficient_curvature)),\n        ],\n        10.0,\n    )\n\n    # Here you need to define baseline values of all hyperparameters\n    baseline_pruning_losses = compute_pruning_losses(\n        od,\n        Dict(\"mem_len\" => 4),\n        Dict(\n            \"Wolfe-Powell sufficient_decrease\" => 1e-4,\n            \"Wolfe-Powell 1-sufficient_curvature\" => 1e-3,\n            \"Improved HZ sigma\" => 0.9,\n        ),\n        Dict(\n            \"vector_transport_method\" => 1,\n            \"retraction_method\" => 1,\n            \"manifold\" => 1,\n            \"manopt_stepsize\" => 1,\n        ),\n    )\n    od.pruning_losses = pruning_coeff * baseline_pruning_losses\n\n    study = optuna.create_study(; study_name=\"L-BFGS\")\n    # Here you can specify number of trials and timeout (in seconds).\n    study.optimize(od; n_trials=1000, timeout=500)\n    println(\"Best params is $(study.best_params) with value $(study.best_value)\")\n    selected_manifold = od.manifold_constructors[pyconvert(Int, study.best_params[\"manifold\"])][1]\n    selected_retraction_method = od.retrs[pyconvert(Int, study.best_params[\"retraction_method\"])]\n    selected_vector_transport = od.vts[pyconvert(Int, study.best_params[\"vector_transport_method\"])]\n    println(\"Selected manifold: $(selected_manifold)\")\n    println(\"Selected retraction method: $(selected_retraction_method)\")\n    println(\"Selected vector transport method: $(selected_vector_transport)\")\n    return study\nend\n\nlbfgs_study()\n\n[I 2025-11-06 15:01:01,300] A new study created in memory with name: L-BFGS\n[I 2025-11-06 15:01:39,567] Trial 0 finished with value: 6113.890084581559 and parameters: {'mem_len': 7, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 0 with value: 6113.890084581559.\n[I 2025-11-06 15:02:20,221] Trial 1 finished with value: 154814.93166376086 and parameters: {'mem_len': 22, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell sufficient_decrease': 2.2329267844390062e-05, 'Wolfe-Powell 1-sufficient_curvature': 0.0055127501906131774}. Best is trial 0 with value: 6113.890084581559.\n[I 2025-11-06 15:02:57,165] Trial 2 finished with value: 6124.912084581583 and parameters: {'mem_len': 15, 'vector_transport_method': 2, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 0 with value: 6113.890084581559.\n[I 2025-11-06 15:03:31,680] Trial 3 finished with value: 6090.120584581558 and parameters: {'mem_len': 7, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 3 with value: 6090.120584581558.\n[I 2025-11-06 15:04:06,175] Trial 4 finished with value: 6109.476084581554 and parameters: {'mem_len': 14, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 3 with value: 6090.120584581558.\n[I 2025-11-06 15:04:40,786] Trial 5 finished with value: 6108.611584581555 and parameters: {'mem_len': 26, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 3 with value: 6090.120584581558.\n[I 2025-11-06 15:04:48,235] Trial 6 pruned. \n[I 2025-11-06 15:05:22,534] Trial 7 finished with value: 6057.355084581624 and parameters: {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 7 with value: 6057.355084581624.\n[I 2025-11-06 15:05:29,959] Trial 8 pruned. \n[I 2025-11-06 15:05:53,521] Trial 9 pruned. \n[I 2025-11-06 15:06:28,408] Trial 10 pruned. \n[I 2025-11-06 15:06:54,861] Trial 11 pruned. \n[I 2025-11-06 15:07:02,401] Trial 12 pruned. \n[I 2025-11-06 15:07:10,090] Trial 13 pruned. \n[I 2025-11-06 15:07:26,574] Trial 14 pruned. \n[I 2025-11-06 15:07:33,994] Trial 15 pruned. \n[I 2025-11-06 15:07:41,526] Trial 16 pruned. \n[I 2025-11-06 15:07:49,059] Trial 17 pruned. \n[I 2025-11-06 15:08:15,446] Trial 18 pruned. \n[I 2025-11-06 15:08:22,923] Trial 19 pruned. \n[I 2025-11-06 15:08:30,406] Trial 20 pruned. \n[I 2025-11-06 15:08:37,893] Trial 21 pruned. \n[I 2025-11-06 15:09:12,437] Trial 22 pruned. \n[I 2025-11-06 15:09:19,869] Trial 23 pruned. \n[I 2025-11-06 15:09:35,027] Trial 24 pruned. \nBest params is {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1} with value 6057.355084581624\nSelected manifold: Sphere\nSelected retraction method: ManifoldsBase.ProjectionRetraction()\nSelected vector transport method: ManifoldsBase.ProjectionTransport()\n\nPython: <optuna.study.study.Study object at 0x7690e17f0d70>","category":"section"},{"location":"examples/HyperparameterOptimization/#Summary","page":"Hyperparameter optimziation","title":"Summary","text":"We’ve shown how to automatically select the best hyperparameter values for your optimization problem.","category":"section"},{"location":"examples/HyperparameterOptimization/#Technical-details","page":"Hyperparameter optimziation","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/.julia/dev/ManoptExamples/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.122\n  [682c06a0] JSON v1.2.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.3\n  [3362f125] ManifoldsBase v2.2.0\n  [0fc0a36d] Manopt v0.5.26\n  [5b8d5e80] ManoptExamples v0.1.17 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered November 6, 2025, 15:9:36.","category":"section"},{"location":"examples/HyperparameterOptimization/#Literature","page":"Hyperparameter optimziation","title":"Literature","text":"T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2019), arXiv:1907.10902.\n\n\n\n","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#A-2D-illustration-of-Spectral-and-Robust-Procrustes","page":"Spectral & Robust Procrustes","title":"A 2D illustration of Spectral and Robust Procrustes","text":"Ronny Bergmann 2025-11-10\n\nusing CairoMakie, CSV, DataFrames, LinearAlgebra, Manifolds, Manopt, NamedColors, PrettyTables,  Random\n\nThis example reproduces the results preented in Section 5 of [JBK+25] about the effect of the Procrustes problem\n\noperatorname*argmin_p in mathrmSO(n) lVert A - pBrVert_s\n\nfor different norms s  mathrmFmathrmSmathrmR, namely the Frobenius, spectral and a robust norm, respectively.\n\nWe first define a set of points, that originally started with a duch example from the master thesis of S. Oddsen, hence the name “duck”, though for this experiment it is merely an abstract idea of a duck.\n\nduck = [\n    # 1 left (a bit moved up), maybe a beak\n    # 2 right (a bit moved down), maybe a tail\n    # 3 top, maybe its head\n    # 4 bottom – feat are under water, so we have an abstract duck!\n    -1/2 1/2 1/2 -1/4 0.0 1/4 -3/8 -1/8 1/8 3/8;\n    1/4 -2/12 -5/12 1/2 1/2 1/2 -1/2 -1/2 -1/2 -1/2\n]\n\nThe general scheme of the following experiments is as follows: We first rotate the duck by a known angle, then apply certain noises and try to recover the true rotation angle.\n\nα_true = 5π/8\nR(α) = [cos(α) sin(α); -sin(α) cos(α)]\nα(R) = atan(R[2,1], R[1,1])\nerror_on_nonoutliers(res, remaining_indices) = norm(\n    res[:,remaining_indices]-duck[:,remaining_indices]\n)\n# true exact minimizer if there were not errors\np_true = R(-α_true) # Point on SO(2), the true reconstruction point we aim for\nrotated_duck = p_true'*duck","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Manifold,-costs,-and-minimizers","page":"Spectral & Robust Procrustes","title":"Manifold, costs, and minimizers","text":"M = Rotations(2)\n\n@doc raw\"\"\"\n    Spectral_cost(M, p)\n\nCompute the spectral norm ``\\lVert p\\cdot B- A\\rVert_2``.\n\"\"\"\nSpectral_cost(M, p; B=r_duck, A=duck) = opnorm(p*B - A)\n\n@doc raw\"\"\"\n    Frobenius_cost(M, p)\n\nCompute the Frobenious norm ``\\lVert p\\cdot B- A\\rVert_{\\mathrm{F}}``.\n\"\"\"\nFrobenius_cost(M, p; B=r_duck, A=duck) = norm(p*B - A)\n\n@doc raw\"\"\"\n    Robust_cost(M, p)\n\nCompute the 2-1-norm ``\\lVert p\\cdot B- A\\rVert_{2,1}``, i.e. the sum of the 2-norms of the columns.\n\"\"\"\nRobust_cost(M,p; B=r_duck, A=duck) = sum([norm(c, 2.0) for c in eachcol(p*B-A)])\n\np0 = Matrix{Float64}(I,2,2)\n\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n\nfunction Frobenius_minimizer(A,B; p0=p0)\n    svd_AB = svd(A*B')\n    return svd_AB.U*svd_AB.V'\nend\nfunction spectral_minimizer(A,B; p0=p0)\n    return mesh_adaptive_direct_search(\n        M,\n       (M,p) -> Spectral_cost(M, p; A=A, B=B), p0;\n       debug=[:Iteration, :Cost, \"\\n\", 10, :Stop]\n    )\nend\nfunction robust_minimizer(A, B; p0=p0)\n    return mesh_adaptive_direct_search(\n         M,\n        (M,p) -> Robust_cost(M, p; A=A, B=B), p0;\n        debug=[:Iteration, :Cost, \"\\n\", 10, :Stop]\n    )\n end","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Experiment-1:-Outliers-that-are-orthogonal-and-the-same-distance","page":"Spectral & Robust Procrustes","title":"Experiment 1: Outliers that are orthogonal and the same distance","text":"We start with two outliers at the same (even) distance\n\nsub_experiment1 = \"even\"\noutlier_shifts1 = [[2.0, 0.0], [0.0,2.0]]\noutlier_indices1 = [2,8]\nremaining_indices1 = [k for k=1:size(duck,2) if k ∉ outlier_indices1]\n\nSince we apply these outliers after rotation, one way to illustrate them on the duck is to rotate them back and draw that into the original data.\n\noutlier_shifts1_rp = [R(-α_true)x for x in outlier_shifts1]\noutlier_lines1 = [\n    [\n        [\n            duck[xy,outlier_indices1[k]],\n            duck[xy,outlier_indices1[k]] + outlier_shifts1_rp[k][xy]\n        ] for xy = 1:2\n    ]\n    for k=1:length(outlier_indices1)\n]\n\noutliers1_rotated_duck = copy(rotated_duck)\n    # generate outliers on the rotated duck\n    for (i,k) in enumerate(outlier_indices1)\n        outliers1_rotated_duck[:,k] += outlier_shifts1[i]\n    end\n\nWe compute the minimizers\n\np1F = Frobenius_minimizer(duck, outliers1_rotated_duck)\np1S = spectral_minimizer(duck, outliers1_rotated_duck)\np1R = robust_minimizer(duck, outliers1_rotated_duck)\nnothing\n\nInitial f(x): 2.929307\n# 10    f(x): 2.000000\n# 20    f(x): 2.000000\n# 30    f(x): 2.000000\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\nInitial f(x): 9.096802\n# 10    f(x): 4.000000\n# 20    f(x): 4.000000\n# 30    f(x): 4.000000\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\n\nand we generate the rotated results\n\nduck1F = p1F * outliers1_rotated_duck\nduck1S = p1S * outliers1_rotated_duck\nduck1R = p1R * outliers1_rotated_duck\n\nThe result is\n\nres1_fig = Figure()\nres1_ax = Axis(res1_fig[1, 1], title=\"Reconstructing the duck “$(sub_experiment1)” \")\nscatter!(res1_ax, duck[1,:], duck[2,:], label=\"a duck\", color=color_duck, markersize=20)\nfor k=1:length(outlier_indices1)\n    lines!(res1_ax, outlier_lines1[k][1], outlier_lines1[k][2], color=:black)\nend\nscatter!(res1_ax, duck1F[1,:], duck1F[2,:], label=\"Frobenius result\", color=color_Frobenius, markersize=10)\nscatter!(res1_ax, duck1S[1,:], duck1S[2,:], label=\"Spectral result\", color=color_spectral, markersize=10)\nscatter!(res1_ax, duck1R[1,:], duck1R[2,:], label=\"Robust result\", color=color_robust, markersize=10)\naxislegend(res1_ax,position = :lt)\nres1_fig\n\n(Image: )\n\nWe can also look at the numbers: How well the angle is reconstructed and how large the error on the non-outliers is;\n\nα_errors = [α(p1F), α(p1S), α(p1R)] .- α_true\nl2_errors = [error_on_nonoutliers(d, remaining_indices1) for d in [duck1F, duck1S, duck1R]]\npretty_table(\n    DataFrame(:rownames => [\"angle\", \"non-outlier\"], :F => [α_errors[1], l2_errors[1]], :S => [α_errors[2], l2_errors[2]], :R => [α_errors[3], l2_errors[3]]);\n    backend=:markdown, column_labels = [\"error on\", \"Frobenius\", \"spectral\", \"robust\"])\n\nerror on Frobenius spectral robust\nangle 0.0102065 -2.22045e-16 -2.22045e-16\nnon-outlier 0.0166399 4.23505e-16 4.23505e-16\n\nWe see both in outlier error and angle reconstruction, Frobenius is not reconstructions perfectly, while both spectral and robust do.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Experiment-2:-Outliers-that-are-orthogonal-and-the-same-distance","page":"Spectral & Robust Procrustes","title":"Experiment 2: Outliers that are orthogonal and the same distance","text":"We start with two outliers that are orthogonal but of different length in movement.\n\nsub_experiment2 = \"orth\"\noutlier_shifts2 = [[1.0, 0.0], [0.0,2.0]]\noutlier_indices2 = [2,8]\nremaining_indices2 = [k for k=1:size(duck,2) if k ∉ outlier_indices2]\n\nSince we apply these outliers after rotation, one way to illustrate them on the duck is to rotate them back and draw that into the original data.\n\noutlier_shifts2_rp = [R(-α_true)x for x in outlier_shifts2]\noutlier_lines2 = [\n    [\n        [\n            duck[xy,outlier_indices2[k]],\n            duck[xy,outlier_indices2[k]] + outlier_shifts2_rp[k][xy]\n        ] for xy = 1:2\n    ]\n    for k=1:length(outlier_indices2)\n]\n\noutliers2_rotated_duck = copy(rotated_duck)\n    # generate outliers on the rotated duck\n    for (i,k) in enumerate(outlier_indices2)\n        outliers2_rotated_duck[:,k] += outlier_shifts2[i]\n    end\n\nWe compute the minimizers\n\np2F = Frobenius_minimizer(duck, outliers2_rotated_duck)\np2S = spectral_minimizer(duck, outliers2_rotated_duck)\np2R = robust_minimizer(duck, outliers2_rotated_duck)\nnothing\n\nInitial f(x): 2.929222\n# 10    f(x): 1.948920\n# 20    f(x): 1.947072\n# 30    f(x): 1.947072\n# 40    f(x): 1.947072\n# 50    f(x): 1.947072\n# 60    f(x): 1.947072\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\nInitial f(x): 8.101267\n# 10    f(x): 3.000000\n# 20    f(x): 3.000000\n# 30    f(x): 3.000000\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\n\nand we generate the rotated results\n\nduck2F = p2F * outliers2_rotated_duck\nduck2S = p2S * outliers2_rotated_duck\nduck2R = p2R * outliers2_rotated_duck\n\nThe result is\n\n    res2_fig = Figure()\n    res2_ax = Axis(res2_fig[1, 1], title=\"Reconstructing the duck “$(sub_experiment2)” \")\n    scatter!(res2_ax, duck[1,:], duck[2,:], label=\"a duck\", color=color_duck, markersize=20)\n    for k=1:length(outlier_indices2)\n        lines!(res2_ax, outlier_lines2[k][1], outlier_lines2[k][2], color=:black)\n    end\n    scatter!(res2_ax, duck2F[1,:], duck2F[2,:], label=\"Frobenius result\", color=color_Frobenius, markersize=10)\n    scatter!(res2_ax, duck2S[1,:], duck2S[2,:], label=\"Spectral result\", color=color_spectral, markersize=10)\n    scatter!(res2_ax, duck2R[1,:], duck2R[2,:], label=\"Robust result\", color=color_robust, markersize=10)\n    axislegend(res2_ax,position = :lt)\n    res2_fig\n\n(Image: )\n\nAgain we can look at both the angle and non-outlier errors:\n\nα_errors = [α(p2F), α(p2S), α(p2R)] .- α_true\nl2_errors = [error_on_nonoutliers(d, remaining_indices1) for d in [duck2F, duck2S, duck2R]]\npretty_table(\n    DataFrame(:rownames => [\"angle\", \"non-outlier\"], :F => [α_errors[1], l2_errors[1]], :S => [α_errors[2], l2_errors[2]], :R => [α_errors[3], l2_errors[3]]);\n    backend = :markdown, column_labels = [\"error on\", \"Frobenius\", \"spectral\", \"robust\"])\n\nerror on Frobenius spectral robust\nangle 0.123315 0.248392 -2.22045e-16\nnon-outlier 0.200917 0.403922 4.23505e-16\n\nFor this second experiment both the figure and the errors indicate, that robust is still reconstructing perfectly, while spectral even performs worth than Frobenius.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Experiment-3:-Outliers-that-are-orthogonal-and-the-same-distance","page":"Spectral & Robust Procrustes","title":"Experiment 3: Outliers that are orthogonal and the same distance","text":"We continue with two outliers that are neither of same length nor orthogonal.\n\nsub_experiment3 = \"orth\"\noutlier_shifts3 = [[1.0, 2.0], [0.0,2.0]]\noutlier_indices3 = [2,8]\nremaining_indices3 = [k for k=1:size(duck,2) if k ∉ outlier_indices3]\n\nSince we apply these outliers after rotation, one way to illustrate them on the duck is to rotate them back and draw that into the original data.\n\noutlier_shifts3_rp = [R(-α_true)x for x in outlier_shifts3]\noutlier_lines3 = [\n    [\n        [\n            duck[xy,outlier_indices3[k]],\n            duck[xy,outlier_indices3[k]] + outlier_shifts3_rp[k][xy]\n        ] for xy = 1:2\n    ]\n    for k=1:length(outlier_indices3)\n]\n\noutliers3_rotated_duck = copy(rotated_duck)\n    # generate outliers on the rotated duck\n    for (i,k) in enumerate(outlier_indices3)\n        outliers3_rotated_duck[:,k] += outlier_shifts3[i]\n    end\n\nWe compute the minimizers\n\np3F = Frobenius_minimizer(duck, outliers3_rotated_duck)\np3S = spectral_minimizer(duck, outliers3_rotated_duck)\np3R = robust_minimizer(duck, outliers3_rotated_duck)\nnothing\n\nInitial f(x): 3.583216\n# 10    f(x): 2.717797\n# 20    f(x): 2.717797\n# 30    f(x): 2.717797\n# 40    f(x): 2.717797\n# 50    f(x): 2.717797\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\nInitial f(x): 9.791093\n# 10    f(x): 4.236068\n# 20    f(x): 4.236068\n# 30    f(x): 4.236068\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\n\nand we generate the rotated results\n\nduck3F = p3F * outliers3_rotated_duck\nduck3S = p3S * outliers3_rotated_duck\nduck3R = p3R * outliers3_rotated_duck\n\nThe result is\n\n    res3_fig = Figure()\n    res3_ax = Axis(res3_fig[1, 1], title=\"Reconstructing the duck “$(sub_experiment3)” \")\n    scatter!(res3_ax, duck[1,:], duck[2,:], label=\"a duck\", color=color_duck, markersize=20)\n    for k=1:length(outlier_indices3)\n        lines!(res3_ax, outlier_lines3[k][1], outlier_lines3[k][2], color=:black)\n    end\n    scatter!(res3_ax, duck3F[1,:], duck3F[2,:], label=\"Frobenius result\", color=color_Frobenius, markersize=10)\n    scatter!(res3_ax, duck3S[1,:], duck3S[2,:], label=\"Spectral result\", color=color_spectral, markersize=10)\n    scatter!(res3_ax, duck3R[1,:], duck3R[2,:], label=\"Robust result\", color=color_robust, markersize=10)\n    axislegend(res3_ax,position = :rb)\n    res3_fig\n\n(Image: )\n\nAgain we can look at both the angle and non-outlier errors:\n\nα_errors = [α(p3F), α(p3S), α(p3R)] .- α_true\nl2_errors = [error_on_nonoutliers(d, remaining_indices1) for d in [duck3F, duck3S, duck3R]]\npretty_table(\n    DataFrame(:rownames => [\"angle\", \"non-outlier\"], :F => [α_errors[1], l2_errors[1]], :S => [α_errors[2], l2_errors[2]], :R => [α_errors[3], l2_errors[3]]);\n    backend = :markdown, column_labels = [\"error on\", \"Frobenius\", \"spectral\", \"robust\"])\n\nerror on Frobenius spectral robust\nangle 0.396949 0.785611 -2.22045e-16\nnon-outlier 0.642918 1.24812 4.23505e-16\n\nAlso for the third, robust is still performing very well.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Experiment-4:-Outliers-of-rank-1","page":"Spectral & Robust Procrustes","title":"Experiment 4: Outliers of rank 1","text":"Next we consider a rank-1 update from the nullspace of the duck.\n\nsub_experiment4 = \"spectral\"\nsingular_duck = svd(duck)\nV = nullspace(duck)\nv = 1/size(V,2) * sum(V, dims=2)\nu = [1.0, 1.0]\nvt_shift4 = u*v'\noutlier_shifts4_all = [ vt_shift4[:,i] for i in 1:size(singular_duck.Vt,2) ]\noutlier_indices4 = filter(\n    (i) -> norm(outlier_shifts4_all[i])>1e-13,\n    collect(1:length(outlier_shifts4_all))\n)\nσ = 8.0\noutlier_shifts4 = [ σ*outlier_shifts4_all[i] for i in outlier_indices4]\nremaining_indices4 = [k for k=1:size(duck,2) if k ∉ outlier_indices4]\n\nSince we apply these outliers after rotation, one way to illustrate them on the duck is to rotate them back and draw that into the original data.\n\noutlier_shifts4_rp = [R(-α_true)x for x in outlier_shifts4]\noutlier_lines4 = [\n    [\n        [\n            duck[xy,outlier_indices4[k]],\n            duck[xy,outlier_indices4[k]] + outlier_shifts4_rp[k][xy]\n        ] for xy = 1:2\n    ]\n    for k=1:length(outlier_indices4)\n]\n\noutliers4_rotated_duck = copy(rotated_duck)\n# generate outliers on the rotated duck\nfor (i,k) in enumerate(outlier_indices4)\n    outliers4_rotated_duck[:,k] += outlier_shifts4[i]\nend\n\nWe compute the minimizers\n\np4F = Frobenius_minimizer(duck, outliers4_rotated_duck)\np4S = spectral_minimizer(duck, outliers4_rotated_duck)\np4R = robust_minimizer(duck, outliers4_rotated_duck)\nnothing\n\nInitial f(x): 4.160638\n# 10    f(x): 4.000000\n# 20    f(x): 4.000000\n# 30    f(x): 4.000000\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\nInitial f(x): 13.503372\n# 10    f(x): 11.887835\n# 20    f(x): 11.887818\n# 30    f(x): 11.887818\n# 40    f(x): 11.887818\n# 50    f(x): 11.887818\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\n\nand we generate the rotated results\n\nduck4F = p4F * outliers4_rotated_duck\nduck4S = p4S * outliers4_rotated_duck\nduck4R = p4R * outliers4_rotated_duck\n\nThe result is\n\n    res4_fig = Figure()\n    res4_ax = Axis(res4_fig[1, 1], title=\"Reconstructing the duck “$(sub_experiment4)” \")\n    scatter!(res4_ax, duck[1,:], duck[2,:], label=\"a duck\", color=color_duck, markersize=20)\n    for k=1:length(outlier_indices4)\n        lines!(res4_ax, outlier_lines4[k][1], outlier_lines4[k][2], color=:black)\n    end\n    scatter!(res4_ax, duck4F[1,:], duck4F[2,:], label=\"Frobenius result\", color=color_Frobenius, markersize=10)\n    scatter!(res4_ax, duck4S[1,:], duck4S[2,:], label=\"Spectral result\", color=color_spectral, markersize=10)\n    scatter!(res4_ax, duck4R[1,:], duck4R[2,:], label=\"Robust result\", color=color_robust, markersize=10)\n    axislegend(res4_ax,position = :rt)\n    res4_fig\n\n(Image: )\n\nWe can also look at the error, though since all points are outliers, we can only look at the angle reconstruction:\n\nα_errors = [α(p4F), α(p4S), α(p4R)] .- α_true\npretty_table(\n    DataFrame(:rownames => [\"angle\", ], :F => [α_errors[1],], :S => [α_errors[2],], :R => [α_errors[3],]);\n    backend = :markdown, column_labels = [\"error on\", \"Frobenius\", \"spectral\", \"robust\"])\n\nerror on Frobenius spectral robust\nangle 0.0 -1.82865e-10 0.595122\n\nnow, robust slightly fails, while both Frobenius and spectral perform very well, the algorithm for spectral maybe just stopping a bit early.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Experiment-5:-Outliers-of-rank-1-plus-uniform-noise","page":"Spectral & Robust Procrustes","title":"Experiment 5: Outliers of rank 1 plus uniform noise","text":"Next we consider a rank-1 update from the nullspace of the duck and add uniform noise\n\nsub_experiment5 = \"spectral_and_uniform\"\nRandom.seed!(23)\nRmat = 0.125*rand(size(duck)...)\nvt_shift5 = u*v'.+ Rmat\noutlier_shifts5_all = [ vt_shift5[:,i] for i in 1:size(singular_duck.Vt,2) ]\noutlier_indices5 = filter(\n    (i) -> norm(outlier_shifts5_all[i])>1e-13,\n    collect(1:length(outlier_shifts5_all))\n)\nσ = 4.0\noutlier_shifts5 = [ σ*outlier_shifts5_all[i] for i in outlier_indices5]\nremaining_indices5 = [k for k=1:size(duck,2) if k ∉ outlier_indices5]\n\nSince we apply these outliers after rotation, one way to illustrate them on the duck is to rotate them back and draw that into the original data.\n\noutlier_shifts5_rp = [R(-α_true)x for x in outlier_shifts5]\noutlier_lines5 = [\n    [\n        [\n            duck[xy,outlier_indices5[k]],\n            duck[xy,outlier_indices5[k]] + outlier_shifts5_rp[k][xy]\n        ] for xy = 1:2\n    ]\n    for k=1:length(outlier_indices5)\n]\n\noutliers5_rotated_duck = copy(rotated_duck)\n# generate outliers on the rotated duck\nfor (i,k) in enumerate(outlier_indices5)\n    outliers5_rotated_duck[:,k] += outlier_shifts5[i]\nend\n\nWe compute the minimizers\n\np5F = Frobenius_minimizer(duck, outliers5_rotated_duck)\np5S = spectral_minimizer(duck, outliers5_rotated_duck)\np5R = robust_minimizer(duck, outliers5_rotated_duck)\nnothing\n\nInitial f(x): 3.278339\n# 10    f(x): 3.138811\n# 20    f(x): 3.138811\n# 30    f(x): 3.138811\n# 40    f(x): 3.138811\n# 50    f(x): 3.138811\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\nInitial f(x): 11.255009\n# 10    f(x): 9.349293\n# 20    f(x): 9.348961\n# 30    f(x): 9.348960\n# 40    f(x): 9.348960\n# 50    f(x): 9.348960\n# 60    f(x): 9.348960\nThe algorithm computed a poll step size (5.820766091346741e-11) less than 1.0e-10.\n\nand we generate the rotated results\n\nduck5F = p5F * outliers5_rotated_duck\nduck5S = p5S * outliers5_rotated_duck\nduck5R = p5R * outliers5_rotated_duck\n\nThe result is\n\n    res5_fig = Figure()\n    res5_ax = Axis(res5_fig[1, 1], title=\"Reconstructing the duck “$(sub_experiment5)” \")\n    scatter!(res5_ax, duck[1,:], duck[2,:], label=\"a duck\", color=color_duck, markersize=20)\n    for k=1:length(outlier_indices5)\n        lines!(res5_ax, outlier_lines5[k][1], outlier_lines5[k][2], color=:black)\n    end\n    scatter!(res5_ax, duck5F[1,:], duck5F[2,:], label=\"Frobenius result\", color=color_Frobenius, markersize=10)\n    scatter!(res5_ax, duck5S[1,:], duck5S[2,:], label=\"Spectral result\", color=color_spectral, markersize=10)\n    scatter!(res5_ax, duck5R[1,:], duck5R[2,:], label=\"Robust result\", color=color_robust, markersize=10)\n    axislegend(res5_ax,position = :rt)\n    res5_fig\n\n(Image: )\n\nWe can also look at the error, though since all points are outliers, we can only look at the angle reconstruction:\n\nα_errors = [α(p5F), α(p5S), α(p5R)] .- α_true\npretty_table(\n    DataFrame(\n        :rownames => [\"angle\"],\n        :F => [α_errors[1]], :S => [α_errors[2]], :R => [α_errors[3]]\n    );\n    backend = :markdown,\n    column_labels = [\"error on\", \"Frobenius\", \"spectral\", \"robust\"],\n)\n\nerror on Frobenius spectral robust\nangle 0.0516324 0.0107993 0.562687\n\nHere, booth robust and Frobenius struggle to reconstruct, while spectral performs best.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Technical-details","page":"Spectral & Robust Procrustes","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:13:14.","category":"section"},{"location":"examples/Spectral-Procrustes-2D/#Literature","page":"Spectral & Robust Procrustes","title":"Literature","text":"H. Jasa, R. Bergmann, C. Kümmerle, A. Athreya and Z. Lubberts. Procrustes Problems on Random Matrices, preprint (2025), arXiv:2510.05182.\n\n\n\n","category":"section"},{"location":"objectives/#List-of-Objectives-defined-for-the-Examples","page":"Objectives","title":"List of Objectives defined for the Examples","text":"","category":"section"},{"location":"objectives/#Rayleigh","page":"Objectives","title":"Rayleigh Quotient on the Sphere","text":"See the Rayleigh example (TODO) to see these in use.","category":"section"},{"location":"objectives/#BezierCurves","page":"Objectives","title":"Bézier Curves","text":"See the Bezier Curves example to see these in use.","category":"section"},{"location":"objectives/#RiemannianMean","page":"Objectives","title":"Riemannian Mean","text":"See the Riemannian mean example to see these in use.","category":"section"},{"location":"objectives/#RobustPCA","page":"Objectives","title":"Robust PCA","text":"See the Robust PCA example to see these in use.","category":"section"},{"location":"objectives/#Rosenbrock","page":"Objectives","title":"Rosenbrock Function","text":"See the Rosenbrock example  and The Difference of Convex Rosenbrock Example to see these in use.","category":"section"},{"location":"objectives/#Total-Variation","page":"Objectives","title":"Total Variation","text":"See the Total Variation example to see these in use.","category":"section"},{"location":"objectives/#VariationalProblemAssembler","page":"Objectives","title":"Variational Problem Assembler","text":"","category":"section"},{"location":"objectives/#Literature","page":"Objectives","title":"Literature","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"section"},{"location":"objectives/#ManoptExamples.RayleighQuotientCost","page":"Objectives","title":"ManoptExamples.RayleighQuotientCost","text":"RayleighQuotientCost\n\nA functor representing the Rayleigh Quotient cost function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Rayleigh Quotient in two forms. Either\n\nf(p) = p^mathrmTApqquad p  𝕊^n-1\n\nor extended into the embedding as\n\nf(x) = x^mathrmTAx qquad x  ℝ^n\n\nwhich is not the orignal Rayleigh quotient for performance reasons, but useful if you want to use this as the Euclidean cost in the emedding of 𝕊^n-1.\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientCost(A)\n\nCreate the Rayleigh cost function.\n\nSee also\n\nRayleighQuotientGrad!!, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientGrad!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientGrad!!","text":"RayleighQuotientGrad!!\n\nA functor representing the Rayleigh Quotient gradient function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the gradient of the Rayleigh Quotient in two forms. Either\n\noperatornamegrad f(p) = 2 Ap - 2 (p^mathrmTAp)*pqquad p  𝕊^n-1\n\nor taking the Euclidean gradient of the Rayleigh quotient on the sphere as\n\nf(x) = 2Ax qquad x  ℝ^n\n\nFor details, see Example 3.62 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientGrad!!(A)\n\nCreate the Rayleigh quotient gradient function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientHess!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientHess!!","text":"RayleighQuotientHess!!\n\nA functor representing the Rayleigh Quotient Hessian.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Hessian of the Rayleigh Quotient in two forms. Either\n\noperatornameHess f(p)X = 2 bigl(AX - (p^mathrmTAX)p - (p^mathrmTAp)Xbigr)qquad p  𝕊^n-1 X in T_p𝕊^n-1\n\nor taking the Euclidean Hessian of the Rayleigh quotient on the sphere as\n\n^2f(x)V = 2AV qquad x V  ℝ^n\n\nFor details, see Example 5.27 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientHess!!(A)\n\nCreate the Rayleigh quotient Hessian function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientGrad!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.BezierSegment","page":"Objectives","title":"ManoptExamples.BezierSegment","text":"BezierSegment\n\nA type to capture a Bezier segment. With n points, a Bézier segment of degree n-1 is stored. On the Euclidean manifold, this yields a polynomial of degree n-1.\n\nThis type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an AbstractVector of BezierSegments where each of the points might be a nested array on a PowerManifold already.\n\nNot that this can also be used to represent tangent vectors on the control points of a segment.\n\nSee also: de_Casteljau.\n\nConstructor\n\nBezierSegment(pts::AbstractVector)\n\nGiven an abstract vector of pts generate the corresponding Bézier segment.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}, AbstractFloat, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.L2_acceleration_Bezier","text":"L2_acceleration_Bezier(M,B,pts,λ,d)\n\ncompute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e.\n\nfracλ2sum_i=0^N d_mathcal M(d_i c_B(i))^2+\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i and d_2 refers to the second order absolute difference second_order_Total_Variation (squared), the junction points are denoted by p_i, and to each p_i corresponds one data item in the manifold points given in d. For details on the acceleration approximation, see acceleration_Bezier. Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_L2_acceleration_Bezier, acceleration_Bezier, grad_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}}} where P","page":"Objectives","title":"ManoptExamples.acceleration_Bezier","text":"acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P}\n\ncompute the value of the discrete Acceleration of the composite Bezier curve\n\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i, i=1N, and d_2 refers to the second order absolute difference second_order_Total_Variation (squared). Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nThis acceleration discretization was introduced in Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, grad_L2_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    T::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    T::AbstractVector,\n    X::AbstractVector,\n)\n\nEvaluate the adjoint of the differential with respect to the controlpoints at several times T. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\n\nevaluate the adjoint of the differential of a composite Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t, η)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t,\n    η,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a point t01 on the curve and a tangent vector ηT_β(t)mathcal M. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.de_Casteljau-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any}}","page":"Objectives","title":"ManoptExamples.de_Casteljau","text":"de_Casteljau(M::AbstractManifold, b::BezierSegment NTuple{N,P}) -> Function\n\nreturn the Bézier curve β(b_0b_n) 01  mathcal M defined by the control points b_0b_nmathcal M, nmathbb N, as a BezierSegment. This function implements de Casteljau's algorithm Casteljau, 1959, Casteljau, 1963 generalized to manifolds by Popiel, Noakes, J Approx Theo, 2007: Let γ_ab(t) denote the shortest geodesic connecting abmathcal M. Then the curve is defined by the recursion\n\nbeginaligned\n    β(tb_0b_1) = gamma_b_0b_1(t)\n    β(tb_0b_n) = gamma_β(tb_0b_n-1) β(tb_1b_n)(t)\nendaligned\n\nand P is the type of a point on the Manifold M.\n\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) -> Function\n\nGiven a vector of Bézier segments, i.e. a vector of control points B=bigl( (b_00b_n_00)(b_0m b_n_mm) bigr), where the different segments might be of different degree(s) n_0n_m. The resulting composite Bézier curve c_B0m  mathcal M consists of m segments which are Bézier curves.\n\nc_B(t) =\n    begincases\n        β(t b_00b_n_00)  text if  t 01\n        β(t-i b_0ib_n_ii)  text if \n            t(ii+1 quad i1m-1\n    endcases\n\nde_Casteljau(M::AbstractManifold, b::BezierSegment, t::Real)\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}, t::Real)\nde_Casteljau(M::AbstractManifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\nde_Casteljau(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n) -> AbstractVector\n\nEvaluate the Bézier curve at time t or at times t in T.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Θ::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at the points in T, which are elementwise in 0N, and each depending the corresponding segment(s). Here, N is the length of B. For the mutating variant the result is computed in Θ.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at t0N, which depends only on the corresponding segment. Here, N is the length of B. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at the points T, elementwise in t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t::Float, X::BezierSegment)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    t,\n    X::BezierSegment\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X given in the tangent spaces of the control points. The result is the “change” of the curve at t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degree-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.get_Bezier_degree","text":"get_Bezier_degree(M::AbstractManifold, b::BezierSegment)\n\nreturn the degree of the Bézier curve represented by the tuple b of control points on the manifold M, i.e. the number of points minus 1.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degrees-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_degrees","text":"get_Bezier_degrees(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\n\nreturn the degrees of the components of a composite Bézier curve represented by tuples in B containing points on the manifold M.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_inner_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_inner_points","text":"get_Bezier_inner_points(M::AbstractManifold, B::AbstractVector{<:BezierSegment} )\nget_Bezier_inner_points(M::AbstractManifold, b::BezierSegment)\n\nreturns the inner (i.e. despite start and end) points of the segments of the composite Bézier curve specified by the control points B. For a single segment b, its inner points are returned\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junction_tangent_vectors-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_junction_tangent_vectors","text":"get_Bezier_junction_tangent_vectors(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junction_tangent_vectors(M::AbstractManifold, b::BezierSegment)\n\nreturns the tangent vectors at start and end points of the composite Bézier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points B, either a vector of segments of controlpoints.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junctions","page":"Objectives","title":"ManoptExamples.get_Bezier_junctions","text":"get_Bezier_junctions(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junctions(M::AbstractManifold, b::BezierSegment)\n\nreturns the start and end point(s) of the segments of the composite Bézier curve specified by the control points B. For just one segment b, its start and end points are returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_points","page":"Objectives","title":"ManoptExamples.get_Bezier_points","text":"get_Bezier_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_Bezier_points(M::AbstractManifold, b::BezierSegment, reduce::Symbol=:default)\n\nreturns the control points of the segments of the composite Bézier curve specified by the control points B, either a vector of segments of controlpoints or a.\n\nThis method reduces the points depending on the optional reduce symbol\n\n:default:        no reduction is performed\n:continuous:     for a continuous function, the junction points are doubled at b_0i=b_n_i-1i-1, so only b_0i is in the vector.\n:differentiable: for a differentiable function additionally log_b_0ib_1i = -log_b_n_i-1i-1b_n_i-1-1i-1 holds. hence b_n_i-1-1i-1 is omitted.\n\nIf only one segment is given, all points of b, b.pts, is returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_segments-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any, Symbol}} where P","page":"Objectives","title":"ManoptExamples.get_Bezier_segments","text":"get_Bezier_segments(M::AbstractManifold, c::AbstractArray{P}, d[, s::Symbol=:default])\n\nreturns the array of BezierSegments B of a composite Bézier curve reconstructed from an array c of points on the manifold M and an array of degrees d.\n\nThere are a few (reduced) representations that can get extended; see also get_Bezier_points. For ease of the following, let c=(c_1c_k) and d=(d_1d_m), where m denotes the number of components the composite Bézier curve consists of. Then\n\n:default: k = m + sum_i=1^m d_i since each component requires one point more than its degree. The points are then ordered in tuples, i.e.\nB = bigl c_1c_d_1+1 (c_d_1+2c_d_1+d_2+2 c_k-m+1+d_mc_k bigr\n:continuous: k = 1+ sum_i=1m d_i, since for a continuous curve start and end point of successive components are the same, so the very first start point and the end points are stored.\nB = bigl c_1c_d_1+1 c_d_1+1c_d_1+d_2+1 c_k-1+d_mb_k) bigr\n:differentiable – for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence k = 2 - m + sum_i=1m d_i and at a junction point b_n with its given prior point c_n-1, i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as b = exp_c_n(-log_c_n c_n-1) such that the assumed differentiability holds\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector, Any, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.grad_L2_acceleration_Bezier","text":"grad_L2_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector,\n    λ,\n    d::AbstractVector{P}\n) where {P}\n\ncompute the gradient of the discretized acceleration of a composite Bézier curve on the Manifold M with respect to its control points B together with a data term that relates the junction points p_i to the data d with a weight λ compared to the acceleration. The curve is evaluated at the points given in pts (elementwise in 0N), where N is the number of segments of the Bézier curve. The summands are grad_distance for the data term and grad_acceleration_Bezier for the acceleration with interpolation constrains. Here the get_Bezier_junctions are included in the optimization, i.e. setting λ=0 yields the unconstrained acceleration minimization. Note that this is ill-posed, since any Bézier curve identical to a geodesic is a minimizer.\n\nNote that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_acceleration_Bezier-Tuple{ManifoldsBase.AbstractManifold, AbstractVector, AbstractVector{<:Integer}, AbstractVector}","page":"Objectives","title":"ManoptExamples.grad_acceleration_Bezier","text":"grad_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector,\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector\n)\n\ncompute the gradient of the discretized acceleration of a (composite) Bézier curve c_B(t) on the Manifold M with respect to its control points B given as a point on the PowerManifold assuming C1 conditions and known degrees. The curve is evaluated at the points given in T (elementwise in 0N, where N is the number of segments of the Bézier curve). The get_Bezier_junctions are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see grad_L2_acceleration_Bezier and set λ=0 therein. This gradient is computed using adjoint_Jacobi_fields. For details, see Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018. See de_Casteljau for more details on the curve.\n\nSee also\n\nacceleration_Bezier,  grad_L2_acceleration_Bezier, L2_acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.RiemannianMeanCost","page":"Objectives","title":"ManoptExamples.RiemannianMeanCost","text":"RiemannianMeanCost{P}\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\nf(p) = sum_j=i^N d_mathcal M^2(d_i p)\n\nwhere d_mathcal M is the distance on a Riemannian manifold.\n\nConstructor\n\nRiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P}\n\nInitialize the cost function to a data set data of points on a manfiold M, where each point is of type P.\n\nSee also\n\nRiemannianMeanGradient!!, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RiemannianMeanGradient!!","page":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","text":"RiemannianMeanGradient!!{P} where P\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\noperatornamegradf(p) = sum_j=i^N log_p d_i\n\nwhere d_mathcal M is the distance on a Riemannian manifold and we employ grad_distance to compute the single summands.\n\nThis functor provides both the allocating variant grad_f(M,p) as well as the in-place variant grad_f!(M, X, p) which computes the gradient in-place of X.\n\nConstructors\n\nRiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T}\n\nGenerate the Riemannian mean gradient based on some data points data an intial tangent vector initial_vector. If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant.\n\nRiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T}\n\nInitialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold M is provideed.\n\nSee also\n\nRiemannianMeanCost, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.Riemannian_mean_objective","page":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","text":"Riemannian_mean_objective(data, initial_vector=nothing, evaluation=Manopt.AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n)\n\nGenerate the objective for the Riemannian mean task for some given vector of data points on the Riemannian manifold M.\n\nSee also\n\nRiemannianMeanCost, RiemannianMeanGradient!!\n\nnote: Note\nThe first constructor should only be used if an additional storage of the vector is not feasible, since initialising the initial_vector to nothing disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided.\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.RobustPCACost","page":"Objectives","title":"ManoptExamples.RobustPCACost","text":"RobustPCACost{D,F}\n\nA functor representing the Riemannian robust PCA function on the Grassmann manifold. For some given (column) data Dmathbb R^dtimes n the cost function is defined on some operatornameGr(dm), mn as the sum of the distances of the columns D_i to the subspace spanned by pinoperatornameGr(dm) (represented as a point on the Stiefel manifold). The function reads\n\nf(U) = frac1nsum_i=1^n lVert pp^mathrmTD_i - D_irVert\n\nThis cost additionally provides a Huber regularisation of the cost, that is for some ε0 one use ℓ_ε(x) = sqrtx^2+ε^2 - ε in\n\nf_ε(p) = frac1nsum_i=1^n ℓ_εbigl(lVert pp^mathrmTD_i - D_irVertbigr)\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCACost(data::AbstractMatrix, ε=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, ε=1.0)\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RobustPCAGrad!!","page":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","text":"RobustPCAGrad!!{D,F}\n\nA functor representing the Riemannian robust PCA gradient on the Grassmann manifold. For some given (column) data Xmathbb R^ptimes n the gradient of the RobustPCACost can be computed by projecting the Euclidean gradient onto the corresponding tangent space.\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCAGrad!!(data, ε=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, ε=1.0; evaluation=AllocatingEvaluation())\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the evaluation= keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.robust_PCA_objective","page":"Objectives","title":"ManoptExamples.robust_PCA_objective","text":"robust_PCA_objective(data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluton())\n\nGenerate the objective for the robust PCA task for some given data D and Huber regularization parameter ε.\n\nSee also\n\nRobustPCACost, RobustPCAGrad!!\n\nnote: Note\nSince the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the evaluation, indeed the gradient always allows for both the allocating and the in-place variant to be used, though that keyword is used to setup the objective.\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.RosenbrockCost","page":"Objectives","title":"ManoptExamples.RosenbrockCost","text":"RosenbrockCost\n\nProvide the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nf(mathcal M p) = a(p_1^2-p_2)^2 + (p_1-b)^2\n\nwhich means that for the 2D case, the manifold mathcal M is ignored.\n\nSee also 📖 Rosenbrock (with slightly different parameter naming).\n\nConstructor\n\nf = Rosenbrock(a,b)\n\ngenerates the struct/function of the Rosenbrock cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockGradient!!","page":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","text":"RosenbrockGradient\n\nProvide Euclidean gradient fo the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nnabla f(mathcal M p) = beginpmatrix\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \n    -2a(p_1^2-p_2)\nendpmatrix\n\ni.e. also here the manifold is ignored.\n\nConstructor\n\nRosenbrockGradient(a,b)\n\nFunctors\n\ngrad_f!!(M,p)\ngrad_f!!(M, X, p)\n\nevaluate the gradient at p the manifoldmathcal M is ignored.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockMetric","page":"Objectives","title":"ManoptExamples.RosenbrockMetric","text":"RosenbrockMetric <: AbstractMetric\n\nA metric related to the Rosenbrock problem, where the metric at a point pmathbb R^2 is given by\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nwhere the mathrmRb stands for Rosenbrock\n\n\n\n\n\n","category":"type"},{"location":"objectives/#Base.log-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}}, ManoptExamples.RosenbrockMetric}, Any, Any}","page":"Objectives","title":"Base.log","text":"X = log(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, X, p, q)\n\nCompute the logarithmic map with respect to the RosenbrockMetric. The formula reads for any j  1m\n\nX = beginpmatrix\n  q_1 - p_1 \n  q_2 - p_2 + (q_1 - p_1)^2\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.inverse_local_metric-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.inverse_local_metric","text":"inverse_local_metric(::MetricManifold{ℝ,Euclidean{ℝ,Tuple{2}},RosenbrockMetric}, p)\n\nReturn the inverse of the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG^-1_p =\nbeginpmatrix\n    1  2p_1\n    2p_1  1+4p_1^2 \nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.local_metric-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.local_metric","text":"local_metric(::MetricManifold{ℝ,Euclidean{ℝ,<:Union{TypeParameter{Tuple{2}},Tuple{<:Int}}},RosenbrockMetric}, p)\n\nReturn the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.change_representer-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","page":"Objectives","title":"ManifoldsBase.change_representer","text":"Y = change_representer(M::MetricManifold{ℝ,Euclidean{ℝ,Tuple{2}},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{ℝ,Euclidean{ℝ,Tuple{2}},RosenbrockMetric}, Y, ::EuclideanMetric, p, X)\n\nGiven the Euclidean gradient X at p, this function computes the corresponding Riesz representer Ysuch that⟨X,Z⟩ = ⟨ Y, Z ⟩_{\\mathrm{Rb},p}holds for allZ, in other wordsY = G(p)^{-1}X`.\n\nthis function is used in riemannian_gradient to convert a Euclidean into a Riemannian gradient.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.inner-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","page":"Objectives","title":"ManifoldsBase.inner","text":"inner(M::MetricManifold{ℝ,Euclidean{ℝ,<:Union{TypeParameter{Tuple{2}},Tuple{<:Int}}},RosenbrockMetric}, p, X, Y)\n\nCompute the inner product on mathbb R^2 with respect to the RosenbrockMetric, i.e. for XY in T_pmathcal M we have\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1\n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Rosenbrock_objective","page":"Objectives","title":"ManoptExamples.Rosenbrock_objective","text":"Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation())\n\nReturn the gradient objective of the Rosenbrock example.\n\nSee also RosenbrockCost, RosenbrockGradient!!\n\nnote: Note\n\n\nThe objective is available when Manopt.jl is loaded.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.expt-Tuple{ManifoldsBase.MetricManifold{ℝ, <:Manifolds.Euclidean{ℝ, <:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","page":"Objectives","title":"ManoptExamples.expt","text":"q = exp(::MetricManifold{ℝ,Euclidean{ℝ,Tuple{2}},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{ℝ,Euclidean{ℝ,Tuple{2}},RosenbrockMetric}, q, p, X)\n\nCompute the exponential map with respect to the RosenbrockMetric.\n\n    q = beginpmatrix p_1 + X_1  p_2+X_2+X_1^2endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","page":"Objectives","title":"ManoptExamples.minimizer","text":"minimizer(::RosenbrockCost)\n\nReturn the minimizer of the RosenbrockCost, which is given by\n\np^* = beginpmatrix bb^2 endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.Intrinsic_infimal_convolution_TV12","text":"Intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are second_order_Total_Variation and Total_Variation. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( βmathrmTV(v) + (1-β)mathrmTV_2(w) bigr)\n\nfor more details see [BFPS17, BFPS18].\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation-NTuple{4, Any}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation","text":"L2_Total_Variation(M, p_data, α, p)\n\ncompute the ℓ^2-TV functional on the PowerManifold M for given (fixed) data p_data (on M), a nonnegative weight α, and evaluated at p (on M), i.e.\n\nE(p) = d_mathcal M^2(fp) + alpha operatornameTV(p)\n\nSee also\n\nTotal_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation_1_2-Tuple{ManifoldsBase.PowerManifold, Vararg{Any, 4}}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation_1_2","text":"L2_Total_Variation_1_2(M, f, α, β, x)\n\ncompute the ℓ^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + βoperatornameTV_2(x)\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_second_order_Total_Variation-Tuple{ManifoldsBase.PowerManifold, Any, Any, Any}","page":"Objectives","title":"ManoptExamples.L2_second_order_Total_Variation","text":"L2_second_order_Total_Variation(M, f, β, x)\n\ncompute the ℓ^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + βoperatornameTV_2(x)\n\nas used in [BBSW16].\n\nSee also\n\nsecond_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Total_Variation","page":"Objectives","title":"ManoptExamples.Total_Variation","text":"Total_Variation(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1kcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nsee [WDS14] for more details. In long function names, this might be shortened to TV1 and the 1 might be ommitted if only total variation is present.\n\nSee also\n\ngrad_Total_Variation, prox_Total_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\nadjoint_differential_forward_logs!(M, Y, p, X)\n\nCompute the adjoint differential of forward_logs F occurring, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n. The adjoint differential can be computed in place of Y.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOutput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_forward_logs-Tuple{ManifoldsBase.PowerManifold, Any, Any}","page":"Objectives","title":"ManoptExamples.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\ndifferential_forward_logs!(M, Y, p, X)\n\ncompute the differential of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i  mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the   logs, where mathcal N is the power manifold with the number of dimensions added   to size(x). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.forward_logs","text":"Y = forward_logs(M,x)\nforward_logs!(M, Y, x)\n\ncompute the forward logs F (generalizing forward differences) occurring, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i. This can also be done in place of ξ.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is the power manifold with the number of dimensions added to size(x). The computation can be done in place of Y.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, λ, x[, p=1])\ngrad_Total_Variation!(M, X, λ, x[, p=1])\n\nCompute the (sub)gradient f of all forward differences occurring, in the power manifold array, i.e. of the function\n\nf(p) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nX – resulting tangent vector in T_xmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Any}} where T","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, (x,y)[, p=1])\ngrad_Total_Variation!(M, X, (x,y)[, p=1])\n\ncompute the (deterministic) (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.grad_intrinsic_infimal_convolution_TV12","text":"grad_u, grad_v = grad_intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see second_order_Total_Variation, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nβmathrmTV(v) + (1-β)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, grad_Total_Variation and grad_second_order_Total_Variation, respectively.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"grad_second_order_Total_Variation(M::PowerManifold, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1q_2q_3) with respect to all q_1q_2q_3 occurring along any array dimension in the point q, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation-2","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"Y = grad_second_order_Total_Variation(M, q[, p=1])\ngrad_second_order_Total_Variation!(M, Y, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1 q_2 q_3) with respect to all three components of qmathcal M^3, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\nmathcal C = bigl c  mathcal M   g(tfrac12q_1q_3) text for some geodesic gbigr\n\ndenote the mid points between q_1 and q_3 on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(q_1q_2q_3) = min_c  mathcal C_q_1q_3 d(c q_2)\n\nWhile the (sub)gradient with respect to q_2 is easy, the other two require the evaluation of an adjoint_Jacobi_field.\n\nThe derivation of this gradient can be found in [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.project_collaborative_TV","page":"Objectives","title":"ManoptExamples.project_collaborative_TV","text":"project_collaborative_TV(M, λ, x, Ξ[, p=2,q=1])\nproject_collaborative_TV!(M, Θ, λ, x, Ξ[, p=2,q=1])\n\ncompute the projection onto collaborative Norm unit (or α-) ball, i.e. of the function\n\nF^q(x) = sum_imathcal G\n  Bigl( sum_jmathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for xmathcal M and mathcal I_i is the set of its forward neighbors. The computation can also be done in place of Θ.\n\nThis is adopted from the paper Duran, Möller, Sbert, Cremers, SIAM J Imag Sci, 2016, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"ξ = prox_Total_Variation(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all forward differences occurring in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting  point containing with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"[y1,y2] = prox_Total_Variation(M, λ, [x1,x2] [,p=1])\nprox_Total_Variation!(M, [y1,y2] λ, [x1,x2] [,p=1])\n\nCompute the proximal map operatornameprox_λvarphi of φ(xy) = d_mathcal M^p(xy) with parameter λ. A derivation of this closed form solution is given in see [WDS14].\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_λφ((x1,x2)). The result can also be computed in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.prox_parallel_TV","page":"Objectives","title":"ManoptExamples.prox_parallel_TV","text":"y = prox_parallel_TV(M, λ, x [,p=1])\nprox_parallel_TV!(M, y, λ, x [,p=1])\n\ncompute the proximal maps operatornameprox_λφ of all forward differences occurring in the power manifold array, i.e. φ(x_ix_j) = d_mathcal M^p(x_ix_j) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements). The computation can also be done in place.\n\nSee also prox_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"(y1,y2,y3) = prox_second_order_Total_Variation(M,λ,(x1,x2,x3),[p=1], kwargs...)\nprox_second_order_Total_Variation!(M, y, λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_λvarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2. The result can be computed in place of y.\n\nNote that this function does not have a closed form solution but is solbed by a few steps of the subgradient mehtod from manopt.jl by default. See [BBSW16] for a derivation.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map. The computation can also be done in place.\nnote: Note\n\nThis function requires Manopt.jl to be loaded\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation-Union{Tuple{T}, Tuple{N}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any, Int64}} where {N, T}","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"y = prox_second_order_Total_Variation(M, λ, x[, p=1])\nprox_second_order_Total_Variation!(M, y, λ, x[, p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all centered second order differences occurring in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting point with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place.\n\nnote: Note\nThis function requires Manopt.jl to be loaded\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1kcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i() denotes the mid point between its two arguments that is nearest to x_i, see [BBSW16] for a derivation.\n\nIn long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation-Union{Tuple{T}, Tuple{MT}, Tuple{MT, Tuple{T, T, T}}, Tuple{MT, Tuple{T, T, T}, Any}} where {MT<:ManifoldsBase.AbstractManifold, T}","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the function reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nsee [BBSW16] for a derivation. In long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation, Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.subgrad_Total_Variation","page":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","text":"X = subgrad_TV(M, λ, p[, k=1; atol=0])\nsubgrad_TV!(M, X, λ, p[, k=1; atol=0])\n\nCompute the (randomized) subgradient partial F of all forward differences occurring, in the power manifold array, i.e. of the function\n\nF(p) = sum_isum_j  mathcal I_i d^k(p_ip_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\np – a point.\n\nOuput\n\nX – resulting tangent vector in T_pmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.subgrad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","text":"X = subgrad_TV(M, (p,q)[, k=1; atol=0])\nsubgrad_TV!(M, X, (p,q)[, k=1; atol=0])\n\ncompute the (randomized) subgradient of frac1kd^k_mathcal M(pq) with respect to both p and q (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.assemble_local_jacobian_with_connection!-NTuple{14, Any}","page":"Objectives","title":"ManoptExamples.assemble_local_jacobian_with_connection!","text":"This function is called to assemble (one block of) the matrix for the Newton step locally (i.e. in one interval)\n\nM :                     manifold yleft:                 left value of iterate\nyright:                right value of iterate\nA:                      Matrix to be written into\nh:                      length of interval\ni:                      index of interval\nbaseansatz:                      basis vector for ansatz function\nbfl:                    0/1 scaling factor at left boundary\nbfr:                    0/1 scaling factor at right boundary \nbasetest:                      basis vector for test function\ntfl:                    0/1 scaling factor at left boundary\ntfr:                    0/1 scaling factor at right boundary \nintegrand:\t            integrand of the functional as a struct, must have a field value and a field derivative\ntransport:\t            vectortransport used to compute the connection term (as a struct, must have a field value and a field derivative)\n\nKeyword Arguments:\n\nrow_index:                row index of block inside system\n\n\n...\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_jacobian_block!-Tuple{ManifoldsBase.ProductManifold, Vararg{Any, 6}}","page":"Objectives","title":"ManoptExamples.get_jacobian_block!","text":"This function is called by Newton's method to compute one block of the matrix for the Newton step\n\nInput:\n\nM:                      Product manifold\\\ny:                      iterate\\\neval:                   function that evaluates y at left and right boundary point of i-th interval, signature: eval(y, i, scaling), must return an element of M A:                      Matrix to be written into\\\nintegrand:\t            integrand of the functional as a struct, must have a field value and a field derivative\\\ntransport:\t            vectortransport used to compute the connection term (as a struct, must have a field value and a field derivative)\\\ntime_intervals:\t\t\ttime interval with discrete time points\n\nKeyword arguments:\n\nrowindex:                  row index of block inside system\\\ncolumnindex:               column index of block inside system\\\ntestspace:    \t\t\t\tspace of test functions as a struct, must have a field manifold (base manifold of the tangent spaces) and a field degree (degree of test functions (1: linear, 0: constant))\\\nansatzspace:   \t\t\tspace of ansatz functions as a struct, must have a field manifold (base manifold of the tangent spaces) and a field degree (degree of ansatz functions (1: linear, 0: constant))\\\n ...\n\n\n\n\n\n","category":"method"},{"location":"examples/CRPG-Convex-SPD/#A-Geodesically-Convex-Example-on-SPDs","page":"Convex Example on SPDs","title":"A Geodesically Convex Example on SPDs","text":"Hajg Jasa 2025-04-16","category":"section"},{"location":"examples/CRPG-Convex-SPD/#Introduction","page":"Convex Example on SPDs","title":"Introduction","text":"In this example we compare the Convex Riemannian Proximal Gradient (CRPG) method [BJJP25a] with the Cyclic Proximal Point Algorithm, which was introduced in [Bac14], on the space of symmetric positive definite matrices. This example reproduces the results from [BJJP25a], Section 5.3.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots, LaTeXStrings, CairoMakie, Chairmarks\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples\nimport ColorSchemes.tol_vibrant","category":"section"},{"location":"examples/CRPG-Convex-SPD/#The-Problem","page":"Convex Example on SPDs","title":"The Problem","text":"Let mathcal M = mathcal H^2 be the 2-dimensional hyperbolic space.\n\nLet g colon mathcal M to mathbb R be defined by\n\ng(p) = log(det(p))^4\n\nObserve that the function g is geodesically convex with respect to the Riemannian metric on mathcal M.\n\nLet now q_1 be a given point, and let h colon mathcal M to mathbb R be defined by\n\nh(p) = tau mathrmdist(p q_1)\n\nfor some tau  0. We define our total objective function as f = g + h. Notice that this objective function is also geodesically convex with respect to the Riemannian metric on mathcal M. The goal is to find the minimizer of f on mathcal M.","category":"section"},{"location":"examples/CRPG-Convex-SPD/#Numerical-Experiment","page":"Convex Example on SPDs","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\nrandom_seed = 42\n\natol = 1e-7\nmax_iters = 20000\nτ = 1/2 # weight for the component h\nspd_dims = [2, 3, 4, 5]\n\n# Objective, gradient, and proxes\ng(M, p) = log(det(p))^4\ngrad_g(M, p) = 4log(det(p))^3 * p\n#\nh(M, p, q) = τ * distance(M, p, q)\nprox_h(M, λ, p, q) = ManifoldDiff.prox_distance(M, τ * λ, q, p, 1)\n#\nf(M, p, q) = g(M, p) + h(M, p, q)\n# Function to generate points close to the given point p\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend\n# Estimate Lipschitz constant of the gradient of g\nfunction theoretical_lipschitz_constant(M, anchor, n, R=D, N=100_000)\n    constants = []\n    for i in 1:N\n        p = close_point(M, anchor, R)\n\n        push!(constants, 12 * n * log(det(p))^2)\n    end\n    return maximum(constants)\nend\n\nWe introduce some keyword arguments for the solvers we will use in this experiment\n\n# Solver arguments for backtracking\npgm_kwargs(contraction_factor, initial_stepsize, warm_start_factor) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        contraction_factor=contraction_factor,\n        strategy=:convex,\n        initial_stepsize=initial_stepsize,\n        stop_when_stepsize_less=atol,\n        warm_start_factor=warm_start_factor,\n    ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs(contraction_factor, initial_stepsize, warm_start_factor) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        contraction_factor=contraction_factor,\n        strategy=:convex,\n        initial_stepsize=initial_stepsize,\n        stop_when_stepsize_less=atol,\n        warm_start_factor=warm_start_factor,\n    ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n# Solver arguments for constant stepsize\npgm_kwargs_constant(stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs_constant(stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n\nBefore running the experiments, we initialize data collection functions that we will use later\n\n# Header for the dataframe\nglobal col_names_1 = [\n    :Dimension,\n    :Iterations_1,\n    :Time_1,\n    :Cost_1,\n    :Iterations_2,\n    :Time_2,\n    :Cost_2,\n]\ncol_types_1 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)\n# Function for initializing the dataframe\nfunction initialize_dataframes(\n    results_folder,\n    experiment_name,\n    named_tuple_1,\n)\n    A1 = DataFrame(named_tuple_1)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"-Comparisons.csv\",\n        ),\n        A1;\n        header=false,\n    )\n    return A1\nend\n\nfunction export_dataframes(\n    M,\n    records,\n    times,\n)\n    B1 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations_1=maximum(first.(records[1])),\n        Time_1=times[1],\n        Cost_1=minimum([r[2] for r in records[1]]),\n        Iterations_2=maximum(first.(records[2])),\n        Time_2=times[2],\n        Cost_2=minimum([r[2] for r in records[2]]),\n    )\n    return B1\nend\nfunction write_dataframes(\n    B1,\n    results_folder,\n    experiment_name,\n)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"-Comparisons.csv\",\n        ),\n        B1;\n        append=true,\n    )\nend\n\nglobal A1_SPD = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    named_tuple_1,\n)\nstats = Dict(:CRPG_CN => Dict(), :CRPG_BT => Dict())\nfor n in spd_dims\n\n    Random.seed!(random_seed)\n\n    M = SymmetricPositiveDefinite(Int(n))\n    q = rand(M)\n    p0 = rand(M)\n\n    prox_h_spd(M, λ, p) = prox_h(M, λ, p, q)\n    f_spd(M, p) = f(M, p, q)\n\n    D = 4*distance(M, p0, q)\n    # Conseravative estimate of the Lipschitz constant for grad_g\n    L_g = 1.05 * theoretical_lipschitz_constant(M, p0, n, D/2)\n    constant_stepsize = 1/L_g\n    initial_stepsize = 3/2 * constant_stepsize\n    contraction_factor = 0.9\n    warm_start_factor = 2.0\n\n    # Optimization\n    pgm_constant = proximal_gradient_method(M, f_spd, g, grad_g, p0;\n        prox_nonsmooth=prox_h_spd,\n        pgm_bm_kwargs_constant(constant_stepsize)...\n    )\n    pgm_constant_result = get_solver_result(pgm_constant)\n    pgm_constant_record = get_record(pgm_constant)\n    stats[:CRPG_CN][n] = Dict()\n    stats[:CRPG_CN][n][:Iteration] = length(get_record(pgm_constant, :Iteration)) + 1\n    stats[:CRPG_CN][n][:Cost] = get_record(pgm_constant, :Iteration, :Cost)\n    pushfirst!(stats[:CRPG_CN][n][:Cost], f_spd(M, p0))\n\n    # We can also use a backtracked stepsize\n    pgm = proximal_gradient_method(M, f_spd, g, grad_g, p0;\n        prox_nonsmooth=prox_h_spd,\n        pgm_kwargs(contraction_factor, initial_stepsize, warm_start_factor)...\n    )\n    pgm_result = get_solver_result(pgm)\n    pgm_record = get_record(pgm)\n    stats[:CRPG_BT][n] = Dict()\n    stats[:CRPG_BT][n][:Iteration] = length(get_record(pgm, :Iteration)) + 1\n    stats[:CRPG_BT][n][:Cost] = get_record(pgm, :Iteration, :Cost)\n    pushfirst!(stats[:CRPG_BT][n][:Cost], f_spd(M, p0))\n\n    records = [\n        pgm_constant_record,\n        pgm_record,\n    ]\n\n    # Benchmarking\n    if benchmarking\n        pgm_constant_bm = @benchmark proximal_gradient_method($M, $f_spd, $g, $grad_g, $p0;\n            prox_nonsmooth=$prox_h_spd,\n            $pgm_bm_kwargs_constant($constant_stepsize)...\n        )\n        pgm_bm = @benchmark proximal_gradient_method($M, $f_spd, $g, $grad_g, $p0;\n            prox_nonsmooth=$prox_h_spd,\n            $pgm_bm_kwargs($contraction_factor, $initial_stepsize, $warm_start_factor)...\n        )\n\n        times = [\n            median(pgm_constant_bm).time * 1e-9,\n            median(pgm_bm).time * 1e-9,\n        ]\n        # Export the results\n        B1 = export_dataframes(\n            M,\n            records,\n            times,\n        )\n        append!(A1_SPD, B1)\n        (export_table) && (write_dataframes(B1, results_folder, experiment_name))\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to CRPG with a constant stepsize, while columns 5 to 7 refer to the backtracked case…\n\n| **Dimension** | **Iterations\\_1** | **Time\\_1** | **Cost\\_1** | **Iterations\\_2** | **Time\\_2** | **Cost\\_2** |\n|--------------:|------------------:|------------:|------------:|------------------:|------------:|------------:|\n|             3 |               367 |  0.00453621 |     0.18593 |               249 |  0.00818665 |     0.18593 |\n|             6 |              1944 |   0.0414022 |     0.27078 |              1341 |    0.094039 |     0.27078 |\n|            10 |              8640 |    0.280774 |    0.371274 |              6027 |    0.578688 |    0.371274 |\n|            15 |             15535 |    0.645499 |    0.449625 |             12544 |     3.05128 |    0.449625 |\n\nLastly, we showcase the rate of decay of the function values for n = 2.\n\nfunction plot_convergence(\n    stats,\n    dimensions=[2],\n)\n    # Extract the results for the specified dimensions\n    crpg_cn = [stats[:CRPG_CN][d] for d in dimensions]\n    crpg_bt = [stats[:CRPG_BT][d] for d in dimensions]\n\n    figs = Vector{Figure}()\n\n    for i in 1:length(crpg_cn)\n\n        # Get the solvers' results and records\n        # crpg_cn_result = crpg_cn[i][:Result]\n        crpg_cn_record = crpg_cn[i][:Cost]\n\n        # crpg_bt_result = crpg_bt[i][:Result]\n        crpg_bt_record = crpg_bt[i][:Cost]\n\n\n        # Calculate the minimum cost for relative error\n        min_cost_crpg_cn = minimum(crpg_cn_record)\n        min_cost_crpg_bt = minimum(crpg_bt_record)\n\n        # Create vectors for plotting\n        iterations_crpg_cn = 1:crpg_cn[i][:Iteration]\n        iterations_crpg_bt = 1:crpg_bt[i][:Iteration]\n\n        # Get initial error for scaling reference lines\n        relative_errors_crpg_cn = max.(crpg_cn_record .- min_cost_crpg_cn, 0)\n        initial_error_crpg_cn = relative_errors_crpg_cn[1]\n\n        relative_errors_crpg_bt = max.(crpg_bt_record .- min_cost_crpg_bt, 0)\n        initial_error_crpg_bt = relative_errors_crpg_bt[1]\n\n        initial_error = max(\n            initial_error_crpg_cn,\n            initial_error_crpg_bt,\n        )\n\n        iterations = max(\n            iterations_crpg_cn,\n            iterations_crpg_bt,\n        )\n\n        # Create reference trajectories\n        # O(1/k)\n        ref_rate_1 = [initial_error/k for k in iterations]\n        # O(1/2^k)\n        ref_rate_2k = [initial_error/2^k for k in iterations]\n\n        # Create the convergence plot\n        fig = Figure()\n        ax = Axis(fig[1, 1],\n            title =L\"\\mathcal{P}(%$(dimensions[i]))\",\n            xlabel =L\"\\text{Iterations }(k)\",\n            ylabel =L\"f(p_k) - f_*\",\n            # xscale=log10,\n            yscale=log10,\n            yminorticksvisible = true,\n            yminorgridvisible = true,\n            yminorticks = IntervalsBetween(8),\n        )\n        lines!(\n            ax,\n            iterations_crpg_cn,\n            abs.(relative_errors_crpg_cn);\n            label=\"CRPG, constant step\",\n            linewidth=2,\n            color=tol_vibrant[1],\n        )\n        lines!(\n            ax,\n            iterations_crpg_bt,\n            abs.(relative_errors_crpg_bt);\n            label=\"CRPG, backtracked step\",\n            linewidth=2,\n            color=tol_vibrant[5],\n        )\n        lines!(\n            ax,\n            iterations,\n            abs.(ref_rate_1);\n            linestyle=:dash,\n            linewidth=1.5,\n            color=:blue,\n            label=L\"\\mathcal O\\left(k^{-1}\\right)\"\n        )\n        lines!(\n            ax,\n            iterations,\n            abs.(ref_rate_2k);\n            linestyle=:dot,\n            linewidth=1.5,\n            color=:black,\n            label=L\"\\mathcal O\\left(2^{-k}\\right)\"\n        )\n        fig[1, 2] = Legend(fig, ax, framevisible = true)\n        fig\n\n        push!(figs, fig)\n    end\n    return figs\nend\nfigs = plot_convergence(stats)\n\n<details class=\"code-fold\"> <summary>Code</summary>\n\nfor fig in figs\n    display(fig)\nend\n\n</details>\n\n<img src=\"CRPG-Convex-SPD_files/figure-commonmark/cell-13-output-1.png\" width=\"672\" height=\"480\" />\n\nThis is in line with the convergence rates of the CRPG method in the geodesically convex setting, as shown in [BJJP25a], Theorem 4.7.","category":"section"},{"location":"examples/CRPG-Convex-SPD/#Technical-details","page":"Convex Example on SPDs","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 15, 2025, 13:40:45.","category":"section"},{"location":"examples/CRPG-Convex-SPD/#Literature","page":"Convex Example on SPDs","title":"Literature","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization, preprint (2025), arXiv:2507.16055.\n\n\n\n","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#Sparse-PCA","page":"Sparse PCA","title":"Sparse PCA","text":"Paula John, Hajg Jasa 2025-10-01","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#Introduction","page":"Sparse PCA","title":"Introduction","text":"In this example we use the Nonconvex Riemannian Proximal Gradient (NCRPG) method [BJJP25b] and compare it to the Riemannian Proximal Gradient (RPG) method [HW21]. This example reproduces the results from [BJJP25b], Section 6.1. The numbers may vary slightly due to having run this notebook on a different machine.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots, LaTeXStrings\nusing Random, LinearAlgebra, LRUCache, Distributions\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#The-Problem","page":"Sparse PCA","title":"The Problem","text":"Let mathcal M = mathrmOB(nr) be the oblique manifold, i.e., the set of n times r matrices with unit-norm columns. Let g colon mathcal M to mathbb R be defined by\n\ng(X) = frac12 Vert X^top A^top A X - D^2 Vert^2\n\nwhere A in mathbb R^m times n is a data matrix, D = mathrmdiag(d_1 ldots d_r) is a diagonal matrix containing the top r singular values of A, and Vert cdot Vert is the Frobenius norm.\n\nLet h colon mathcal M to mathbb R be defined by\n\nh(X) = mu Vert X Vert_1\n\nbe the sparsity-enforcing term given by the ell_1-norm, where mu ge 0 is a regularization parameter.\n\nWe define our total objective function as f = g + h. The goal is to find the minimizer of f on mathcal M, which is heuristically the point that diagonalizes A^top A as much as possible while being sparse.","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#Numerical-Experiment","page":"Sparse PCA","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\n# Set random seed for reproducibility\nrandom_seed = 1520\nRandom.seed!(random_seed)\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 2.0\nm_tests = 10 # number of tests for each parameter setting\nmeans = 20 # number of means to compute\n\natol = 1e-7\nmax_iters = 100000\nn_p_array = [(100,5), (200,5), (300, 5)]\nμs = [t for t in [0.1, 0.5, 1.0]]\n\nWe define a function to generate the test data for the Sparse PCA problem.\n\nfunction gen_test_data_SPCA(n, m, p)\n    A = rand(Normal(0, 1.0), (m, n))\n    for i in 1:n\n        A[:, i] = A[:, i] .- mean(A[:, i])\n        A[:, i] = A[:, i] / std(A[:, i])\n    end\n    svdA = svd(A)\n    Vt = svdA.Vt\n    PCs = Vt[:, 1:p]\n    d = svdA.S[1:p]\n    return A, PCs, d\nend\n\nWe define the proximal operator for the \\ell_1-norm on the oblique manifold, following [BJJP25b].\n\n# Returns prox_{μ||.||_1}(M,x) on the Oblique Manifold OB(n,p) with respect to riemannian distance\nfunction prox_l1_OB(n, p, μ; tol = 1e-10, max_iters = 10)\n    return function prox_l1_OB_μ(M, λ, X)\n        μλ = μ * λ\n        prox_X = Array{Float64}(undef, n, p)\n        for k in 1:p\n            x = X[:, k]\n            t = μλ\n            px_t = X[:, k]\n            for _ in 1:max_iters\n                t_old = t\n\n                z = abs.(x) .- t\n                prox_Rn_t = (z .> 0) .* sign.(x) .* z\n\n                px_t = prox_Rn_t/norm(prox_Rn_t)\n                xpx_t = x'px_t\n                if xpx_t < 1\n                    t = μλ * sqrt(1-(xpx_t)^2)/acos(xpx_t)\n                else\n                    px_t = x\n                    prox_X[:, k] = x\n                    break\n                end\n                if abs(t - t_old) < tol\n                    prox_X[:, k] = px_t\n                    break\n                end\n            end\n            prox_X[:, k] = px_t\n        end\n        return prox_X\n    end\nend\n\n# Objective, gradient, and proxes\ng_(M, X, H, D) = 0.5 * norm(X'H * X - D)^2\nfunction grad_g_(M, X, H, D)\n    HX = H*X\n    return project(M, X, 2*HX*(X'HX - D))\nend\nh_(M, X, μ) = μ * norm(X, 1)\nf_(M, X, H, D, μ) = 0.5 * norm(X'H * X - D)^2 + μ * norm(X, 1)\n\nWe introduce an implementation of the RPG method for the Sparse PCA problem on the oblique manifold, following [HW21].\n\n# Implementation of the proximal operator for the ℓ1-norm on the Oblique manifold\nfunction RPG_prox_OB(S, X, grad_fX, λ, L, n, p; max_iters  = 10, tol=1e-10)\n    λ̃ = λ/L\n    d = 0\n    for k in 1:p\n        x = X[:,k]\n        ξ_x = 1/L * grad_fX[:,k]\n\n        neg∇h = (x-ξ_x)/λ̃\n        i_max = argmax(abs.(neg∇h))\n        if abs(neg∇h[i_max]) <= 1.0\n            y = sign(neg∇h[i_max])*(1:n .== i_max)\n        else\n            z = abs.(neg∇h) .- 1.0\n            Act_set = z .> 0\n            y = Act_set .* sign.(neg∇h) .* z\n            y = y/norm(y)\n        end\n        for j in 1:max_iters\n            xty = x'y\n            if xty >= 1\n                sy = -1\n                ty = 1\n            else\n                ξty = ξ_x'y\n                acosxty = acos(xty)\n                α = 1-xty^2\n                sy = - acosxty/sqrt(α) - ξty/α + acosxty * ξty * xty / sqrt(α^3)\n                ty = acosxty/sqrt(α)\n            end\n            neg∇h = -(sy*x+ty*ξ_x)/λ̃\n\n            i_max = argmax(abs.(neg∇h))\n            if abs(neg∇h[i_max]) <= 1.0\n                y_new = sign(neg∇h[i_max])*(1:n .== i_max)\n            else\n                z = abs.(neg∇h) .- 1.0\n                Act_set = z .> 0\n                y_new = Act_set .* sign.(neg∇h) .* z\n                y_new = y_new/norm(y_new)\n            end\n\n            if max(abs(xty-x'y_new), abs(ξ_x'y-ξ_x'y_new)) < tol\n                break\n            end\n            y = y_new\n        end\n\n        d += distance(S, x,y)^2\n        X[:,k] = y\n    end\n    return sqrt(d)\nend\n#\n# RPG implementation for Sparse PCA on the Oblique manifold\nfunction RPG_SPCA_OB(M, H, D, μ, L, start, prox_fun; max_iters  = 1000, stop = 1e-8, record = false)\n    n, p = size(start)\n    S = Sphere(n-1)\n    cost_fun(M,X) =  0.5*norm(X'H*X-D)^2 + μ*norm(X,1)\n    function grad_f(M,X, D=D)\n        HX = H*X\n        return project(M, X, 2*HX*(X'HX-D))\n    end\n    X = copy(start)\n    if !record\n        for i in 1:max_iters\n            change = prox_fun(S, X, grad_f(M,X,D), μ, L, n, p)\n            if L*change < stop\n                return X, i\n            end\n        end\n        return X, max_iters\n    else\n        Iterates = []\n        for i in 1:max_iters\n            change = prox_fun(S, X, grad_f(M,X,D), μ, L, n, p)\n            push!(Iterates, copy(X))\n            if L*change < stop\n                return Iterates, i\n            end\n        end\n        return Iterates, max_iters\n    end\nend\n\nWe set up some variables to collect the results of the experiments and initialize the dataframes\n\nAnd run the experiments\n\nfor (n, p) in n_p_array\n    # Define manifold\n    OB = Oblique(n, p)\n    for m in 1:m_tests\n        # Construct problem\n        A, PCs, d = gen_test_data_SPCA(n, means, p)\n        H = A'A / norm(A'A) * 10\n        D = diagm(svd(H).S[1:p])\n        L = 2 * tr(H)\n\n        for (c, μ) in enumerate(μs)\n            # Localize functions\n            g(M, X) = g_(M, X, H, D)\n            grad_g(M, X) = grad_g_(M, X, H, D)\n            h(M, X) = h_(M, X, μ)\n            prox_norm1_NCRPG = prox_l1_OB(n, p, μ)\n            f(M, X) = f_(M, X, H, D, μ)\n            #\n            # Parameters\n            step_size = 1/L\n            init_step_size_bt = 10 * step_size\n            stop_step_size_bt = atol\n            stop_RPG = atol\n            stop_NCRPG = atol\n            stop_NCRPG_bt = atol\n            #\n            # Fix starting point\n            start = rand(OB)\n            #\n            # Optimization\n            # NCRPG\n            rec_NCRPG = proximal_gradient_method(OB, f, g, grad_g, start;\n                prox_nonsmooth = prox_norm1_NCRPG,\n                stepsize = ConstantLength(step_size),\n                record = [:Iteration, :Iterate],\n                return_state = true,\n                stopping_criterion = StopAfterIteration(max_iters)| StopWhenGradientMappingNormLess(stop_NCRPG)\n            )\n            # Benchmark NCRPG\n            bm_NCRPG = @benchmark proximal_gradient_method($OB, $f, $g, $grad_g, $start;\n                prox_nonsmooth = $prox_norm1_NCRPG,\n                stepsize = ConstantLength($step_size),\n                stopping_criterion = StopAfterIteration($max_iters)| StopWhenGradientMappingNormLess($stop_NCRPG)\n            )\n            # NCRPG with backtracking\n            rec_NCRPG_bt = proximal_gradient_method(OB, f, g, grad_g, start;\n                prox_nonsmooth = prox_norm1_NCRPG,\n                stepsize = ProximalGradientMethodBacktracking(;\n                    strategy = :nonconvex,\n                    initial_stepsize = init_step_size_bt,\n                    stop_when_stepsize_less = stop_step_size_bt\n                ),\n                record = [:Iteration, :Iterate],\n                return_state = true,\n                stopping_criterion = StopAfterIteration(max_iters)| StopWhenGradientMappingNormLess(stop_NCRPG_bt)\n            )\n            # Benchmark NCRPG with backtracking\n            bm_NCRPG_bt = @benchmark proximal_gradient_method($OB, $f, $g, $grad_g, $start;\n                prox_nonsmooth = $prox_norm1_NCRPG,\n                stepsize = ProximalGradientMethodBacktracking(;\n                    strategy = :nonconvex,\n                    initial_stepsize = $init_step_size_bt,\n                    stop_when_stepsize_less = $stop_step_size_bt\n                ),\n                stopping_criterion = StopAfterIteration($max_iters)| StopWhenGradientMappingNormLess($stop_NCRPG_bt)\n            )\n            # RPG\n            Iterates_RPG, it_RPG = RPG_SPCA_OB(OB, H, D, μ, L, start, RPG_prox_OB;\n                max_iters = max_iters,\n                stop = stop_RPG,\n                record = true\n            )\n            bm_RPG = @benchmark RPG_SPCA_OB($OB, $H, $D, $μ, $L, $start, $RPG_prox_OB;\n                max_iters = $max_iters,\n                stop = $stop_RPG\n            )\n            #\n            # Collect test results\n            Iterates_NCRPG  = get_record(rec_NCRPG, :Iteration, :Iterate)\n            res_NCRPG       = Iterates_NCRPG[end]\n            time_NCRPG      = time(median(bm_NCRPG))/1e9\n            obj_NCRPG       = f(OB, res_NCRPG)\n            spar_NCRPG      = sum(abs.(res_NCRPG).< 1e-8)/n/p\n            it_NCRPG        = length(Iterates_NCRPG)\n            orth_NCRPG      = norm(res_NCRPG'*res_NCRPG - I(p))\n            # NCRPG with backtracking\n            Iterates_NCRPG_bt  = get_record(rec_NCRPG_bt, :Iteration, :Iterate)\n            res_NCRPG_bt       = Iterates_NCRPG_bt[end]\n            time_NCRPG_bt      = time(median(bm_NCRPG_bt))/1e9\n            obj_NCRPG_bt       = f(OB, res_NCRPG_bt)\n            spar_NCRPG_bt      = sum(abs.(res_NCRPG_bt).< 1e-8)/n/p\n            it_NCRPG_bt        = length(Iterates_NCRPG_bt)\n            orth_NCRPG_bt      = norm(res_NCRPG_bt'*res_NCRPG_bt - I(p))\n            # RPG\n            res_RPG         = Iterates_RPG[end]\n            time_RPG        = time(median(bm_RPG))/1e9\n            obj_RPG         = f(OB, res_RPG)\n            spar_RPG        = sum(abs.(res_RPG).< 1e-8)/n/p\n            orth_RPG        = norm(res_RPG'*res_RPG - I(p))\n            #\n            # Update results\n            # Time values\n            time_NCRPG_tmp[c]      += time_NCRPG\n            time_NCRPG_bt_tmp[c]   += time_NCRPG_bt\n            time_RPG_tmp[c]        += time_RPG\n            # Objective values\n            obj_NCRPG_tmp[c]       += obj_NCRPG\n            obj_NCRPG_bt_tmp[c]    += obj_NCRPG_bt\n            obj_RPG_tmp[c]         += obj_RPG\n            # Sparsity values\n            spar_NCRPG_tmp[c]      += spar_NCRPG\n            spar_NCRPG_bt_tmp[c]   += spar_NCRPG_bt\n            spar_RPG_tmp[c]        += spar_RPG\n            # Orthogonality values\n            orth_NCRPG_tmp[c]      += orth_NCRPG\n            orth_NCRPG_bt_tmp[c]   += orth_NCRPG_bt\n            orth_RPG_tmp[c]        += orth_RPG\n            # Iteration values\n            it_NCRPG_tmp[c]        += it_NCRPG\n            it_NCRPG_bt_tmp[c]     += it_NCRPG_bt\n            it_RPG_tmp[c]          += it_RPG\n        end\n    end\n    for (c, μ) in enumerate(μs)\n        push!(df_results_RPG,\n            [μ, n, p, time_RPG_tmp[c]/m_tests, obj_RPG_tmp[c]/m_tests, spar_RPG_tmp[c]/m_tests, it_RPG_tmp[c]/m_tests, orth_RPG_tmp[c]/m_tests]\n        )\n        push!(df_results_NCRPG,\n            [μ, n, p, time_NCRPG_tmp[c]/m_tests, obj_NCRPG_tmp[c]/m_tests, spar_NCRPG_tmp[c]/m_tests, it_NCRPG_tmp[c]/m_tests, orth_NCRPG_tmp[c]/m_tests]\n        )\n        push!(df_results_NCRPG_bt,\n            [μ, n, p, time_NCRPG_bt_tmp[c]/m_tests, obj_NCRPG_bt_tmp[c]/m_tests, spar_NCRPG_bt_tmp[c]/m_tests, it_NCRPG_bt_tmp[c]/m_tests, orth_NCRPG_bt_tmp[c]/m_tests]\n        )\n    end\n    #\n    # Reset data collection variables\n    time_RPG_tmp      .= zeros(length(μs))\n    time_NCRPG_tmp    .= zeros(length(μs))\n    time_NCRPG_bt_tmp .= zeros(length(μs))\n    obj_RPG_tmp       .= zeros(length(μs))\n    obj_NCRPG_tmp     .= zeros(length(μs))\n    obj_NCRPG_bt_tmp  .= zeros(length(μs))\n    spar_RPG_tmp      .= zeros(length(μs))\n    spar_NCRPG_tmp    .= zeros(length(μs))\n    spar_NCRPG_bt_tmp .= zeros(length(μs))\n    it_RPG_tmp        .= zeros(length(μs))\n    it_NCRPG_tmp      .= zeros(length(μs))\n    it_NCRPG_bt_tmp   .= zeros(length(μs))\n    orth_RPG_tmp      .= zeros(length(μs))\n    orth_NCRPG_tmp    .= zeros(length(μs))\n    orth_NCRPG_bt_tmp .= zeros(length(μs))\nend\n\nWe export the results to CSV files\n\n# Sort the dataframes by the parameter μ and create the final results dataframes\ndf_results_NCRPG = sort(df_results_NCRPG, :μ)\ndf_results_NCRPG_bt = sort(df_results_NCRPG_bt, :μ)\ndf_results_RPG = sort(df_results_RPG, :μ)\ndf_results_time_iter = DataFrame(\n    μ             = df_results_NCRPG.μ,\n    n             = Int.(df_results_NCRPG.n),\n    p             = Int.(df_results_NCRPG.p),\n    NCRPG_time     = df_results_NCRPG.time,\n    NCRPG_iter     = Int.(round.(df_results_NCRPG.iterations, digits = 0)),\n    NCRPG_bt_time  = df_results_NCRPG_bt.time,\n    NCRPG_bt_iter  = Int.(round.(df_results_NCRPG_bt.iterations, digits = 0)),\n    RPG_time     = df_results_RPG.time,\n    RPG_iter     = Int.(round.(df_results_RPG.iterations, digits = 0)),\n)\ndf_results_obj_spar_orth = DataFrame(\n    μ               = df_results_NCRPG.μ,\n    n               = Int.(df_results_NCRPG.n),\n    p               = Int.(df_results_NCRPG.p),\n    NCRPG_obj       = df_results_NCRPG.objective,\n    NCRPG_sparsity  = df_results_NCRPG.sparsity,\n    NCRPG_orth      = df_results_NCRPG.orthogonality,\n    NCRPG_bt_obj    = df_results_NCRPG_bt.objective,\n    NCRPG_bt_sparsity = df_results_NCRPG_bt.sparsity,\n    NCRPG_bt_orth   = df_results_NCRPG_bt.orthogonality,\n    RPG_obj         = df_results_RPG.objective,\n    RPG_sparsity    = df_results_RPG.sparsity,\n    RPG_orth        = df_results_RPG.orthogonality,\n)\n# Write the results to CSV files\nCSV.write(joinpath(results_folder, \"results-OB-time-iter-$(m_tests).csv\"), df_results_time_iter)\nCSV.write(joinpath(results_folder, \"results-OB-obj-spar-orth-$(m_tests).csv\"), df_results_obj_spar_orth)\n\nWe can take a look at how the algorithms compare to each other in their performance with the following tables. First, we look at the time and number of iterations for each algorithm.\n\nμ n p NCRPGconsttime NCRPGconstiter NCRPGbttime NCRPGbtiter RPG_time RPG_iter\n0.1 100 5 0.557614 30611 0.414584 4315 0.881888 30607\n0.1 200 5 1.32795 31701 0.62706 2904 2.07484 31702\n0.1 300 5 3.46168 43584 1.59039 3888 5.06904 43589\n0.5 100 5 0.173558 8721 0.0750915 774 0.249546 8723\n0.5 200 5 0.647338 14245 0.256883 935 0.937679 14253\n0.5 300 5 1.58775 18910 0.717006 1327 2.17608 18928\n1.0 100 5 0.173952 8988 0.120466 940 0.253129 8958\n1.0 200 5 0.277949 6572 0.194787 510 0.420091 6583\n1.0 300 5 0.0404357 500 0.00780451 26 0.0633285 500\n\nSecond, we look at the objective values, sparsity, and orthogonality of the solutions found by each algorithm.\n\nμ n p NCRPGconstobj NCRPGconstspar NCRPGconstorth NCRPGbtobj NCRPGbtspar NCRPGbtorth RPG_obj RPG_spar RPG_orth\n0.1 100 5 3.22671 0.4668 0.161146 3.22624 0.4662 0.167034 3.22671 0.4668 0.161146\n0.1 200 5 4.3604 0.5253 0.115807 4.38201 0.5265 0.116072 4.3604 0.5253 0.115807\n0.1 300 5 5.22596 0.5534 0.0950749 5.22159 0.553867 0.0956675 5.22596 0.5534 0.0950749\n0.5 100 5 13.0358 0.7348 0.129335 13.1202 0.7326 0.127941 13.0358 0.7348 0.129335\n0.5 200 5 16.7412 0.8128 0.0864384 16.7758 0.8136 0.0813886 16.7412 0.8128 0.0864384\n0.5 300 5 19.1849 0.874333 0.0720433 19.2195 0.873733 0.0657896 19.1849 0.874333 0.0720433\n1.0 100 5 22.031 0.8754 0.0519277 22.1683 0.8828 0.0416069 22.031 0.8754 0.0519277\n1.0 200 5 25.4411 0.9791 0.0407826 25.5277 0.9852 0.0395372 25.4411 0.9791 0.0407826\n1.0 300 5 24.8276 0.996667 0.0 24.8266 0.996667 0.0 24.8276 0.996667 0.0","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#Technical-details","page":"Sparse PCA","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nusing Pkg\nPkg.status()\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 19, 2026, 12:52:17.","category":"section"},{"location":"examples/NCRPG-Sparse-PCA/#Literature","page":"Sparse PCA","title":"Literature","text":"R. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Nononvex Optimization, preprint (2025), arXiv:2506.09775.\n\n\n\nW. Huang and K. Wei. Riemannian proximal gradient methods. Mathematical Programming 194, 371–413 (2021).\n\n\n\n","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#The-Constrained-mean-on-high-dimensional-Hyperbolic-space.","page":"Mean on mathbb H^n","title":"The Constrained mean on high-dimensional Hyperbolic space.","text":"Hajg Jasa, Ronny Bergmann 2026-04-06","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#Introduction","page":"Mean on mathbb H^n","title":"Introduction","text":"This example is to be thought of as a continuation of the Constrained Mean on Hyperbolic Space, where we compare the Intrinsic Convex Riemannian Proximal Gradient Method (CRPG) from [BJJP25a] with the Projected Gradient Algorithm (PGA) as introduced in [BFNZ25]. For CRPG, we test performances of both constant and backtracked stepsize strategies.\n\nusing Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random\nimport ColorSchemes.tol_vibrant\n\nConsider the constrained Riemannian center of mass for a given set of points ``q_i M$ i=1ldotsN given by\n\noperatorname*argmin_pinmathcal C\nsum_i=1^N d_mathrmM^2(pq_i)\n\nconstrained to a set mathcal C subset mathcal M.\n\nThe same problem can be formulated as an unconstrained optimization problem by introducing the characteristic function for the set mathcal C:\n\noperatorname*argmin_pinmathcal M\nsum_i=1^N d_mathrmM^2(pq_i) + chi_mathcal C(p)\n\nwhere chi_mathcal C(p) = 0 if p in mathcal C and chi_mathcal C(p) = infty otherwise. This formulation allows us to use CRPG to solve the problem.\n\nFor this experiment set mathcal M = mathbb H^d for d=2ldots200, the Hyperbolic space and the constrained set mathcal C = C_cr as the ball of radius r around the center point c, where we choose here r=frac1sqrtn and c = (0ldots01)^mathrmT and a σ = frac32n^14.\n\nn_range = Vector(2:200)\nradius_range = [1 / sqrt(n) for n in n_range]\nN_range = [400 for n ∈ n_range]\nM_range = [Hyperbolic(n) for n ∈ n_range]\nσ_range = [ 1.5/sqrt(sqrt(n-1)) for n ∈ n_range]\ntol = 1e-7\n\nThe data consists of N=200 points, where we skew the data a bit to force the mean to be outside of the constrained set mathcal C.","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#Cost,-gradient-and-projection","page":"Mean on mathbb H^n","title":"Cost, gradient and projection","text":"We can formulate the constrained problem above in two different forms. Both share a cost and require a gradient. For performance reasons, we also provide a mutating variant of the gradient\n\nf(M, p; pts=[]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts)\n\ngrad_f(M, p; pts=[]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts)\n\nfunction grad_f!(M, X, p; pts=[])\n    zero_vector!(M, X, p)\n    Y = zero_vector(M, p)\n    for q in pts\n        log!(M, Y, p, q)\n        X .+= Y\n    end\n    X .*= -1 / length(pts)\n    return X\nend\n\nWe can model the constraint either with an inequality constraint g(p) geq 0 or using a projection onto the set. For the gradient of g and the projection we again also provide mutating variants. Lastly, we define the cost function F as the sum of the original cost and the characteristic function for the set mathcal C.\n\ng(M, p; op=[], radius=1) = distance(M, op, p)^2 - radius^2;\n# The characteristic function for the set C is defined with tol^2 to avoid numerical issues\ncharacteristic_C(M, p; op=[], radius=1) = (g(M, p; op=op, radius=radius) ≤ tol^2) ? 0 : Inf;\n\nfunction project_C(M, p; op=[], radius=1)\n    X = log(M, op, p)\n    n = norm(M, op, X)\n    q = (n > radius) ? exp(M, op, (radius / n) * X) : copy(M, p)\n    return q\nend;\n\nfunction project_C!(M, q, p; radius=1, op=[], X=zero_vector(M, op))\n    log!(M, X, op, p)\n    n = norm(M, op, X)\n    if (n > radius)\n        exp!(M, q, op, (radius / n) * X)\n    else\n        copyto!(M, q, p)\n    end\n    return q\nend;\n\ngrad_g(M, p; op=[]) = -2 * log(M, p, op)\nfunction grad_g!(M, X, p; op=[])\n    log!(M, X, p, op)\n    X .*= -2\n    return X\nend\n\nF(M, p; pts=[], radius=1, op=[]) = f(M, p; pts=pts) + characteristic_C(M, p; op=op, radius=radius)","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#The-mean","page":"Mean on mathbb H^n","title":"The mean","text":"For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to mathcal C. We can then project this onto mathcal C. For the projected mean we obtain g(p) = 0 since the original mean is outside of the set, the projected one lies on the boundary.\n\nWe first generate all data\n\ncenters = [[zeros(n)..., 1.0] for n in n_range]\nbegin\n    Random.seed!(5)\n    data = [\n        [\n            exp(\n                M,\n                c,\n                get_vector(\n                    M, c, σ * randn(n) .+ 2 * r .* ones(n), DefaultOrthonormalBasis()\n                ),\n            ) for _ in 1:N\n        ] for\n        (c, r, n, N, M, σ) in zip(centers, radius_range, n_range, N_range, M_range, σ_range)\n    ]\nend\n\nmeans = [mean(M, d) for (M, d) in zip(M_range, data)]\ndc = [\n    characteristic_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\nminimum(dc) # Sanity Check, this should be inf\n\nInf\n\nProj_means = [\n    project_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\n# Samll sanity check, these should all be about zero\nds = [distance(M, m, c) - r for (M, m, c, r) in zip(M_range, Proj_means, centers, radius_range)]\nmaximum(abs.(ds))\n\n1.1102230246251565e-16","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#The-experiment","page":"Mean on mathbb H^n","title":"The experiment","text":"First, we define a single test function for one set of data for a manifold\n\nfunction bench_aep(Manifold, center, radius, data)\n    # local functions\n    _f(M, p) = f(M, p; pts=data)\n    _grad_f!(M, X, p) = grad_f!(M, X, p; pts=data)\n    _proj_C!(M, q, p) = project_C!(M, q, p; radius=radius, op=center)\n    _F(M, p) = F(M, p; pts=data, radius=radius, op=center)\n    _prox_I!(M, q, λ, p) = _proj_C!(M, q, p)\n    # Copmute the Lipschitz constant of the gradient of f for the stepsize\n    D = 2 * maximum([distance(Manifold, center, pt) for pt in data])\n    L_f = Manopt.ζ_1(-1, D)\n    constant_stepsize = 1 / L_f\n    initial_stepsize = constant_stepsize\n    contraction_factor = 0.9\n    warm_start_factor = 10.0\n    #\n    # returns\n    stats = Dict(:CRPG_CN => Dict(), :CRPG_BT => Dict(), :PGA => Dict())\n    #\n    mean_crpg_cn = copy(Manifold, center)\n    crpg_cn = proximal_gradient_method!(\n        Manifold, \n        _F, \n        _f, \n        _grad_f!, \n        mean_crpg_cn;\n        prox_nonsmooth=_prox_I!,\n        evaluation=InplaceEvaluation(), return_state=true,\n        record=[:Iteration, :Cost],\n        stepsize=ConstantLength(\n            constant_stepsize,\n        ),\n        stopping_criterion=StopWhenGradientMappingNormLess(tol)|StopAfterIteration(5000),\n    )\n    stats[:CRPG_CN][:Iter] = length(get_record(crpg_cn, :Iteration))\n    stats[:CRPG_CN][:Cost] = get_record(crpg_cn)\n    # \n    # Backtracked stepsize\n    mean_crpg_bt = copy(Manifold, center)\n    crpg_bt = proximal_gradient_method!(\n        Manifold, \n        _F, \n        _f, \n        _grad_f!, \n        mean_crpg_bt;\n        prox_nonsmooth=_prox_I!,\n        evaluation=InplaceEvaluation(), return_state=true,\n        record=[:Iteration, :Cost],\n        stepsize=ProximalGradientMethodBacktracking(; \n            contraction_factor=contraction_factor,\n            initial_stepsize=initial_stepsize,\n            stop_when_stepsize_less=tol,\n            strategy=:convex,   \n            warm_start_factor=warm_start_factor,\n        ),\n        stopping_criterion=StopWhenGradientMappingNormLess(tol)|StopAfterIteration(5000),\n    )\n    stats[:CRPG_BT][:Iter] = length(get_record(crpg_bt, :Iteration)) \n    stats[:CRPG_BT][:Cost] = get_record(crpg_bt)\n    # \n    mean_pga = copy(Manifold, center)\n    pgas = projected_gradient_method!(\n        Manifold,\n        _f,\n        _grad_f!,\n        _proj_C!,\n        mean_pga;\n        evaluation=InplaceEvaluation(),\n        record=[:Iteration, :Cost],\n        stopping_criterion=StopAfterIteration(150) |\n                           StopWhenProjectedGradientStationary(Manifold, tol),\n        return_state=true,\n    )\n    stats[:PGA][:Iter] = length(get_record(pgas, :Iteration))\n    stats[:PGA][:Cost] = get_record(pgas)\n    #\n    #\n    # Benchmarks\n    crpg_b_cn = @be proximal_gradient_method!($Manifold, $_F, $_f, $_grad_f!,\n        $(copy(Manifold, center)); prox_nonsmooth=$_prox_I!, evaluation=$(InplaceEvaluation()),\n        stepsize=$(ConstantLength(\n            constant_stepsize,\n        )),\n        stopping_criterion=$(StopWhenGradientMappingNormLess(tol)|StopAfterIteration(5000)),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:CRPG_CN][:time] = mean(crpg_b_cn).time\n    crpg_b_bt = @be proximal_gradient_method!($Manifold, $_F, $_f, $_grad_f!,\n        $(copy(Manifold, center)); prox_nonsmooth=$_prox_I!, evaluation=$(InplaceEvaluation()),\n        stepsize=$(ProximalGradientMethodBacktracking(; \n            strategy=:convex,   \n            initial_stepsize=initial_stepsize,\n            stop_when_stepsize_less=tol,\n            contraction_factor=contraction_factor,\n        )),\n        stopping_criterion=$(StopWhenGradientMappingNormLess(tol)|StopAfterIteration(5000)),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:CRPG_BT][:time] = mean(crpg_b_bt).time\n    # \n    pga_b = @be projected_gradient_method!($Manifold, $_f, $_grad_f!, $_proj_C!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        stopping_criterion=$(\n            StopAfterIteration(150) | StopWhenProjectedGradientStationary(Manifold, tol)\n        ),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:PGA][:time] = mean(pga_b).time\n    return stats\nend\n\nbench_aep (generic function with 1 method)\n\nand run these\n\nThe resulting plot of runtime is\n\nfig = Figure()\naxis = Axis(fig[1, 1]; title=L\"\\text{Time needed per dimension }$\\mathbb{H}^d$\")\nlines!(axis, n_range, [bi[:CRPG_CN][:time] for bi in b]; label=\"CRPG, constant step\", color=tol_vibrant[1],)\nlines!(axis, n_range, [bi[:CRPG_BT][:time] for bi in b]; label=\"CRPG, backtracked step\", color=tol_vibrant[5],)\nlines!(axis, n_range, [bi[:PGA][:time] for bi in b]; label=\"PGA\", color=tol_vibrant[2],)\naxis.xlabel = \"Manifold dimension d\"\naxis.ylabel = \"runtime (sec.)\"\naxislegend(axis; position=:lt)\nfig\n\n<img src=\"CRPG-Constrained-Mean-Hn_files/figure-commonmark/cell-13-output-1.png\" width=\"672\" height=\"480\" />\n\nand the number of iterations reads\n\nfig2 = Figure()\naxis2 = Axis(fig2[1, 1]; title=L\"\\text{Iterations needed per dimension }$\\mathbb{H}^d$\")\nlines!(axis2, n_range, [bi[:CRPG_CN][:Iter] for bi in b]; label=\"CRPG constant step\", color=tol_vibrant[1])\nlines!(axis2, n_range, [bi[:CRPG_BT][:Iter] for bi in b]; label=\"CRPG backtracked step\", color=tol_vibrant[5],)\nlines!(axis2, n_range, [bi[:PGA][:Iter] for bi in b]; label=\"PGA\", color=tol_vibrant[2],)\naxis2.xlabel = \"Manifold dimension d\"\naxis2.ylabel = \"# Iterations\"\naxislegend(axis2; position=:rt)\nfig2\n\n<img src=\"CRPG-Constrained-Mean-Hn_files/figure-commonmark/cell-15-output-1.png\" width=\"672\" height=\"480\" />","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#Literature","page":"Mean on mathbb H^n","title":"Literature","text":"R. Bergmann, O. P. Ferreira, S. Z. Németh and J. Zhu. On projection mappings and the gradient projection method                on hyperbolic space forms, arXiv preprint (2025).\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization, preprint (2025), arXiv:2507.16055.\n\n\n\n","category":"section"},{"location":"examples/CRPG-Constrained-Mean-Hn/#Technical-details","page":"Mean on mathbb H^n","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 15, 2025, 13:36:39.","category":"section"},{"location":"examples/Elastic-Geodesic-under-forcefield/#Elastic-Geodesic-under-Force-Field-on-the-Sphere","page":"Elastic Geodesic under force field","title":"Elastic Geodesic under Force Field on the Sphere","text":"Laura Weigl, Ronny Bergmann, and Anton Schiela 2025-11-25\n\nusing LinearAlgebra, SparseArrays, OffsetArrays\nusing Manifolds, Manopt, ManoptExamples\nusing GLMakie, Makie, GeometryTypes, Colors, ColorSchemes, NamedColors\nusing CSV, DataFrames","category":"section"},{"location":"examples/Elastic-Geodesic-under-forcefield/#Introduction","page":"Elastic Geodesic under force field","title":"Introduction","text":"In this example we compute an elastic geodesic under a force field on the sphere by applying Newton’s method on vector bundles which was introduced in [WBS25]. This example reproduces the results from the example in Section 6.1 therein.\n\nWe consider the sphere mathbbS^2 equipped with the Riemannian metric         given by the Euclidean inner product with corresponding norm lVert  rVert and an interval I = 0T  mathbb R. Let mathcal X = H^1(I mathbb S^2) and mathcal E^* = T^*mathcal X its cotangent bundle.\n\nOur goal is to find a zero of the mapping F mathcal X  mathcal E^* with\n\nF(γ)ϕ = int_I dotγ(t) dotϕ(t) + ω(γ(t))ϕ(t)  mathrmdt\n\nfor γ  mathcal X and ϕ  T_γmathcal X.\n\nAdditionally, we have to take into account that boundary conditions γ(0) = γ_0 and γ(T) = γ_T for given γ_0 γ_T  mathbb S^2 are satisfied. This yields an elastic geodesic under a given force field ω mathbb S^2  T^*mathbb S^2 connecting γ_0 and γ_T.\n\nFor our example we set\n\n    N = 50\n    S = Manifolds.Sphere(2)\n    power = PowerManifold(S, NestedPowerRepresentation(), N)\n\n    mutable struct VariationalSpace\n        manifold::AbstractManifold\n        degree::Integer\n    end\n\n    test_space = VariationalSpace(S, 1)\n\n    start_interval = 0.4\n    end_interval = π - 0.4\n    discrete_time = range(; start = start_interval, stop = end_interval, length=N+2) # equidistant discrete time points\n\n    y0 = [sin(start_interval),0,cos(start_interval)] # startpoint of geodesic\n    yT = [sin(end_interval),0,cos(end_interval)] # endpoint of geodesic\n\n3-element Vector{Float64}:\n  0.3894183423086505\n  0.0\n -0.9210609940028851\n\nAs a starting point, we use the geodesic connecting γ_0 and γ_T:\n\ny(t) =  [sin(t), 0, cos(t)]\ndiscretized_y = [y(ti) for ti in discrete_time[2:end-1]]\n\n50-element Vector{Vector{Float64}}:\n [0.4312823089035602, 0.0, 0.9022170304460086]\n [0.47223726752861295, 0.0, 0.8814714760881995]\n [0.5121968979637886, 0.0, 0.8588680560576649]\n [0.5510769778290171, 0.0, 0.8344544112813096]\n [0.5887955600985645, 0.0, 0.8082819980725896]\n [0.6252731458196872, 0.0, 0.7804059796777266]\n [0.6604328516715138, 0.0, 0.7508851100088696]\n [0.6942005720109947, 0.0, 0.7197816098092585]\n [0.7265051350643781, 0.0, 0.6871610355113928]\n [0.7572784529350077, 0.0, 0.6530921410646137]\n ⋮\n [0.726505135064378, 0.0, -0.6871610355113931]\n [0.6942005720109946, 0.0, -0.7197816098092586]\n [0.6604328516715138, 0.0, -0.7508851100088696]\n [0.6252731458196874, 0.0, -0.7804059796777264]\n [0.5887955600985644, 0.0, -0.8082819980725897]\n [0.5510769778290171, 0.0, -0.8344544112813096]\n [0.5121968979637888, 0.0, -0.8588680560576648]\n [0.4722372675286131, 0.0, -0.8814714760881994]\n [0.43128230890356006, 0.0, -0.9022170304460086]\n\nIn order to apply Newton’s method to find a zero of F, we need the linear mapping Q_F(γ)^* F(γ) [WBS25] which can be seen as a covariant derivative. Since the sphere is an embedded submanifold of mathbb R^3, we can use the formula\n\nQ_F(γ)^* F(γ)δ γϕ = F(γ)(oversetV_γ(γ)δ γϕ) + F_mathbb R^3(γ)δ γϕ\n\nfor δ γ  ϕ  T_γ mathcal X, where oversetV_γ(hat γ)  L(T_γ mathcal X T_hatγmathcal X) is a vector transport and F_mathbb R^3(γ)δ γ ϕ = int_I  dotδ γ(t)dotϕ(t) + ω(γ(t))δ γ(t)ϕ(t)  mathrmdt is the euclidean derivative of F.\n\nWe define a structure that has to be filled for two purposes:\n\nDefinition of an integrand and its derivative\nDefinition of a vector transport and its derivative\n\nmutable struct DifferentiableMapping{F1<:Function,F2<:Function,T}\n    value::F1\n    derivative::F2\n    scaling::T\nend;\n\nThe following routines define a vector transport and its euclidean derivative. As seen above, they are needed to derive a covariant derivative of F.\n\nAs a vector transport we use the (pointwise) orthogonal projection onto the tangent spaces, i.e. for p q  mathbb S^2 and X  T_pmathbb S^2 we set\n\noversetV_p(q)X = (I-q   q^T)X  T_qmathbb S^2\n\nThe derivative of the vector transport is then given by\n\nleft(fracddqoversetV_p(q)bigvert_q=pδ qright)X = left( - δ q   p^T - p   δ q^Tright)   X\n\ntransport_by_proj(S, p, X, q) =  X - q*(q'*X)\n\ntransport_by_proj_prime(S, p, X, dq) = (- dq*p' - p*dq')*X\n\ntransport=DifferentiableMapping(transport_by_proj,transport_by_proj_prime,nothing)\n\nDifferentiableMapping{typeof(transport_by_proj), typeof(transport_by_proj_prime), Nothing}(transport_by_proj, transport_by_proj_prime, nothing)\n\nThe following two routines define the integrand of F and its euclidean derivative. They use a force field ω, which is defined, below. A scaling parameter for the force is also employed.\n\nIn this example we consider the force field ω mathbb S^2  T^*mathbb S^2 given by the 1-form corresponding to a (scaled) winding field, i.e. for Cmathbb R and y mathbbS^2 we set\n\nω(y) = fracC y_3y_1^2+y_2^2    bigg beginpmatrix -y_2  y_1  0 endpmatrix    bigg  (T_ymathbbS^2)^*\n\nw(p, c) = c*p[3]*[-p[2]/(p[1]^2+p[2]^2), p[1]/(p[1]^2+p[2]^2), 0.0];\n\nIts derivative is given by\n\nfunction w_prime(p, c)\n    denominator = p[1]^2+p[2]^2\n\n    return c*[p[3]*2*p[1]*p[2]/denominator^2 p[3]*(-1.0/denominator+2.0*p[2]^2/denominator^2) -p[2]/denominator; p[3]*(1.0/denominator-2.0*p[1]^2/(denominator^2)) p[3]*(-2.0*p[1]*p[2]/(denominator^2)) p[1]/denominator; 0.0 0.0 0.0]\nend;\n\nF_at(Integrand, y, ydot, B, Bdot) = ydot'*Bdot+w(y,Integrand.scaling)'*B\n\nF_prime_at(Integrand,y,ydot,B1,B1dot,B2,B2dot) = B1dot'*B2dot+(w_prime(y,Integrand.scaling)*B1)'*B2\n\nintegrand=DifferentiableMapping(F_at,F_prime_at,3.0)\n\nDifferentiableMapping{typeof(F_at), typeof(F_prime_at), Float64}(F_at, F_prime_at, 3.0)","category":"section"},{"location":"examples/Elastic-Geodesic-under-forcefield/#Newton-Equation","page":"Elastic Geodesic under force field","title":"Newton Equation","text":"In this example we implement a functor to compute the Newton matrix and the right hand side for the Newton equation [WBS25]\n\nQ^*_F(γ) F(γ)δ γ + F(γ) = 0^*_γ\n\nby using the assembler provided in ManoptExamples.jl.\n\nIt returns the matrix and the right hand side in base representation.\n\nThe assembly routines need a function for evaluation the iterates at the left and right quadrature point.\n\nevaluate(p, i, tloc) = (1.0-tloc)*p[i-1]+tloc*p[i];\n\nstruct NewtonEquation{F, TS, T, I, NM, Nrhs}\n    integrand::F\n    test_space::TS\n    transport::T\n    time_interval::I\n    A::NM\n    b::Nrhs\nend\n\nfunction NewtonEquation(M, F, test_space, VT, interval)\n    n = manifold_dimension(M)\n    A = spzeros(n,n)\n    b = zeros(n)\n    return NewtonEquation{typeof(F), typeof(test_space), typeof(VT), typeof(interval), typeof(A), typeof(b)}(F, test_space, VT, interval, A, b)\nend\n\nfunction (ne::NewtonEquation)(M, VB, p)\n    n = manifold_dimension(M)\n    ne.A .= spzeros(n,n)\n    ne.b .= zeros(n)\n\n    Op = OffsetArray([y0, p..., yT], 0:(length(p)+1))\n\n    ManoptExamples.get_jacobian!(M, Op, evaluate, ne.A, ne.integrand, ne.transport, ne.time_interval; test_space = ne.test_space)\n    ManoptExamples.get_right_hand_side!(M, Op, evaluate, ne.b, ne.integrand, ne.time_interval; test_space = ne.test_space)\nend\n\nWe compute the Newton direction δ γ by solving the linear system given by the base representation of the Newton equation directly and return the Newton direction in vector representation:\n\nfunction solve_in_basis_repr(problem, newtonstate)\n    X_base = (problem.newton_equation.A) \\ (-problem.newton_equation.b)\n    return get_vector(problem.manifold, newtonstate.p, X_base, DefaultOrthogonalBasis())\nend\n\nsolve_in_basis_repr (generic function with 1 method)\n\nWe adjust the norms for recording\n\n    rec = RecordChange(power;\n    inverse_retraction_method=ProjectionInverseRetraction());\n\nbegin\n    NE = NewtonEquation(power, integrand, test_space, transport, discrete_time)\n\n    st_res = vectorbundle_newton(power, TangentBundle(power), NE, discretized_y; sub_problem=solve_in_basis_repr,\n    stopping_criterion=( StopAfterIteration(150) | StopWhenChangeLess(power,1e-12; outer_norm=Inf, inverse_retraction_method=ProjectionInverseRetraction())),\n    retraction_method=ProjectionRetraction(),\n    debug=[:Iteration, (:Change, \"Change: %1.8e\"), \"\\n\", :Stop, (:Stepsize, \"Stepsize: %1.8e\"), \"\\n\",],\n    record=[:Iterate, rec => :Change],\n    return_state=true\n    )\nend\n\nInitial\n\n# 1     Change: 2.96686818e+00\nStepsize: 1.00000000e+00\n# 2     Change: 3.86526790e-01\nStepsize: 1.00000000e+00\n# 3     Change: 2.32737984e-02\nStepsize: 1.00000000e+00\n# 4     Change: 2.19379934e-04\nStepsize: 1.00000000e+00\n# 5     Change: 1.15340970e-08\nStepsize: 1.00000000e+00\n# 6     Change: 6.63720131e-14\nStepsize: 1.00000000e+00\nAt iteration 6 the algorithm performed a step with a change (1.255273380563256e-14) less than 1.0e-12.\n\n# Solver state for `Manopt.jl`s Vector bundle Newton method\nAfter 6 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ProjectionRetraction()\n* step size: ConstantLength(1.0; type=:relative)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 150:  not reached\n  * |Δp| < 1.0e-12: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [ (:Iteration, \"# %-6d\"), (:Change, \"Change: %1.8e\"), \"\\n\", (:Stepsize, \"Stepsize: %1.8e\"), \"\\n\" ]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIterate(Vector{Vector{Float64}}), RecordChange(; inverse_retraction_method=ManifoldsBase.ProjectionInverseRetraction())]),)\n\nWe extract the recorded values\n\n    change = get_record(st_res, :Iteration, :Change)[2:end]\n    p_res = get_solver_result(st_res)\n\n50-element Vector{Vector{Float64}}:\n [0.3862072810538758, -0.09533087331847834, 0.9174725939521621]\n [0.3834594005073149, -0.17575240047635604, 0.9066807497070671]\n [0.38420034508283774, -0.24281962050178213, 0.8907461629092762]\n [0.38997618444778864, -0.2984067370816809, 0.8711325931382714]\n [0.40135535377804027, -0.34415448366457074, 0.8488059680323159]\n [0.4183423247899146, -0.3813361456953533, 0.8243618400166167]\n [0.44062875023728093, -0.41088642206997666, 0.7981344827927591]\n [0.4677303732599358, -0.43347156157671524, 0.7702796266514941]\n [0.49905909206373583, -0.449559252230437, 0.7408350027924774]\n [0.5339610863554352, -0.4594777974939836, 0.7097645467816784]\n ⋮\n [0.49905909206373367, 0.4495592522304387, -0.7408350027924779]\n [0.4677303732599338, 0.43347156157671657, -0.7702796266514944]\n [0.44062875023727915, 0.41088642206997783, -0.7981344827927593]\n [0.4183423247899131, 0.38133614569535446, -0.8243618400166169]\n [0.4013553537780388, 0.3441544836645715, -0.8488059680323162]\n [0.3899761844477874, 0.29840673708168164, -0.8711325931382716]\n [0.38420034508283685, 0.24281962050178285, -0.8907461629092766]\n [0.3834594005073143, 0.17575240047635673, -0.9066807497070672]\n [0.38620728105387553, 0.09533087331847828, -0.9174725939521623]\n\nand plot the result, where we measure the norms of the Newton direction in each iteration,\n\nf = Figure(;)\nrow, col = fldmod1(1, 2)\nAxis(f[row, col], yscale = log10, title = string(\"Norms of the Newton directions (semilogarithmic)\"), xminorgridvisible = true, xticks = (1:length(change)), xlabel = \"Iteration\", ylabel = \"‖δ γ‖\")\nscatterlines!(change[1:end], color = :blue)\nf\n\n(Image: )\n\nand the resulting elastic geodesic under the force field (orange). The starting geodesic (blue) is plotted as well. The force acting on each point of the geodesic is visualized by green arrows.\n\nn = 25\nu = range(0; stop=2 * π, length=n);\nv = range(0; stop=π, length=n);\nsx = zeros(n, n);\nsy = zeros(n, n);\nsz = zeros(n, n);\n\nws = [-w(p, integrand.scaling) for p in p_res]\nws_start = [-w(p, integrand.scaling) for p in discretized_y]\nfor i in 1:n\n    for j in 1:n\n        sx[i, j] = cos.(u[i]) * sin(v[j])\n        sy[i, j] = sin.(u[i]) * sin(v[j])\n        sz[i, j] = cos(v[j])\n    end\nend\nfig, ax, plt = meshscatter(\n    sx, sy, sz; color=RGBA(1.0, 1.0, 1.0, 0.0), shading=Makie.automatic, transparency=true\n)\ngeodesic_start = [y0, discretized_y..., yT]\ngeodesic_final = [y0, p_res..., yT]\nax.show_axis = false\nwireframe!(ax, sx, sy, sz; color=RGBA(0.5, 0.5, 0.7, 0.1), transparency=true)\nπ1(x) = 1.02 * x[1]\nπ2(x) = 1.02 * x[2]\nπ3(x) = 1.02 * x[3]\nscatterlines!(\n    ax, π1.(geodesic_final), π2.(geodesic_final), π3.(geodesic_final);\n    markersize=5, color=:orange, linewidth=2,\n)\nscatterlines!(\n    ax, π1.(geodesic_start), π2.(geodesic_start), π3.(geodesic_start);\n    markersize=5, color=:blue, linewidth=2,\n)\nscatter!(ax, π1.([y0, yT]), π2.([y0, yT]), π3.([y0, yT]); markersize=5, color=:red)\narrows!(\n    ax, π1.(p_res), π2.(p_res), π3.(p_res), π1.(ws), π2.(ws), π3.(ws);\n    color=:green, linewidth=0.01,\n    arrowsize=Vec3f(0.03, 0.03, 0.05), transparency=true, lengthscale=0.07,\n)\ncam = cameracontrols(ax.scene); cam.lookat[] = [-2.5, 2.5, 2]\nfig\n\n(Image: )","category":"section"},{"location":"examples/Elastic-Geodesic-under-forcefield/#Technical-details","page":"Elastic Geodesic under force field","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n⌃ [13f3f980] CairoMakie v0.15.7\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.122\n⌃ [e9467ef8] GLMakie v0.13.7\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.33.0\n  [682c06a0] JSON v1.3.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n⌃ [d3d80556] LineSearches v7.4.1\n⌅ [ee78f7c6] Makie v0.24.7\n  [af67fdf4] ManifoldDiff v0.4.5\n⌃ [1cead3c2] Manifolds v0.11.6\n⌃ [3362f125] ManifoldsBase v2.2.1\n⌃ [0fc0a36d] Manopt v0.5.28\n  [5b8d5e80] ManoptExamples v0.1.17 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.2\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.30\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.39.0\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered December 14, 2025, 11:26:10.","category":"section"},{"location":"examples/Elastic-Geodesic-under-forcefield/#Literature","page":"Elastic Geodesic under force field","title":"Literature","text":"L. Weigl, R. Bergmann and A. Schiela. Newton's method into vector bundles Part II: : Application to Variational Problems on Manifolds, arXiv Preprint (2025).\n\n\n\n","category":"section"},{"location":"examples/Elastic-Geodesic-Obstacle/#Elastic-Geodesic-on-Sphere-with-Obstacle-Avoidance","page":"Elastic Geodesic Obstacle","title":"Elastic Geodesic on Sphere with Obstacle Avoidance","text":"Laura Weigl, Ronny Bergmann, and Anton Schiela 2025-11-25\n\nusing LinearAlgebra, SparseArrays, OffsetArrays\nusing Manifolds, Manopt, ManoptExamples\nusing GLMakie, Makie, GeometryTypes, Colors, ColorSchemes, NamedColors\nusing CSV, DataFrames","category":"section"},{"location":"examples/Elastic-Geodesic-Obstacle/#Introduction","page":"Elastic Geodesic Obstacle","title":"Introduction","text":"In this example we compute an elastic geodesic on the sphere avoiding an obstacle by applying Newton’s method on vector bundles which was introduced in [WBS25]. This example reproduces the results from the example in Section 6.2 therein.\n\nWe consider the sphere mathbbS^2 equipped with the Riemannian metric langle cdot cdot rangle given by the Euclidean inner product with corresponding norm lVertcdotrVert and a time interval I =0T.\n\nLet mathcal X = H^1(I mathbb S^2) and mathcal E^* = T^*mathcal X its cotangent bundle.\n\nConsider the minimization problem\n\nmin_γ  H^1(I mathbb S^2)  frac12 int_0^T lVertdot γ(t)rVert^2  mathrmdt\n\nunder the constraint that γ_3(t)  1-h_mathrmref  text for all  t 0T where γ_3(t) denotes the third component of γ(t) mathbbS^2 and h_mathrmref  (01) is a given height. Additionally, we have to take into account that boundary conditions γ(0) = γ_0 and γ(T) = γ_T for given γ_0 γ_T  mathbb S^2 are satisfied.\n\nUsing a penalty method, also known as Moreau-Yosida regularization, with a quadratic penalty term we can rewrite this as an unconstrained minimization problem with a penalty coefficient p mathbb R:\n\nmin_γ  H^1(I mathbb S^2)  underbracefrac12 int_0^T lVertdot γ(t)rVert^2 + p max(0 γ_3(t) - 1 + h_mathrmref)^2  mathrmdt_= f(γ)\n\nLet m mathbb R  mathbb R  m(x) = max(0 x). The objective is differentiable function f  mathcal X  mathbb R with derivative\n\nf(γ)δ γ = int_0^T langle dotγ(t) dotδ γ(t)rangle + p cdot m(γ_3(t) - 1 + h_mathrmref)δ γ_3(t)\n\nfor δ γ  T_γ mathcal X. This mapping is semismooth and our goal is to find a critical point of f by applying Newton’s method on f.\n\nThis then yields an elastic geodesic avoiding the north pol cap and connecting γ_0 and γ_T.\n\nFor our example we set\n\nN=200\n\nS = Manifolds.Sphere(2)\npower = PowerManifold(S, NestedPowerRepresentation(), N) # power manifold of S\n\nmutable struct VariationalSpace\n    manifold::AbstractManifold\n    degree::Integer\nend\n\ntest_space = VariationalSpace(S, 1)\n\nstart_interval = -pi/2 + 0.1\nend_interval = pi/2 - 0.1\ndiscrete_time = range(; start=start_interval, stop = end_interval, length=N+2) # equidistant discrete time points\n\nθ = pi/4\ny0 = [sin(θ)*cos(start_interval),sin(θ)*sin(start_interval),cos(θ)] # startpoint of geodesic\nyT = [sin(θ)*cos(end_interval),sin(θ)*sin(end_interval),cos(θ)] # endpoint of geodesic\n\n3-element Vector{Float64}:\n 0.07059288589999425\n 0.7035741925769522\n 0.7071067811865476\n\nIn order to apply Newton’s method to find a zero of f, we need the linear mapping Q_f(γ)^*circ f(γ) (cf. [WBS25]). Since the sphere is an embedded submanifold of mathbb R^3, we can use the formula\n\nQ_f(γ)^*circ f(γ)δ γphi = f(γ)(oversetV_γ(γ)δ γphi) + f_mathbb R^3(γ)δ γphi\n\nfor δ γ  phi  T_γ mathcal X, where oversetV_γ(hat γ)  L(T_γ mathcal X T_hatγmathcal X) is a vector transport and\n\nf_mathbb R^3(γ)δ γ phi = int_0^T langle dotδ γ(t) dotphi(t)rangle + p cdot m(γ_3(t) - 1 + h_mathrmref)phi_3(t) δ γ_3(t)\n\nis an (euclidean) Newton-derivative of f’.\n\nWe define a structure that has to be filled for two purposes:\n\nDefinition of an integrands and their derivatives\nDefinition of a vector transport and its derivative\n\nmutable struct DifferentiableMapping{F1<:Function,F2<:Function,T}\n    value::F1\n    derivative::F2\n    scaling_penalty::T\n    h_ref::T\nend;\n\nThe following routines define a vector transport and its euclidean derivative. As seen above, they are needed to derive Q_f(γ)^*circ f(γ).\n\nAs a vector transport we use the (pointwise) orthogonal projection onto the tangent spaces, i.e. for p q  mathbb S^2 and X  T_pmathbb S^2 we set\n\noversetV_p(q)X = (I-qcdot q^T)X  T_qmathbb S^2\n\nThe derivative of the vector transport is then given by\n\nleft(fracddqoversetV_p(q)bigvert_q=pδ qright)X = left( - δ qcdot p^T - pcdot δ q^Tright)cdot X\n\ntransport_by_proj(S, p, X, q) = X - q*(q'*X)\n\ntransport_by_proj_prime(S, p, X, dq) = (- dq*p' - p*dq')*X\n\ntransport = DifferentiableMapping(transport_by_proj,transport_by_proj_prime,nothing,nothing)\n\nDifferentiableMapping{typeof(transport_by_proj), typeof(transport_by_proj_prime), Nothing}(transport_by_proj, transport_by_proj_prime, nothing, nothing)\n\nThe following two routines define the integrand of f and the euclidean second derivative f_mathbb R^3. Here, a Newton-derivative of the maximum function given by\n\nm(x)= begincases\n        0   x0 \n        textarbitrary   x= 0\n        1   x0\n    endcases\n\nis used. A scaling parameter for the penalty parameter is also employed.\n\nfunction f_prime_at(Integrand, y, ydot, B, Bdot)\n    return ydot'*Bdot + Integrand.scaling_penalty * max(0.0, y[3] - 1.0 + Integrand.h_ref)*B[3]\nend\nfunction max_prime(y, h_ref)\n    if y[3] < 1.0 - h_ref\n        return 0.0\n    else\n        return 1.0\n    end\nend\nfunction f_second_at(Integrand,y,ydot,B1,B1dot,B2,B2dot)\n    return B1dot'*B2dot + Integrand.scaling_penalty*max_prime(y, Integrand.h_ref)*B1[3]*B2[3]\nend\nintegrand = DifferentiableMapping(f_prime_at,f_second_at,1.0,0.1)\n\nDifferentiableMapping{typeof(f_prime_at), typeof(f_second_at), Float64}(f_prime_at, f_second_at, 1.0, 0.1)","category":"section"},{"location":"examples/Elastic-Geodesic-Obstacle/#Newton-Equation","page":"Elastic Geodesic Obstacle","title":"Newton Equation","text":"In this example we implement a functor to compute the Newton matrix and the right hand side for the Newton equation\n\nQ^*_f(γ)circ f(γ)δ γ + f(γ) = 0^*_γ\n\nby using the assembler provided in ManoptExamples.jl.\n\nIt returns the matrix and the right hand side in base representation.\n\nThe assembly routines need a function for evaluation the iterates at the left and right quadrature point.\n\nevaluate(p, i, tloc) = (1.0-tloc)*p[i-1]+tloc*p[i];\n\nstruct NewtonEquation{F, TS, T, I, NM, Nrhs}\n    integrand::F\n    test_space::TS\n    transport::T\n    time_interval::I\n    A::NM\n    b::Nrhs\nend\n\nfunction NewtonEquation(M, F, test_space, VT, interval)\n    n = manifold_dimension(M)\n    A = spzeros(n,n)\n    b = zeros(n)\n    return NewtonEquation{typeof(F), typeof(test_space), typeof(VT), typeof(interval), typeof(A), typeof(b)}(F, test_space, VT, interval, A, b)\nend\n\nfunction (ne::NewtonEquation)(M, VB, p)\n    n = manifold_dimension(M)\n    ne.A .= spzeros(n,n)\n    ne.b .= zeros(n)\n\n    Op = OffsetArray([y0, p..., yT], 0:(length(p)+1))\n\n    ManoptExamples.get_jacobian!(M, Op, evaluate, ne.A, ne.integrand, ne.transport, ne.time_interval; test_space=ne.test_space)\n\n    ManoptExamples.get_right_hand_side!(M, Op, evaluate, ne.b, ne.integrand, ne.time_interval; test_space=ne.test_space)\nend\n\nWe compute the Newton direction δ γ by solving the linear system given by the base representation of the Newton equation directly and return the Newton direction in vector representation:\n\nfunction solve_in_basis_repr(problem, newtonstate)\n    X_base = (problem.newton_equation.A) \\ (-problem.newton_equation.b)\n    return get_vector(problem.manifold, newtonstate.p, X_base, DefaultOrthogonalBasis())\nend;\n\nFor the computation of a solution of the penalized problem we use a simple path-following method increasing the penalty parameter by a factor 1.2 in each iteration. For the first iteration we use a curve along the latitude connecting γ_0 and γ_T:\n\ny(t) = [sin(θ)*cos(t), sin(θ)*sin(t), cos(θ)]\ndiscretized_y = [y(ti) for ti in discrete_time[2:end-1]];\n\nWe compute the resulting elastic geodesic for h_mathrmref = 01 and h_mathrmref = 02.\n\nh_refs = [0.1, 0.2]\ny_star = copy(power, discretized_y)\nres = []\nfor h_ref in h_refs\n\n    integrand.scaling_penalty = 5.0\n    integrand.h_ref = h_ref\n        \n    NE = NewtonEquation(power, integrand, test_space, transport, discrete_time)\n    \n    st_res = vectorbundle_newton(power, TangentBundle(power), NE, y_star; sub_problem=solve_in_basis_repr, sub_state=AllocatingEvaluation(),\n        stopping_criterion=(StopAfterIteration(150)|StopWhenChangeLess(power,1e-13; outer_norm=Inf)),\n        retraction_method=ProjectionRetraction(),\n        debug = [:Iteration, (:Change, \"Change: %1.8e\"), \"\\n\", :Stop],\n        return_state=true)\n    \n    y_star = copy(power, get_solver_result(st_res))\n        \n    for i in range(1,50)\n        integrand.scaling_penalty *= 1.2\n\n        NE = NewtonEquation(power, integrand, test_space, transport, discrete_time)\n\n        (i > 1) && print(\".\")\n \n        st_res = vectorbundle_newton(power, TangentBundle(power), NE, y_star; sub_problem=solve_in_basis_repr, sub_state=AllocatingEvaluation(),\n            stopping_criterion=(StopAfterIteration(150)|StopWhenChangeLess(power,1e-13; outer_norm=Inf)),\n            retraction_method=ProjectionRetraction(),\n            debug = ((i > 1) ? [] : [:Iteration, (:Change, \"Change: %1.8e\"), \"\\n\", :Stop]),\n            return_state=true\n            )\n        y_star = copy(power, get_solver_result(st_res));\n    end\n    println(\"\")\n    push!(res, y_star)\nend\n\nInitial \n# 1     Change: 7.33716599e+00\n# 2     Change: 1.09269188e+00\n# 3     Change: 1.57343110e-02\n# 4     Change: 3.48909794e-05\n# 5     Change: 1.51861742e-10\n# 6     Change: 5.26159425e-15\nAt iteration 6 the algorithm performed a step with a change (5.338891568193822e-16) less than 1.0e-13.\nInitial \n# 1     Change: 1.36854874e-01\n# 2     Change: 2.85972773e-03\n# 3     Change: 1.48331704e-06\n# 4     Change: 3.99564289e-13\nAt iteration 4 the algorithm performed a step with a change (4.1765954767618633e-14) less than 1.0e-13.\n.................................................\nInitial \n# 1     Change: 7.94587041e-01\n# 2     Change: 2.06153348e-01\n# 3     Change: 1.21842504e-02\n# 4     Change: 4.06216523e-05\n# 5     Change: 4.49374037e-10\n# 6     Change: 1.69919912e-14\nAt iteration 6 the algorithm performed a step with a change (1.869333381474851e-15) less than 1.0e-13.\nInitial \n# 1     Change: 5.25153574e-01\n# 2     Change: 6.87239776e-02\n# 3     Change: 1.33983224e-03\n# 4     Change: 5.07882458e-07\n# 5     Change: 7.72342326e-14\nAt iteration 5 the algorithm performed a step with a change (7.982032576245276e-15) less than 1.0e-13.\n.................................................\n\nThis yields the elastic geodesics shown below avoiding the north pole cap (h_mathrmref=01 (left), h_mathrmref=02 (right)) and connecting two points γ_0 and γ_T (orange). The curve along the latitude connecting the two points (used as initial curve for the first iteration) is plotted as well (blue).\n\nn = 30\nu = range(0,stop=2*π,length=n);\nv = range(0,stop=π,length=n);\n    \nsx = [cos(ui) * sin(vj) for ui in u, vj in v]\nsy = [sin(ui) * sin(vj) for ui in u, vj in v]\nsz = [cos(vj) for ui in u, vj in v]\n\nπ1(x) = 1.01*x[1]\nπ2(x) = 1.01*x[2]\nπ3(x) = 1.01*x[3]\n\ngeodesic_start = [y0, discretized_y ...,yT]\n\nfig = Figure(resolution = (1400, 900), padding=0)\nax1 = Axis3(fig[1, 1]; aspect =:data)\nhidedecorations!(ax1)\nhidespines!(ax1)\n\nax2 = Axis3(fig[1, 2]; aspect =:data)\nhidedecorations!(ax2)\nhidespines!(ax2)\n\nax = [ax1, ax2]\n\nfor i in 1:length(ax)\n    x = acos(1-h_refs[i])\n\n    circx = [cos(ui)*sin(x) for ui in u]\n    circy = [sin(ui)*sin(x) for ui in u]\n    circz = fill(cos(x), n)\n\n    wireframe!(ax[i], sx, sy, sz, color = RGBA(0.5,0.5,0.7,0.1); transparency=true)\n\n    scatterlines!(ax[i], circx, circy, circz; markersize =2, color=:black, linewidth=2)\n    \n    scatterlines!(ax[i], π1.(res[i]), π2.(res[i]), π3.(res[i]); markersize =8, color=:orange, linewidth=2)\n\n    scatterlines!(ax[i], π1.(geodesic_start), π2.(geodesic_start), π3.(geodesic_start); markersize =8, color=:blue, linewidth=2)\n    \n    scatter!(ax[i], π1.([y0]), π2.([y0]), π3.([y0]); markersize = 10, color=:green)\n    scatter!(ax[i], π1.([yT]), π2.([yT]), π3.([yT]); markersize = 10, color=:red)\n    \n    ax[i].azimuth[] += 14.445\n    ax[i].elevation[] = 35.02\n\n    limits!(ax[i], -1.5, 1.5, -1.5, 1.5, -1.5, 1.5)\nend\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n(Image: )","category":"section"},{"location":"examples/Elastic-Geodesic-Obstacle/#Technical-details","page":"Elastic Geodesic Obstacle","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n⌃ [13f3f980] CairoMakie v0.15.7\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.122\n⌃ [e9467ef8] GLMakie v0.13.7\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.33.0\n  [682c06a0] JSON v1.3.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n⌃ [d3d80556] LineSearches v7.4.1\n⌅ [ee78f7c6] Makie v0.24.7\n  [af67fdf4] ManifoldDiff v0.4.5\n⌃ [1cead3c2] Manifolds v0.11.6\n⌃ [3362f125] ManifoldsBase v2.2.1\n⌃ [0fc0a36d] Manopt v0.5.28\n  [5b8d5e80] ManoptExamples v0.1.17 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.2\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.30\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.39.0\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered December 14, 2025, 11:22:40.","category":"section"},{"location":"examples/Elastic-Geodesic-Obstacle/#Literature","page":"Elastic Geodesic Obstacle","title":"Literature","text":"L. Weigl, R. Bergmann and A. Schiela. Newton's method into vector bundles Part II: : Application to Variational Problems on Manifolds, arXiv Preprint (2025).\n\n\n\n","category":"section"},{"location":"examples/#List-of-Examples","page":"Overview","title":"List of Examples","text":"Name provides Documentation Comment\nA Benchmark for Difference of Convex contains a few simple functions  \nBézier Curves and Minimizing their Acceleration tools Bézier curves and their acceleration 📚 \nSolving Rosenbrock with Difference of Convex DoC split of Rosenbrock 📚 uses a Rosenbrock based metric\nDifference of Convex vs. Frank-Wolfe   closed-form sub solver\nRiemannian Mean f, operatornamegradf (A/I), objective 📚 \nRobust PCA f, operatornamegradf (A/I), objective 📚 \nRosenbrock f, operatornamegradf (A/I), objective, minimizer 📚 \nThe Rayleigh Quotient f, operatornamegradf (A/I), operatornameHessf (A/I), objective 📚 \nTotal Variation Minimization f, operatornameproxf (A/I), objective 📚 \n\nSymbols:\n\nA Allocating variant\nI In-place variant\n📚 link to documented functions in the documentation","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","page":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","text":"Ronny Bergmann 2023-06-06","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Introduction","page":"Rosenbrock Metric","title":"Introduction","text":"This example illustrates how the 📖 Rosenbrock problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [BFSS24], Section 7.2.\n\nBoth the Rosenbrock problem\n\n    operatorname*argmin_xin ℝ^2 abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n\nwhere ab0 and usually b=1 and a gg b, we know the minimizer x^* = (bb^2)^mathrmT, and also the (Euclidean) gradient\n\nnabla f(x) =\n  beginpmatrix\n  4a(x_1^2-x_2) -2a(x_1^2-x_2)\n  endpmatrix\n  +\n  beginpmatrix\n  2(x_1-b) 0\n  endpmatrix\n\nThey are even available already here in ManifoldExamples.jl, see RosenbrockCost and RosenbrockGradient!!.\n\nFurthermore, the RosenbrockMetric can be used on ℝ^2, that is\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nIn this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e. using a gradient but not a Hessian.\n\nThe Euclidean Gradient\nThe Riemannian gradient descent with respect to the RosenbrockMetric\nThe Euclidean Difference of Convex Algorithm\nThe Riemannian Difference of Convex Algorithm respect to the RosenbrockMetric\n\nWhere we obtain a difference of convex problem by writing\n\nf(x) = abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 - bigl(x_1-bbigr)^2\n\nthat is\n\ng(x) = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 quadtext and quad h(x) = bigl(x_1-bbigr)^2\n\nusing LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_parameter!\nRandom.seed!(42)\n\npaul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"]\n\nTo emphasize the effect, we choose a quite large value of a.\n\na = 2*10^5\nb = 1\n\nand use the starting point and a direction to check gradients\n\np0 = [0.1, 0.2]","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","text":"For the Euclidean gradient we can just use the same approach as in the Rosenbrock example\n\nM = ℝ^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n∇f!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)\n\ndefine a common debug vector\n\ndebug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|δp|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ]\n\nand call the gradient descent algorithm\n\nEucl_GD_state = gradient_descent(M, f, ∇f!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)\n\nInitial F(x): 7.2208e+03 |grad f|: 7.750545e+04\n# 10000000 F(x): 8.9937e-06 |δp|: 1.3835e+00 | |grad f|: 8.170355e-03\nAt iteration 10000000 the algorithm reached its maximal number of iterations (10000000).\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 10000000 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0,\n    retraction_method=ManifoldsBase.ExponentialRetraction(),\n    contraction_factor=0.95,\n    sufficient_decrease=0.1,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: reached\n  * |Δp| < 1.0e-16: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","text":"For the Riemannian case, we define\n\nM_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())\n\nMetricManifold(Euclidean(2; field=ℝ), ManoptExamples.RosenbrockMetric())\n\nand the gradient is now adopted to the new metric\n\nfunction grad_f!(M, X, p)\n    ∇f!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend\n\nR_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)\n\nInitial F(x): 7.2208e+03 |grad f|: 7.600000e+04\n# 1000000  F(x): 1.3571e-09 |δp|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |δp|: 3.6836e-05 | |grad f|: 9.240792e-09\nAt iteration 2443750 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0,\n    retraction_method=ManifoldsBase.ExponentialRetraction(),\n    contraction_factor=0.95,\n    sufficient_decrease=0.1,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","text":"For the convex case, we have to first introduce the two parts of the cost.\n\nf1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b)\n\nand their (Euclidan) gradients\n\nfunction ∇h!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ∇h(M, p; a=100, b=1)\n    X = zero(p)\n    ∇h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ∇g!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ∇g(M, p; a=100, b=1)\n    X = zero(p)\n    ∇g!(M, X, p; a=a, b=b)\n    return X\nend\n\nand we define for convenience\n\ndocE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_∇h!(M, X, p) = ∇h!(M, X, p; a=a, b=b)\ndocE_∇g!(M, X, p) = ∇g!(M, X, p; a=a, b=b)\nfunction docE_∇f!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_∇g!(M, X, p)\n  docE_∇h!(M, Y, p)\n  X .-= Y\n  return X\nend\n\nThen we call the difference of convex algorithm on Eucldiean space ℝ^2.\n\nE_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_∇h!, p0;\n    gradient=docE_∇f!,\n    grad_g = docE_∇g!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)\n\nInitial F(x): 7.2208e+03 |grad f|: 0.000000e+00\n# 10000    F(x): 2.9705e-09 |δp|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |δp|: 1.2173e-04 | |grad f|: 4.541087e-08\nAt iteration 26549 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ManifoldsBase.ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch(;\n    |     initial_stepsize=1.0,\n    |     retraction_method=ManifoldsBase.ExponentialRetraction(),\n    |     contraction_factor=0.95,\n    |     sufficient_decrease=0.1,\n    | )\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 2000:   reached\n    |   * |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","text":"We first have to again defined the gradients with respect to the new metric\n\nfunction grad_h!(M, X, p; a=100, b=1)\n    ∇h!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ∇g!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend\n\nWhile the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For X in   h(p^(k)) the sunproblem top determine p^(k+1) reads\n\noperatorname*argmin_pinmathcal M g(p) - langle X log_p^(k)prangle\n\nfor which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with X = operatornamegrad h(p^(k)) that\n\nphi(p) = g(p) - langle X log_p^(k)prangle\n= abigl( p_1^2-p_2bigr)^2\n        + 2bigl(p_1-bbigr)^2 - 2(p^(k)_1-b)p_1 + 2(p^(k)_1-b)p^(k)_1\n\nits Euclidean gradient reads\n\noperatornamegradphi(p) =\n    nabla varphi(p)\n    = beginpmatrix\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^(k)_1-b)\n        -2a(p_1^2-p_2)\n    endpmatrix\n\nwhere we can again employ the gradient conversion from before to obtain the Riemannian gradient.\n\nmutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (ϕ::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    ϕ(M, X, p)\n    return X\nend\nfunction (ϕ::SubGrad)(M, X, p)\n    X .= [\n        4 * ϕ.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - ϕ.b) - 2 * (ϕ.pk[1] - ϕ.b),\n        -2 * ϕ.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend\n\nAnd in orer to update the sub solvers gradient correctly, we have to overwrite\n\nset_parameter!(ϕ::SubGrad, ::Val{:p}, p) = (ϕ.pk .= p)\nset_parameter!(ϕ::SubGrad, ::Val{:X}, X) = (ϕ.Xk .= X)\n\nAnd we again introduce for ease of use\n\ndocR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)\n\nThen we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.\n\nR_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)\n\nInitial F(x): 7.2208e+03 |grad f|: 0.000000e+00\nAt iteration 1235 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ManifoldsBase.ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch(;\n    |     initial_stepsize=1.0,\n    |     retraction_method=ManifoldsBase.ExponentialRetraction(),\n    |     contraction_factor=0.95,\n    |     sufficient_decrease=0.1,\n    | )\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 2000:   reached\n    |   * |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","page":"Rosenbrock Metric","title":"Comparison in Iterations","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^7),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\")\n\n(Image: )\n\nAnd we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Technical-details","page":"Rosenbrock Metric","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:7:58.","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Literature","page":"Rosenbrock Metric","title":"Literature","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Journal of Optimization Theory and Applications (2024).\n\n\n\n","category":"section"},{"location":"examples/Inextensible-Rod/#Inextensible-Rod","page":"Inextensible Rod","title":"Inextensible Rod","text":"Laura Weigl, Ronny Bergmann, and Anton Schiela 2025-11-25\n\nusing LinearAlgebra\nusing SparseArrays\nusing Manopt\nusing ManoptExamples\nusing Manifolds\nusing OffsetArrays\nusing RecursiveArrayTools\nusing CairoMakie, Makie, GeometryTypes, Colors","category":"section"},{"location":"examples/Inextensible-Rod/#Introduction","page":"Inextensible Rod","title":"Introduction","text":"In this example we compute equilibrium states of an inextensible elastic rod by applying Newton’s method on vector bundles which was introduced in [WBS25]. This example reproduces the results from the example in Section 6.3 therein.\n\nWe start with the following energy minimization problem\n\nmin_y mathcal Mfrac12int_0^1 σ(s)  ddot y(s)ddot y(s) mathrmds\n\nwhere mathcal M = ymid y H^2(01mathbbR^3)dot y(s) mathbb S^2   texton  01  The quantity overline σ  σ(s)  underline σ0 is the flexural stiffness of the rod, and dot y, ddot y are the derivatives of y with respect to s01. Since dot y(s)  mathbbS^2 for all s 01 the rod is inextensible with fixed length 1.\n\nIn addition the following boundary conditions are imposed:\n\ny(0)=y_a  mathbbR^3  dot y(0)=v_a mathbbS^2 \ny(1)=y_b  mathbbR^3  dot y(1)=v_b mathbbS^2\n\nIntroducing v(s)=dot y(s) we reformulate the problem as a mixed problem:\n\nmin_(yv) Y  mathcal V frac12int_0^1 σ  dot vdot v  mathrmds\nquad text st  quad dot y-v = 0\n\nwhere\n\nbeginalign*\nY=y H^2(01mathbbR^3)   y(0)=y_a y(1)=y_b  \nmathcal V=v H^1(01mathbbS^2)  v(0)=v_a v(1)=v_b \nendalign*\n\nTo derive equilibrium conditions for this problem we define the Lagrangian function\n\nL(yvλ) =int_0^1 frac12  σ left dot vdot v right+λ (dot y-v) mathrmds\n\nusing a Lagrangian multiplier λ  Λ = L_2(01mathbb R^3).\n\nWe obtain the following equilibrium conditions via setting the derivatives of the Lagrangian to zero:\n\nbeginalign*\nint_0^1 λ (dotϕ_y) mathrmds = 0 quad  ϕ_y Y\nint_0^1 σ left  dot vdotϕ_vright -  λ(ϕ_v) mathrmds = 0 quad  ϕ_v  T_v mathcal V\nint_0^1  ϕ_λ(dot y-v) mathrmd s = 0 quad  ϕ_λ  Λ\nendalign*\n\nHence, have to find a zero of the mapping\n\nF  Y  mathcal V  Λ  Y^*  T^*mathcal V Λ^*\n\ndefined by the equilibrium conditions.\n\nFor brevity we set mathcal X=Y  mathcal V  Λ and x=(yvλ) and obtain a mapping Fmathcal X  T^*mathcal X.\n\nFor our example we set σ equiv 1 and\n\nN=50\n\nS = Manifolds.Sphere(2)\nR3 = Manifolds.Euclidean(3)\npowerS = PowerManifold(S, NestedPowerRepresentation(), N) # power manifold of S\npowerR3 = PowerManifold(R3, NestedPowerRepresentation(), N) # power manifold of R^3\npowerR3_λ = PowerManifold(R3, NestedPowerRepresentation(), N+1) # power manifold of R^3\nproduct = ProductManifold(powerR3, powerS, powerR3_λ) # product manifold\n\nmutable struct VariationalSpace\n    manifold::AbstractManifold\n    degree::Integer\nend\n\ntest_spaces = ArrayPartition(VariationalSpace(R3, 1), VariationalSpace(S, 1), VariationalSpace(R3, 0))\n\nansatz_spaces = ArrayPartition(VariationalSpace(R3, 1), VariationalSpace(S, 1), VariationalSpace(R3, 0))\n\nstart_interval = 0.0\nend_interval = 1.0\ndiscrete_time = range(; start=start_interval, stop = end_interval, length=N+2)\n\ny0 = [0,0,0] # startpoint of rod\ny1 = [0.8,0,0] # endpoint of rod\n\nv0 = 1/norm([1,0,2])*[1,0,2] # start direction of rod\nv1 = 1/norm([1,0,0.8])*[1,0,0.8] # end direction of rod\n\n3-element Vector{Float64}:\n 0.7808688094430303\n 0.0\n 0.6246950475544243\n\nAs a starting point, we use\n\ny(t) = [t*0.8, 0.1*t*(1-t), 0]\nv(t) = [sin(t*π/2+π/4), cos(t*π/2+π/4), 0]\nλ(t) = [0.1, 0.1, 0.1]\n\ndiscretized_y = [y(ti) for ti in discrete_time[2:end-1]]\ndiscretized_v = [v(ti) for ti in discrete_time[2:end-1]]\ndiscretized_λ = [λ(ti) for ti in discrete_time[1:end-1]]\ndisc_point = ArrayPartition(discretized_y, discretized_v, discretized_λ)\n\n([[0.01568627450980392, 0.0019223375624759706, 0.0], [0.03137254901960784, 0.0037677816224529026, 0.0], [0.047058823529411764, 0.005536332179930796, 0.0], [0.06274509803921569, 0.0072279892349096505, 0.0], [0.0784313725490196, 0.008842752787389465, 0.0], [0.09411764705882353, 0.010380622837370242, 0.0], [0.10980392156862746, 0.011841599384851981, 0.0], [0.12549019607843137, 0.013225682429834679, 0.0], [0.14117647058823532, 0.014532871972318341, 0.0], [0.1568627450980392, 0.015763168012302962, 0.0]  …  [0.6431372549019608, 0.01576316801230296, 0.0], [0.6588235294117647, 0.014532871972318341, 0.0], [0.6745098039215687, 0.013225682429834679, 0.0], [0.6901960784313727, 0.011841599384851978, 0.0], [0.7058823529411765, 0.010380622837370245, 0.0], [0.7215686274509805, 0.008842752787389465, 0.0], [0.7372549019607844, 0.007227989234909656, 0.0], [0.7529411764705882, 0.005536332179930797, 0.0], [0.7686274509803922, 0.0037677816224529004, 0.0], [0.7843137254901961, 0.0019223375624759747, 0.0]], [[0.7285468091117903, 0.6849960196475806, 0.0], [0.7492957662581293, 0.6622354978915302, 0.0], [0.7693339709828789, 0.6388468056519614, 0.0], [0.7886424158350783, 0.6148521285201927, 0.0], [0.8072027855852095, 0.5902742269009196, 0.0], [0.8249974745983023, 0.5651364144225919, 0.0], [0.8420096035339453, 0.5394625358230247, 0.0], [0.8582230353573627, 0.5132769443312207, 0.0], [0.8736223906463696, 0.48660447856685624, 0.0], [0.8881930621796849, 0.4594704389793477, 0.0]  …  [0.8881930621796849, -0.4594704389793476, 0.0], [0.8736223906463696, -0.4866044785668561, 0.0], [0.8582230353573629, -0.5132769443312204, 0.0], [0.8420096035339453, -0.5394625358230248, 0.0], [0.8249974745983025, -0.5651364144225915, 0.0], [0.8072027855852096, -0.5902742269009195, 0.0], [0.7886424158350784, -0.6148521285201924, 0.0], [0.7693339709828788, -0.6388468056519614, 0.0], [0.7492957662581292, -0.6622354978915302, 0.0], [0.7285468091117903, -0.6849960196475806, 0.0]], [[0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1]  …  [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1]])\n\nIn order to apply Newton’s method to find a zero of F, we need the linear mapping Q_F(x)^* F(x). Since the sphere is an embedded submanifold of mathbb R^3, we can use the formula\n\nQ_F(x)^* F(x)δxϕ =  F_mathbb R^9(x)δxϕ + F(x)(oversetV_x(x)δxϕ)\n\nfor x mathcal X and δx  ϕ  T_x mathcal X, where oversetV_x(hat x)  L(T_x mathcal X T_hatxmathcal X) is a vector transport and\n\nF_mathbb R^9(x)δx^2 δx^1 = int_0^1 δλ(dot ϕ_y) + σ  dotδv dot ϕ_v   -δλ(ϕ_v) + ϕ_λ(dotδy)  - ϕ_λ(δv)  mathrmds\n\nis the euclidean derivative of F.\n\nThe part, introduced by the connection is given by\n\nF(x)(oversetV_x(x)δxϕ) = int_0^1  dot v (P(v)δv  ϕ_v)dot   - λ(P(v)δv  ϕ_v)  mathrmds\n\nwhere P(v) mathbbR^3  T_vmathbbS^2 denotes the orthogonal projection.\n\nWe define a structure that has to be filled for two purposes:\n\nDefinition of an integrands and their derivatives\nDefinition of a vector transport and its derivative\n\nmutable struct DifferentiableMapping{F1<:Function,F2<:Function}\n    value::F1\n    derivative::F2\nend\n\nThe following routines define a vector transport and its euclidean derivative. As seen above, they are needed to derive a covariant derivative of F.\n\nAs a vector transport we use the (pointwise) orthogonal projection onto the tangent spaces, i.e. for p q  mathbb S^2 and X  T_pmathbb S^2 we set\n\noversetV_p(q)X = (I-q  q^T)X  T_qmathbb S^2\n\nThe derivative of the vector transport is then given by\n\nleft(fracddqoversetV_p(q)bigvert_q=pδqright)X = left( - δq  p^T - p  δq^Tright)  X\n\ntransport_by_proj(S, p, X, q) = X - q*(q'*X)\n\ntransport_by_proj_prime(S, p, X, dq) =  (- dq*p' - p*dq')*X\n\ntransport = DifferentiableMapping(transport_by_proj,transport_by_proj_prime)\n\nDifferentiableMapping{typeof(transport_by_proj), typeof(transport_by_proj_prime)}(transport_by_proj, transport_by_proj_prime)\n\nThe following routines define the integrand of F and its euclidean derivative.\n\nFy_at(Integrand, y, ydot, T, Tdot) = Tdot'*y.x[3] # y component of F\nFv_at(Integrand, y, ydot, T, Tdot) = ydot.x[2]'*Tdot-T'*y.x[3] # v component of F\nFλ_at(Integrand, y, ydot, T, Tdot) = (ydot.x[1]-y.x[2])'*T # λ component of F\n\nF_prime_yλ_at(Integrand,y,ydot,B,Bdot,T,Tdot) = Tdot'*B # derivative of Fy_at w.r.t. λ (others are zero)\n\nF_prime_vv_at(Integrand,y,ydot,B,Bdot,T,Tdot) = Bdot'*Tdot # derivative of Fv_at w.r.t. v (others are zero)\n\nF_prime_λv_at(Integrand,y,ydot,B,Bdot,T,Tdot) = -B'*T # derivative of Fλ_at w.r.t. v (others are zero)\n\nintegrand_vv = DifferentiableMapping(Fv_at,F_prime_vv_at)\nintegrand_yλ = DifferentiableMapping(Fy_at,F_prime_yλ_at)\nintegrand_λv = DifferentiableMapping(Fλ_at,F_prime_λv_at)\n\nDifferentiableMapping{typeof(Fλ_at), typeof(F_prime_λv_at)}(Fλ_at, F_prime_λv_at)\n\nIf no vector transport is needed, the identity transport is used as dummy\n\nidentity_transport(S, p, X, q) = X\nidentity_transport_prime(S, p, X, dq) = 0.0*X\n\nid_transport = DifferentiableMapping(identity_transport,identity_transport_prime)\n\nDifferentiableMapping{typeof(identity_transport), typeof(identity_transport_prime)}(identity_transport, identity_transport_prime)","category":"section"},{"location":"examples/Inextensible-Rod/#NewtonEquation","page":"Inextensible Rod","title":"NewtonEquation","text":"In this example we implement a functor to compute the Newton matrix and the right hand side for the Newton equation\n\nQ^*_F(x) F(x)δx + F(x) = 0_x^*\n\nby using the assembler provided in ManoptExamples.jl.\n\nIt returns the matrix and the right hand side in base representation. Moreover, for the computation of the simplified Newton direction (which is necessary for affine covariant damping) a method returning the assembled right hand side for the simplified Newton equation is provided.\n\nThe assembly routines need a function for evaluating the test functions at the left and right quadrature point.\n\nfunction evaluate(y, i, tloc)\nreturn ArrayPartition(\n    (1.0-tloc)*y.x[1][i-1]+tloc*y.x[1][i],\n    (1.0-tloc)*y.x[2][i-1]+tloc*y.x[2][i],\n    y.x[3][i]\n)\nend;\n\nstruct NewtonEquation{Fy, Fv, Fλ, TS, AS, T, O, NM, Nrhs}\n    integrand_y::Fy\n    integrand_v::Fv\n    integrand_λ::Fλ\n    test_spaces::TS\n    ansatz_spaces::AS\n    vectortransport::T\n    discrete_time_interval::O\n    A13::NM\n    A22::NM\n    A32::NM\n    A::NM\n    b1::Nrhs\n    b2::Nrhs\n    b3::Nrhs\n    b::Nrhs\nend\n\nfunction NewtonEquation(M, inty, intv, intλ, test_spaces, ansatz_spaces, VT, discrete_time)\n    n1 = Int(manifold_dimension(submanifold(M, 1)))\n    n2 = Int(manifold_dimension(submanifold(M, 2)))\n    n3 = Int(manifold_dimension(submanifold(M, 3)))\n\n    # non-zero blocks of the Newton matrix\n    A13 = spzeros(n1,n3)\n    A22 = spzeros(n2,n2)\n    A32 = spzeros(n3,n2)\n\n    A = spzeros(n1+n2+n3, n1+n2+n3)\n\n    b1 = zeros(n1)\n    b2 = zeros(n2)\n    b3 = zeros(n3)\n    b = zeros(n1+n2+n3)\n\n    return NewtonEquation{typeof(inty), typeof(intv), typeof(intλ), typeof(test_spaces), typeof(ansatz_spaces), typeof(VT), typeof(discrete_time), typeof(A13), typeof(b1)}(inty, intv, intλ, test_spaces, ansatz_spaces, VT, discrete_time, A13, A22, A32, A, b1, b2, b3, b)\nend\n\nfunction (ne::NewtonEquation)(M, VB, p)\n    n1 = Int(manifold_dimension(submanifold(M, 1)))\n    n2 = Int(manifold_dimension(submanifold(M, 2)))\n    n3 = Int(manifold_dimension(submanifold(M, 3)))\n\n    ne.A13 .= spzeros(n1,n3)\n    ne.A22 .= spzeros(n2,n2)\n    ne.A32 .= spzeros(n3,n2)\n\n    ne.b1 .= zeros(n1)\n    ne.b2 .= zeros(n2)\n    ne.b3 .= zeros(n3)\n\n    Op_y = OffsetArray([y0, p[M, 1]..., y1], 0:(length(p[M, 1])+1))\n    Op_v = OffsetArray([v0, p[M, 2]..., v1], 0:(length(p[M, 2])+1))\n    Op_λ = OffsetArray(p[M, 3], 1:length(p[M, 3]))\n\n    Op = ArrayPartition(Op_y,Op_v,Op_λ);\n\n    # assemble (2,2)-block using the connection\n    ManoptExamples.get_jacobian_block!(M, Op, evaluate, ne.A22, ne.integrand_v, ne.vectortransport, ne.discrete_time_interval; row_index = 2, column_index = 2, test_space = ne.test_spaces.x[2], ansatz_space = ne.ansatz_spaces.x[2])\n\n    # assemble (1,3)-block without connection\n    ManoptExamples.get_jacobian_block!(M, Op, evaluate, ne.A13, ne.integrand_y, id_transport, ne.discrete_time_interval; row_index = 1, column_index = 3, test_space = ne.test_spaces.x[1], ansatz_space = ne.ansatz_spaces.x[3])\n\n    # assemble (3,2)-block without connection\n    ManoptExamples.get_jacobian_block!(M, Op, evaluate, ne.A32, ne.integrand_λ, id_transport, ne.discrete_time_interval; row_index = 3, column_index = 2, test_space = ne.test_spaces.x[3], ansatz_space = ne.ansatz_spaces.x[2])\n\n\n    ManoptExamples.get_right_hand_side_row!(M, Op, evaluate, ne.b1, ne.integrand_y, ne.discrete_time_interval; row_index=1, test_space = ne.test_spaces.x[1])\n\n    ManoptExamples.get_right_hand_side_row!(M, Op, evaluate, ne.b2, ne.integrand_v, ne.discrete_time_interval,; row_index=2, test_space = ne.test_spaces.x[2])\n\n    ManoptExamples.get_right_hand_side_row!(M, Op, evaluate, ne.b3, ne.integrand_λ, ne.discrete_time_interval,; row_index=3, test_space = ne.test_spaces.x[3])\n\n\n    ne.A .= vcat(\n        hcat(spzeros(n1,n1) , spzeros(n1,n2) , ne.A13),\n        hcat(spzeros(n2,n1), ne.A22 , ne.A32'),\n        hcat(ne.A13', ne.A32, spzeros(n3,n3))\n    )\n    ne.b .= vcat(ne.b1, ne.b2, ne.b3)\n    return #TODO: Is this correct? # LW: Wir überspeichern A und b und returnen nichts, lässt man das dann einfach weg?\nend\n\n\nfunction (ne::NewtonEquation)(M, VB, p, p_trial)\n    n1 = Int(manifold_dimension(submanifold(M, 1)))\n    n2 = Int(manifold_dimension(submanifold(M, 2)))\n    n3 = Int(manifold_dimension(submanifold(M, 3)))\n\n    btrial_y = zeros(n1)\n    btrial_v = zeros(n2)\n    btrial_λ = zeros(n3)\n\n    Op_y = OffsetArray([y0, p[M, 1]..., y1], 0:(length(p[M, 1])+1))\n    Op_v = OffsetArray([v0, p[M, 2]..., v1], 0:(length(p[M, 2])+1))\n    Op_λ = OffsetArray(p[M, 3], 1:length(p[M, 3]))\n    Op = ArrayPartition(Op_y,Op_v,Op_λ);\n\n    Optrial_y = OffsetArray([y0, p_trial[M,1]..., y1], 0:(length(p_trial[M,1])+1))\n    Optrial_v = OffsetArray([v0, p_trial[M,2]..., v1], 0:(length(p_trial[M,2])+1))\n    Optrial_λ = OffsetArray(p_trial[M,3], 1:length(p_trial[M,3]))\n    Optrial = ArrayPartition(Optrial_y,Optrial_v,Optrial_λ);\n\n    ManoptExamples.get_right_hand_side_simplified_row!(M, Op, Optrial, evaluate, btrial_y, ne.integrand_y, id_transport, ne.discrete_time_interval; row_index=1, test_space = ne.test_spaces.x[1])\n\n    ManoptExamples.get_right_hand_side_simplified_row!(M, Op, Optrial, evaluate, btrial_v, ne.integrand_v, ne.vectortransport, ne.discrete_time_interval; row_index=2, test_space = ne.test_spaces.x[2])\n\n    ManoptExamples.get_right_hand_side_simplified_row!(M, Op, Optrial, evaluate, btrial_λ, ne.integrand_λ, id_transport, ne.discrete_time_interval; row_index=3, test_space = ne.test_spaces.x[3])\n\n    return vcat(btrial_y, btrial_v, btrial_λ)\nend\n\nWe compute the Newton direction δx by solving the linear system given by the base representation of the Newton equation directly and return the Newton direction in vector representation:\n\nfunction solve_in_basis_repr(problem, newtonstate)\n    X = (problem.newton_equation.A) \\ (-problem.newton_equation.b)\n    return get_vector(problem.manifold, newtonstate.p, X, DefaultOrthogonalBasis())\nend;\n\nWe adjust the norms for computation of damping factors and stopping criterion\n\npr_inv = Manifolds.InverseProductRetraction(LogarithmicInverseRetraction(), ProjectionInverseRetraction(), LogarithmicInverseRetraction())\nrec = RecordChange(product;\n    inverse_retraction_method=pr_inv);\n\ny_0 = copy(product, disc_point)\n\nNE = NewtonEquation(product, integrand_yλ, integrand_vv, integrand_λv, test_spaces, ansatz_spaces, transport, discrete_time)\n\nst_res = vectorbundle_newton(\n    product, TangentBundle(product), NE, y_0;\n    sub_problem=solve_in_basis_repr,\n    stopping_criterion=(StopAfterIteration(100)|StopWhenChangeLess(product,1e-12; outer_norm=Inf, inverse_retraction_method=pr_inv)),\n    retraction_method=ProductRetraction(ExponentialRetraction(), ProjectionRetraction(), ExponentialRetraction()),\n    stepsize=Manopt.AffineCovariantStepsize(product, θ_des=0.5, outer_norm=Inf),\n    debug=[:Iteration, (:Change, \"Change: %1.8e\"), \"\\n\", :Stop, (:Stepsize, \"Stepsize: %1.8e\"), \"\\n\",],\n    record=[:Iterate, rec => :Change, :Stepsize],\n    return_state=true\n)\n\nInitial \nStepsize: 1.00000000e+00\n# 1     Change: 3.45474617e+02\nStepsize: 5.59390051e-01\n# 2     Change: 1.37369197e+02\nStepsize: 6.76049167e-01\n# 3     Change: 3.88725651e+01\nStepsize: 7.37587158e-01\n# 4     Change: 4.22305879e+01\nStepsize: 1.00000000e+00\n# 5     Change: 7.89277655e+00\nStepsize: 1.00000000e+00\n# 6     Change: 1.91987626e+00\nStepsize: 1.00000000e+00\n# 7     Change: 2.35146325e-01\nStepsize: 1.00000000e+00\n# 8     Change: 1.77332520e-03\nStepsize: 1.00000000e+00\n# 9     Change: 8.40237088e-08\nStepsize: 1.00000000e+00\n# 10    Change: 8.03403008e-14\nStepsize: 1.00000000e+00\nAt iteration 10 the algorithm performed a step with a change (8.029514851200385e-14) less than 1.0e-12.\n\n# Solver state for `Manopt.jl`s Vector bundle Newton method\nAfter 10 iterations\n\n## Parameters\n* retraction method: ProductRetraction(ManifoldsBase.ExponentialRetraction(), ManifoldsBase.ProjectionRetraction(), ManifoldsBase.ExponentialRetraction())\n* step size: Manopt.AffineCovariantStepsize{Float64, Float64, Float64}(1.0, 1.3, 0.5, 0.55, 1.0, Inf)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 100:  not reached\n  * |Δp| < 1.0e-12: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [ (:Iteration, \"# %-6d\"), (:Change, \"Change: %1.8e\"), \"\\n\", (:Stepsize, \"Stepsize: %1.8e\"), \"\\n\" ]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIterate(RecursiveArrayTools.ArrayPartition{Float64, Tuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}}), RecordChange(; inverse_retraction_method=InverseProductRetraction(ManifoldsBase.LogarithmicInverseRetraction(), ManifoldsBase.ProjectionInverseRetraction(), ManifoldsBase.LogarithmicInverseRetraction())), Manopt.RecordStepsize([0.5593900510125134, 0.6760491669310934, 0.7375871581755759, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])]),)\n\nWe extract the recorded values\n\nchange = get_record(st_res, :Iteration, :Change)[2:end]\nstepsizes = get_record(st_res, :Iteration, :Stepsize)\np_res = get_solver_result(st_res)\n\n([[0.008904276862763037, 0.0003794605795816187, 0.017460500666676], [0.018143286143667323, 0.00152081368394961, 0.034711648377215354], [0.027840292732882903, 0.0034262792250266383, 0.05163960361815171], [0.0381076283761368, 0.006089979876316308, 0.06812141350303859], [0.04904343359669994, 0.009496546613473643, 0.08402438936510451], [0.060728324272217096, 0.013619615418521998, 0.09920651635805777], [0.07322213462797549, 0.01842037924037604, 0.11351804256682929], [0.08656095961408512, 0.023846386838844314, 0.12680432051762686], [0.10075477648675486, 0.029830792165109296, 0.13890987037390587], [0.1157859540186124, 0.03629224746135711, 0.14968350667501418]  …  [0.623225361158195, 0.04419828798244499, -0.0502506851674307], [0.6413224245134539, 0.037073887262322185, -0.052599264478433314], [0.659665705670604, 0.030235957314212275, -0.05329545421595393], [0.6781434043236693, 0.023814718003770743, -0.05228979957432842], [0.6966274730965324, 0.01793804716725821, -0.04956288732299869], [0.7149792443731721, 0.012727376241392002, -0.04512775332276982], [0.7330559727403079, 0.008293653197246143, -0.039030636153589794], [0.7507178285149068, 0.0047337229772822864, -0.03134989687904509], [0.7678348093519181, 0.002127444004369359, -0.022193127581862194], [0.7842930393971138, 0.0005357808165138421, -0.011692677489636396]], [[0.4610226445018718, 0.03870497911732511, 0.8865438770010361], [0.4813563021503653, 0.07771303752821004, 0.8730731894739784], [0.5077383699496241, 0.11664444766164686, 0.8535782451015302], [0.5395298656622737, 0.15505301876989952, 0.8275663631569316], [0.5759222668351662, 0.19241678842014864, 0.794537174773792], [0.6159365820675843, 0.22813622969478356, 0.7540397785074413], [0.6584320742197703, 0.26154168013432827, 0.7057358947872523], [0.7021280743634138, 0.291911094909436, 0.6494644561941004], [0.7456412466488979, 0.3184982483695919, 0.5853016291463566], [0.7875388616005717, 0.3405701918476856, 0.5136092735666933]  …  [0.9153659449496235, -0.3689871160975927, -0.16110150520856303], [0.9305345172867806, -0.3577017573549358, -0.0784535845137046], [0.940480160742526, -0.3397670973522735, 0.007442231286602206], [0.9442451018701246, -0.31519931231276144, 0.0951345421591993], [0.9411299129619203, -0.284221113011519, 0.18301050747643394], [0.9307507572553287, -0.247267321426833, 0.26937316054691024], [0.9130755361925261, -0.2049724290760462, 0.35253279070945487], [0.8884337528165571, -0.1581404533602665, 0.43090261529410345], [0.8574982925585989, -0.10770000187685154, 0.5030878530185496], [0.8212411720513654, -0.05464964328441178, 0.5679580563884856]], [[-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063708, -1.6417144250724176, -5.014626735955926]  …  [-27.135584634063704, -1.6417144250724174, -5.014626735955925], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063704, -1.6417144250724174, -5.014626735955925], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063704, -1.6417144250724174, -5.014626735955925], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063708, -1.6417144250724174, -5.014626735955926], [-27.135584634063704, -1.6417144250724174, -5.014626735955925], [-27.135584634063708, -1.6417144250724174, -5.014626735955926]])\n\nand plot the result, where we measure the norms of the Newton direction in each iteration,\n\nf = Figure(;)\n\nrow, col = fldmod1(1, 2)\n\nAxis(f[row, col], yscale = log10, title = string(\"Semilogarithmic Plot of the norms of the Newton direction\"), xminorgridvisible = true, xticks = (1:length(change)), xlabel = \"Iteration\", ylabel = \"‖δx‖\")\n    scatterlines!(change, color = :blue)\nf\n\n(Image: )\n\nthe stepsizes computed via the affine covariant damping strategy,\n\nf_st = Figure(;)\n\nrow_st, col_st = fldmod1(1, 2)\n\nAxis(f_st[row_st, col_st], title = string(\"Stepsizes\"), xminorgridvisible = true, xticks = (1:length(stepsizes)), xlabel = \"Iteration\", ylabel = \"α\")\nscatterlines!(stepsizes[1:end-1], color = :blue)\nf_st\n\n(Image: )\n\nand the resulting rod (orange). The starting rod (red) is plotted as well.\n\nfig = Figure(size = (1000, 500))\nax = Axis3(fig[1, 1], aspect = :data, viewmode = :fitzoom, azimuth=-3π/4 + 0.3, elevation=π/8 + 0.15)\nπ1(x) = x[1]\nπ2(x) = x[2]\nπ3(x) = x[3]\nscatter!(ax, π1.(p_res[product, 1]), 0.3 .+ 0.0.*π2.(p_res[product, 1]), π3.(p_res[product, 1]); markersize =8, color = RGBAf(0.9, 0.7, 0.5, 0.5))\nscatter!(ax, π1.(discretized_y), 0.3 .+ 0.0.*π2.(discretized_y), π3.(discretized_y); markersize =8, color = RGBAf(0.8, 0.5, 0.5, 0.5))\nscatter!(ax, π1.(p_res[product, 1]), π2.(p_res[product, 1]), π3.(p_res[product, 1]); markersize =8, color=:orange)\nscatter!(ax, π1.([y0, y1]), π2.([y0, y1]), π3.([y0, y1]); markersize =8, color=:red)\nscatter!(ax, π1.(discretized_y), π2.(discretized_y), π3.(discretized_y); markersize =8, color=:red)\nfig\n\n(Image: )","category":"section"},{"location":"examples/Inextensible-Rod/#Technical-details","page":"Inextensible Rod","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:9:33.","category":"section"},{"location":"examples/Inextensible-Rod/#Literature","page":"Inextensible Rod","title":"Literature","text":"L. Weigl, R. Bergmann and A. Schiela. Newton's method into vector bundles Part II: : Application to Variational Problems on Manifolds, arXiv Preprint (2025).\n\n\n\n","category":"section"},{"location":"changelog/#Changelog","page":"Changelog","title":"Changelog","text":"All notable changes to this Julia package will be documented in this file.\n\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"section"},{"location":"changelog/#[0.1.18](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.18)-(–-23/10/2025)","page":"Changelog","title":"0.1.18 (– 23/10/2025)","text":"","category":"section"},{"location":"changelog/#Added","page":"Changelog","title":"Added","text":"Two examples about elastic geodesics and one about an inextensible rod to illustrate the new vector bundle newton method in Manopt.jl. (#38)","category":"section"},{"location":"changelog/#[0.1.17](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.17)-(–-16/10/2025)","page":"Changelog","title":"0.1.17 (– 16/10/2025)","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Changelog","title":"Added","text":"An example comparing spectral and robust Procrustes","category":"section"},{"location":"changelog/#Changed","page":"Changelog","title":"Changed","text":"update rendering to use the QuartoNotebookRunner.jl removing the dependency on Python\nswitched to Runic.jl for code formatting","category":"section"},{"location":"changelog/#[0.1.16](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.16)-(–-03/10/2025)","page":"Changelog","title":"0.1.16 (– 03/10/2025)","text":"","category":"section"},{"location":"changelog/#Added-3","page":"Changelog","title":"Added","text":"Two numerical experiments from the Proximal Gradient preprints.","category":"section"},{"location":"changelog/#[0.1.15](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.15)-(–-18/08/2025)","page":"Changelog","title":"0.1.15 (– 18/08/2025)","text":"","category":"section"},{"location":"changelog/#Added-4","page":"Changelog","title":"Added","text":"Numerical experiments from the Proximal Gradient preprints.","category":"section"},{"location":"changelog/#[0.1.14](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.14)-(–-14/04/2025)","page":"Changelog","title":"0.1.14 (– 14/04/2025)","text":"","category":"section"},{"location":"changelog/#Added-5","page":"Changelog","title":"Added","text":"Projected Gradient example.","category":"section"},{"location":"changelog/#[0.1.13](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.13)-(–-21/03/2025)","page":"Changelog","title":"0.1.13 (– 21/03/2025)","text":"","category":"section"},{"location":"changelog/#Changed-2","page":"Changelog","title":"Changed","text":"Updated numerical experiments from the Riemannian Convex Bundle paper.","category":"section"},{"location":"changelog/#[0.1.12](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.12)-(–-11/02/2025)","page":"Changelog","title":"0.1.12 (– 11/02/2025)","text":"","category":"section"},{"location":"changelog/#Changed-3","page":"Changelog","title":"Changed","text":"Update ManifoldDiff.jl compat to 0.4.","category":"section"},{"location":"changelog/#[0.1.11](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.11)-(–-10/02/2025)","page":"Changelog","title":"0.1.11 (– 10/02/2025)","text":"","category":"section"},{"location":"changelog/#Changed-4","page":"Changelog","title":"Changed","text":"Bump dependencies on CI\nAdapt to ManifoldsBase 1.0","category":"section"},{"location":"changelog/#[0.1.10](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.10)-(–-18/10/2024)","page":"Changelog","title":"0.1.10 (– 18/10/2024)","text":"","category":"section"},{"location":"changelog/#Changed-5","page":"Changelog","title":"Changed","text":"Bump dependencies","category":"section"},{"location":"changelog/#[0.1.9](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.9)-(–-28/06/2024)","page":"Changelog","title":"0.1.9 (– 28/06/2024)","text":"","category":"section"},{"location":"changelog/#Added-6","page":"Changelog","title":"Added","text":"Three numerical experiments from the Riemannian Convex Bundle paper.","category":"section"},{"location":"changelog/#[0.1.8](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.8)-(–-12/06/2024)","page":"Changelog","title":"0.1.8 (– 12/06/2024)","text":"","category":"section"},{"location":"changelog/#Changed-6","page":"Changelog","title":"Changed","text":"use range compatible with Julia 1.6 and hence lower the compatibility entry for Julia again.","category":"section"},{"location":"changelog/#[0.1.7](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.7)-(–-07/06/2024)","page":"Changelog","title":"0.1.7 (– 07/06/2024)","text":"","category":"section"},{"location":"changelog/#Changed-7","page":"Changelog","title":"Changed","text":"make Manopt.jl a weak dependency and load functions that require parts of it only load as an extension. This makes it easier to use the examples in the tests of Manopt itself.","category":"section"},{"location":"changelog/#[0.1.6](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.6)-(–-22/03/2024)","page":"Changelog","title":"0.1.6 (– 22/03/2024)","text":"","category":"section"},{"location":"changelog/#Added-7","page":"Changelog","title":"Added","text":"Hyperparameter optimization example.","category":"section"},{"location":"changelog/#[0.1.3](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.3)-(–-11/12/2023)","page":"Changelog","title":"0.1.3 (– 11/12/2023)","text":"","category":"section"},{"location":"changelog/#Added-8","page":"Changelog","title":"Added","text":"Total variation Minimization cost, proxes, and an example\nBézier curve cost, gradients, and an example.","category":"section"},{"location":"changelog/#[0.1.3](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.3)-(–-16/09/2023)","page":"Changelog","title":"0.1.3 (– 16/09/2023)","text":"","category":"section"},{"location":"changelog/#Added-9","page":"Changelog","title":"Added","text":"Rayleigh Quotient functions added\nan example illustrating Euclidean gradient/HEssian conversion\nAdd Literature with DocumenterCitations","category":"section"},{"location":"changelog/#[0.1.2](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.2)-(–-13/06/2023)","page":"Changelog","title":"0.1.2 (– 13/06/2023)","text":"","category":"section"},{"location":"changelog/#Added-10","page":"Changelog","title":"Added","text":"Update examples to use Quarto\nAdd DC examples","category":"section"},{"location":"changelog/#[0.1.1](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.1)-(–-01/03/2023)","page":"Changelog","title":"0.1.1 (– 01/03/2023)","text":"","category":"section"},{"location":"changelog/#Added-11","page":"Changelog","title":"Added","text":"Rosenbrock function and examples","category":"section"},{"location":"changelog/#[0.1.0](https://github.com/JuliaManifolds/ManoptExamples.jl/releases/tag/v0.1.0)-(–-18/02/2023)","page":"Changelog","title":"0.1.0 (– 18/02/2023)","text":"","category":"section"},{"location":"changelog/#Added-12","page":"Changelog","title":"Added","text":"Setup the project to collect example objectives for Manopt.jl which are well-documented and well-tested\nSetup Documentation to provide one example Quarto file for every example objective to illustrate how to use them.","category":"section"},{"location":"examples/Bezier-curves/#Minimizing-the-Acceleration-of-Bézier-Curves-on-the-Sphere","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Ronny Bergmann 2023-06-06\n\nusing Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/Bezier-curves/#Introduction","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Introduction","text":"Bézier Curves can be generalized to manifolds by generalizing the de Casteljau algorithm 📖 to work with geodesics instead of straight lines. An implementation in just a few lines as we demonstrated in [ABBR23] as\n\nfunction bezier(M::AbstractManifold, t, pts::NTuple)\n    p = bezier(M, t, pts[1:(end - 1)])\n    q = bezier(M, t, pts[2:end])\n    return shortest_geodesic(M, p, q, t)\nend\nfunction bezier(M::AbstractManifold, t, pts::NTuple{2})\n    return shortest_geodesic(M, pts[1], pts[2], t)\nend\n\nwhich is also available within this package as de_Casteljau using a simple BezierSegment struct to make it easier to also discuss the case where we compose a set of segments into a composite Bézier course.\n\nIn the following we will need the following packages and functions. They are documented in the section on Bezier Curves and were derived in [BG18] based on [PN07].\n\nusing ManoptExamples:\n    artificial_S2_composite_Bezier_curve,\n    BezierSegment,\n    de_Casteljau,\n    get_Bezier_degrees,\n    get_Bezier_inner_points,\n    get_Bezier_junctions,\n    get_Bezier_junction_tangent_vectors,\n    get_Bezier_points,\n    get_Bezier_segments,\n    grad_L2_acceleration_Bezier,\n    L2_acceleration_Bezier\n\nThis notebook reproduces the example form Section 5.2 in [BG18].\n\nThe following image illustrates how the de-Casteljau algorithm works for one segment.\n\n(Image: A Bezier segment and illustration of the de-Casteljau algorithm)\n\nA Bezier segment and illustration of the de-Casteljau algorithm","category":"section"},{"location":"examples/Bezier-curves/#Approximating-data-by-a-curve-with-minimal-accelartion","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Approximating data by a curve with minimal accelartion","text":"We first load our example data\n\nM = Sphere(2)\nB = artificial_S2_composite_Bezier_curve()\ndata_points = get_Bezier_junctions(M, B)\n\nWhich is the following cure, which clearly starts and ends slower than its speed in the middle, which can be seen by the increasing length of the tangent vectors in the middle.\n\n(Image: The original curve)\n\nThe original curve\n\nWe continue to recude the points, since we “know” sme points due to the C^1 property: the second to last control point of the first segment b_02, the joint junction point connecting both segments b_03=b_10 and the second control point b_11 of the second segment have to line in the tangent space of the joint junction point. Hence we only have to store one of the control points.\n\nWe can use this reduced form as the variable to optimize and the one from the data as our initial point.\n\npB = get_Bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB))\n\nPowerManifold(Sphere(2), ManifoldsBase.NestedPowerRepresentation(), 8)\n\nAnd we further define the acceleration of the curve as our cost function, where we discretize the acceleration at a certain set of points and set the λ=10\n\ncurve_samples = [range(0, 3; length=101)...] # sample curve for the gradient\nλ = 10.0\nfunction f(M, pB)\n    return L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend\nfunction grad_f(M, pB)\n    return grad_L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend\n\ngrad_f (generic function with 1 method)\n\nThen we can optimize\n\nx0 = pB\npB_opt = gradient_descent(\n    N,\n    f,\n    grad_f,\n    x0;\n    stepsize=ArmijoLinesearch(N;\n        initial_stepsize=1.0,\n        retraction_method=ExponentialRetraction(),\n        contraction_factor=0.5,\n        sufficient_decrease=0.001,\n    ),\n    stopping_criterion=StopWhenChangeLess(N, 1e-5) |\n                       StopWhenGradientNormLess(1e-7) |\n                       StopAfterIteration(300),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        25,\n        :Stop,\n    ],\n);\n\nInitial  | f(x): 10.647244 | |grad f(p)|:29.60035598883496 | s:1.0 | \n# 25     | f(x): 2.667564 | |grad f(p)|:0.8908455714609308 | s:0.01326670131422904 | Last Change: 0.763281\n# 50     | f(x): 2.650064 | |grad f(p)|:0.055369896074228944 | s:0.05306680525691616 | Last Change: 0.081780\n# 75     | f(x): 2.649707 | |grad f(p)|:0.02135638599842257 | s:0.01326670131422904 | Last Change: 0.011590\n# 100    | f(x): 2.649700 | |grad f(p)|:0.0012709553851854285 | s:0.02653340262845808 | Last Change: 0.001760\nAt iteration 112 the algorithm performed a step with a change (1.8130123627532867e-7) less than 1.0e-5.\n\nAnd we can again look at the result\n\nThe result looks as\n\n(Image: The resulting curve)\n\nThe resulting curve\n\nwhere all control points are evenly spaced and we hence have less acceleration as the final cost compared to the initial one indicates. Note that the cost is not zero, since we always have a tradeoff between approximating the initial junctinons (data points) and minimizing the acceleration.","category":"section"},{"location":"examples/Bezier-curves/#Technical-details","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:0:57.","category":"section"},{"location":"examples/Bezier-curves/#Literature","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Literature","text":"S. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\n","category":"section"},{"location":"data/#Data-sets","page":"Data","title":"Data sets","text":"","category":"section"},{"location":"data/#Signals-on-manifolds","page":"Data","title":"Signals on manifolds","text":"","category":"section"},{"location":"data/#images-on-manifolds","page":"Data","title":"images on manifolds","text":"","category":"section"},{"location":"data/#Literature","page":"Data","title":"Literature","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\n","category":"section"},{"location":"data/#ManoptExamples.Lemniscate-Tuple{Number}","page":"Data","title":"ManoptExamples.Lemniscate","text":"Lemniscate(t::Float; kwargs...)\nLemniscate(n::integer; interval=[0.0, 2π], kwargs...)\n\ngenerate the Lemniscate of Bernoulli as a curve on a manifold, by generating the curve emplying the keyword arguments below.\n\nTo be precise on the manifold M we use the tangent space at p and generate the curve\n\nγ(t) fracasin^2(t) + 1 beginpmatrix cos(t)  cos(t)sin(t) endpmatrix\n\nin the plane spanned by X and Y in the tangent space. Note that this curve is 2π-periodic and a is the half-width of the curve.\n\nTo reproduce the first examples from [BBSW16] as a default, on the sphere p defaults to the North pole.\n\nTHe second variant generates n points equispaced in ìnterval` and calls the first variant.\n\nKeywords\n\nmanifold - (Sphere(2)) the manifold to build the lemniscate on\np        - ([0.0, 0.0, 1.0] on the sphere, `rand(M) else) the center point of the Lemniscate\na        – (π/2.0) half-width of the Lemniscate\nX        – ([1.0, 0.0, 0.0] for the 2-sphere with default p, the first DefaultOrthonormalBasis() vector otherwise) first direction for the plane to define the Lemniscate in, unit vector recommended.\nY        – ([0.0, 1.0, 0.0] if p is the default, the second DefaultOrthonormalBasis() vector otherwise) second direction for the plane to define the Lemniscate in, unit vector orthogonal to X recommended.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_signal","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to ``[BLSW14].\n\nOptional\n\npts: (500) number of points to sample the function\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S1_signal-Tuple{Real}","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of  [BLSW14] for values outside that interval, this Signal is missing.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_slope_signal","page":"Data","title":"ManoptExamples.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the Circle with increasing slope.\n\nOptional\n\npts:    (500) number of points to sample the function.\nslope:  (4.0) initial slope that gets increased afterwards\n\nThis data set was introduced for the numerical examples in [BLSW14]\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_composite_Bezier_curve-Tuple{}","page":"Data","title":"ManoptExamples.artificial_S2_composite_Bezier_curve","text":"artificial_S2_composite_Bezier_curve()\n\nGenerate a composite Bézier curve on the [BG18].\n\nIt consists of 4 egments connecting the points\n\nmathbf d_0 = beginpmatrix 001endpmatrixquad\nmathbf d_1 = beginpmatrix 0-10endpmatrixquad\nmathbf d_2 = beginpmatrix -100endpmatrixtext and \nmathbf d_3 = beginpmatrix 00-1endpmatrix\n\nwhere instead of providing the two center control points explicitly we provide them as velocities from the corresponding points, such thtat we can directly define the curve to be C^1.\n\nWe define\n\nX_0 = fracπ8sqrt2beginpmatrix1-10endpmatrixquad\nX_1 = fracπ4sqrt2beginpmatrix101endpmatrixquad\nX_2 = fracπ4sqrt2beginpmatrix01-1endpmatrixtext and \nX_3 = fracπ8sqrt2beginpmatrix-110endpmatrix\n\nwhere we defined each X_i in T_d_imathbb S^2. We defined three BezierSegments\n\nof cubic Bézier curves as follows\n\nbeginalign*\nb_00 = d_0 quad  b_10 = exp_d_0X_0 quad  b_20 = exp_d_1X_1 quad  b_30 = d_1\nb_01 = d_1 quad  b_11 = exp_d_1(-X_1) quad  b_21 = exp_d_2X_2 quad  b_31 = d_2\nb_02 = d_2 quad  b_11 = exp_d_2(-X_2) quad  b_22 = exp_d_3X_3 quad  b_32 = d_3\nendalign*\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificialIn_SAR_image-Tuple{Integer}","page":"Data","title":"ManoptExamples.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\nThis data set was introduced for the numerical examples in [BLSW14].\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S2_rotation_image","page":"Data","title":"ManoptExamples.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\nCreate an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts:       (64) number of pixels along one dimension\nrotations: ((.5,.5)) number of total rotations performed on the axes.\n\nThis dataset was used in the numerical example of Section 5.1 of [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_image","page":"Data","title":"ManoptExamples.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts::Int=64])\n\nGenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts: (64) size of the image in pts×pts pixel.\n\nThis example dataset was used in the numerical example in Section 5.5 of [LNPS17]\n\nIt is based on artificial_S2_rotation_image extended by small whirl patches.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_patch","page":"Data","title":"ManoptExamples.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the pts×pts patch of Sphere(@ref)(2)-valued image data.\n\nThese patches are used within artificial_S2_whirl_image.\n\nOptional Parameters\n\npts: (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image","page":"Data","title":"ManoptExamples.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with a jump of size stepsize.\n\nThis dataset was used in the numerical example of Section 5.2 of [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image2","page":"Data","title":"ManoptExamples.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with right hand side fraction is moved upwards.\n\nThis data set was introduced in the numerical examples of Section of [BPS16]\n\n\n\n\n\n","category":"function"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#Row-sparse-Low-rank-Matrix-Recovery","page":"Row-Sparse Low-Rank Matrix Recovery","title":"Row-sparse Low-rank Matrix Recovery","text":"Paula John, Hajg Jasa 2025-10-01","category":"section"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#Introduction","page":"Row-Sparse Low-Rank Matrix Recovery","title":"Introduction","text":"In this example we use the Nonconvex Riemannian Proximal Gradient (NCRPG) method [BJJP25b] and compare it to the Riemannian Alternating Direction Method of Multipliers (RADMM) [LMS22]. This example reproduces the results from [BJJP25b], Section 6.3. The numbers may vary slightly due to having run this notebook on a different machine.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots, LaTeXStrings\nusing Random, LinearAlgebra, LRUCache, Distributions\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#The-Problem","page":"Row-Sparse Low-Rank Matrix Recovery","title":"The Problem","text":"Let \\mathcal M = \\mathcal M_r be the manifold of rank r matrices. Let g \\colon \\mathcal M \\to \\mathbb R be defined by\n\ng(X) = Vertmathbb A (X) - yVert_2^2\n\nwhere \\mathbb A \\colon \\mathbb R^{M \\times N} \\to \\mathbb R^m is a linear measurement operator.\n\nLet h \\colon \\mathcal M \\to \\mathbb R be defined by\n\nh(X) = mu Vert X Vert_1 2\n\nbe the row sparsity-enforcing term given by the \\ell_{1,2}-norm, where \\mu \\ge 0 is a regularization parameter.\n\nWe define our total objective function as f = g + h. The goal is to recover the (low-rank and row-sparse) signal X from as few measurements y as possible.","category":"section"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#Numerical-Experiment","page":"Row-Sparse Low-Rank Matrix Recovery","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\n# Set random seed for reproducibility\nrandom_seed = 1520\nBenchmarkTools.DEFAULT_PARAMETERS.seconds = 180.0\n\natol = 1e-7\nmax_iters = 5000\nc = 1e-4    # penalty parameter\nM = 500     # amount of rows\nN = 100     # amount of columns\ns = 10      # amount of non-zero rows\nr_m_array = [(1, 300), (2, 500), (3, 700)] # (rank, number of measurements)\nstep_size = 0.25\ninit_step_size_bt = 2 * step_size\nstop_NCRPG = atol\nstop_RADMM = stop_NCRPG * step_size\n\nfunction mean_error_zero_rows(Mr, X, zero_rows)\n    err = 0.0\n    for j in zero_rows\n        err += norm(X.U[j, :] .* X.S)\n    end\n    return err/length(zero_rows)\nend\n\nWe define a function to generate the test data for the Sparse PCA problem.\n\nfunction generate_data(M, N, r, m, s)\n    # Generate rank r matrix with s non-zero rows\n    X = rand(M, r) * transpose(Matrix(qr(rand(N, r)).Q[:, 1:r]))\n    smpl = sample(1:M, M - s , replace = false)\n    X[smpl, :] .= 0.0\n\n    # Normalize\n    X = X/norm(X)\n\n    # Generate measurement operator A and signal y\n    A = rand(Normal(0, 1/sqrt(m)), m, M * N)\n    y = A * vec(X)\n    return X, A, y, smpl\nend\n\nWe implement the proximal operators for the \\ell_{1, 2}-norm on the fixed-rank manifold, following [BJJP25b] and [LMS22].\n\n# NCRPG\nfunction prox_norm12_global(Mr::FixedRankMatrices, prox_param, X::SVDMPoint; c = c)\n    λ = prox_param * c\n    M, N, k = Manifolds.get_parameter(Mr.size)\n    YU = zeros(M, k)\n    for i in 1:M\n        normx1_i = norm(X.U[i, :] .* X.S)\n        if  normx1_i > λ\n            YU[i, :] = ((normx1_i - λ)/normx1_i) * X.U[i, :]\n        end\n    end\n    Z = SVDMPoint(YU * diagm(X.S))\n    return SVDMPoint(Z.U, Z.S, Z.Vt * X.Vt)\nend\n#\n#RADMM\nfunction prox_norm12_global(Mr::FixedRankMatrices, prox_param, X1::Matrix{Float64}; c = c)\n    λ = prox_param * c\n    M, N, k = Manifolds.get_parameter(Mr.size)\n    Y1 = zeros(M, N)\n    for i in 1:M\n        normx1_i = norm(X1[i, :])\n        if  normx1_i > λ\n            Y1[i, :] = ((normx1_i - λ)/normx1_i) * X1[i, :]\n        end\n    end\n    return Y1\nend\n\nNext, we define the objective function, its gradient, and the proximal operator for the \\ell_{1,2}-norm on the fixed-rank manifold.\n\n# Objective(s), gradient, and proxes\nfunction norm12(X::SVDMPoint)\n    M = size(X.U)[1]\n    sum([norm(X.U[i, :] .* X.S) for i = 1:M])\nend\nfunction f_global(M::FixedRankMatrices, X::SVDMPoint, A, c, y, max_cost = 1e5)\n    X_vec = vec(embed(M, X))\n    cost =  0.5 * (norm(A*X_vec - y))^2 + c * norm12(X)\n    if cost >= max_cost\n        return NaN\n    else\n        return cost\n    end\nend\n#\ng_global(M::FixedRankMatrices, X::SVDMPoint, A, c, y) = 0.5*(norm(A*vec(embed(M, X)) - y))^2\nfunction grad_g_global(M::FixedRankMatrices, X::SVDMPoint, A, c, y)\n    X_mat = embed(M, X)\n    return project(M, X, reshape(A'*(A*vec(X_mat) - y), size(X_mat)))\nend\nh_global(M::FixedRankMatrices, X::SVDMPoint, A, c, y) = c * norm12(X)\n\nWe introduce an implementation of the RADMM method for the Row-sparse Low-rank Matrix Recovery problem on the oblique manifold, following [LMS22].\n\n\"\"\"\n\\argmin F(X) = 0.5||AX-y||^2 + c *||X||_{1,2}\nf(X) = 0.5||AX - y||^2\ng(X) = c * ||X||_{1,2}\nL_{ρ,γ}(X, Z, Λ) = f(X) + g_γ(Z) + <Λ, X - Z> + ρ/2 * ||X - Z||^2\ngrad_X L_{ρ,γ}(X, Z, Λ) = project(A'(AX - y) + Λ + ρ(X - Z))\n\n\"\"\"\nfunction RADMM_blind_deconv(\n    A,\n    M, # rows\n    N, # columns\n    rank,\n    c,  # penalty parameter\n    y, # signal\n    prox_l12;\n    η = 1e-1,\n    ρ = 0.1,\n    γ = 1e-7,\n    stop = 1e-8,\n    max_iters  = 100,\n    start = 0,\n    record = false,\n    ϵ_spars = 1e-5,\n    max_cost = 1e5\n)\n    flag_succ = false\n    Mr = FixedRankMatrices(M, N, rank)\n    γρ = γ * ρ\n    F(X) = 0.5 * norm(A*vec(X) - y)^2 + c * sum([norm(X[i, :]) for i=1:M])\n    grad_augLagr(X::SVDMPoint, X_mat::Matrix, Λ::Matrix, Z::Matrix) = project(Mr, X, reshape(A'*(A*vec(X_mat) - y) + vec(Λ + ρ*(X_mat - Z)), (M, N)))\n\n    if start == 0\n        X = rand(Mr)\n    else\n        X = start\n    end\n    X_mat = embed(Mr, X)\n    Z = X_mat\n    Λ = zeros(M, N)\n    it = -1\n    if !record\n        for i in 1:max_iters\n            descent_dir = -η * grad_augLagr(X, X_mat, Λ, Z)\n            X = retract(Mr, X, descent_dir)\n            X_mat = embed(Mr, X)\n            Y = prox_l12(Mr, (1 + γρ)/ρ , X_mat + 1/ρ * Λ; c = c)\n            Z = (1/γ * Y + Λ + ρ * X_mat)/ (1/γ + ρ)\n            Λ = Λ + ρ * (X_mat - Z)\n            if (norm(embed(Mr, X, descent_dir)) < stop)\n                flag_succ = true\n                it = i\n                break\n            end\n        end\n        if it == -1\n            it = max_iters\n        end\n        return X, flag_succ, it\n    else\n        Iterates = []\n        for i in 1:max_iters\n            descent_dir = -η * grad_augLagr(X, X_mat, Λ, Z)\n            X = retract(Mr, X, descent_dir)\n            push!(Iterates, X)\n            X_mat = embed(Mr, X)\n            Y = prox_l12(Mr, (1 + γρ)/ρ , X_mat + 1/ρ * Λ; c = c)\n            Z = (1/γ * Y + Λ + ρ * X_mat)/ (1/γ + ρ)\n            Λ = Λ + ρ * (X_mat - Z)\n            if (norm(embed(Mr, X, descent_dir)) < stop)\n                flag_succ = true\n                it = i\n                break\n            end\n            # if i%100 == 0\n            #     println(i, \"\\t\", norm(embed(Mr, X, descent_dir)))\n            # end\n        end\n        if it == -1\n            it = max_iters\n        end\n        return X, flag_succ, it, Iterates\n    end\nend\n\nWe set up some variables to collect the results of the experiments and initialize the dataframes\n\nAnd run the experiments\n\nfor (r, m) in r_m_array\n    # Set random seed for reproducibility\n    Random.seed!(random_seed)\n    #\n    # Define manifold\n    Mr = FixedRankMatrices(M, N, r) #fixed rank manifold\n    #\n    # Generate rank r matrix with s non-zero rows\n    Sol_mat, A, y, zero_rows = generate_data(M, N, r, m, s)\n    Sol = SVDMPoint(Sol_mat, r)\n    # Local starting point\n    Y = rand(Normal(0, 0.01/sqrt(r)), M, N)\n    start = SVDMPoint(Sol_mat + Y, r)\n    dist_start_sol = distance(Mr, Sol, start, OrthographicInverseRetraction())\n    # Localize objectives\n    f(Mr, X) = f_global(Mr, X, A, c, y)\n    g(Mr, X) = g_global(Mr, X, A, c, y)\n    grad_g(Mr, X) = grad_g_global(Mr, X, A, c, y)\n    h(Mr, X) = h_global(Mr, X, A, c, y)\n    prox_norm12(Mr, prox_param, X) = prox_norm12_global(Mr, prox_param, X; c = c)\n    #\n    # Optimization\n    # NCRPG\n    rec_NCRPG = proximal_gradient_method(Mr, f, g, grad_g, start;\n        prox_nonsmooth=prox_norm12,\n        retraction_method=OrthographicRetraction(),\n        inverse_retraction_method=OrthographicInverseRetraction(),\n        stepsize = ConstantLength(step_size),\n        record=[:Iteration, :Iterate],\n        return_state=true,\n        debug=[\n            :Iteration,( \"|Δp|: %1.9f |\"),\n            DebugChange(; inverse_retraction_method= OrthographicInverseRetraction()),\n            (:Cost, \" F(x): %1.11f | \"),\n            \"\\n\",\n            :Stop,\n            100\n        ],\n        stopping_criterion =  StopAfterIteration(max_iters )|StopWhenGradientMappingNormLess(stop_NCRPG)\n    )\n    bm_NCRPG = @benchmark proximal_gradient_method($Mr, $f, $g, $grad_g, $start;\n        prox_nonsmooth=$prox_norm12,\n        retraction_method=OrthographicRetraction(),\n        inverse_retraction_method=OrthographicInverseRetraction(),\n        stepsize = ConstantLength($step_size),\n        stopping_criterion = StopAfterIteration($max_iters )|StopWhenGradientMappingNormLess($stop_NCRPG)\n    )\n    it_NCRPG, res_NCRPG = get_record(rec_NCRPG)[end]\n    # NCRPG with backtracking\n    rec_NCRPG_bt = proximal_gradient_method(Mr, f, g, grad_g, start;\n        prox_nonsmooth=prox_norm12,\n        retraction_method=OrthographicRetraction(),\n        inverse_retraction_method=OrthographicInverseRetraction(),\n        stepsize = ProximalGradientMethodBacktracking(;\n            strategy=:nonconvex,\n            initial_stepsize=init_step_size_bt\n        ),\n        record=[:Iteration, :Iterate],\n        return_state=true,\n        debug=[\n            :Iteration,( \"|Δp|: %1.9f |\"),\n            DebugChange(; inverse_retraction_method=OrthographicInverseRetraction()),\n            (:Cost, \" F(x): %1.11f | \"),\n            \"\\n\",\n            :Stop,\n            100\n        ],\n        stopping_criterion = StopAfterIteration(max_iters )|    StopWhenGradientMappingNormLess(stop_NCRPG)\n    )\n    bm_NCRPG_bt = @benchmark proximal_gradient_method($Mr, $f, $g, $grad_g, $start;\n        prox_nonsmooth=$prox_norm12,\n        retraction_method=OrthographicRetraction(),\n        inverse_retraction_method=OrthographicInverseRetraction(),\n        stepsize = ProximalGradientMethodBacktracking(; strategy=:nonconvex, initial_stepsize=$init_step_size_bt),\n        stopping_criterion = StopAfterIteration($max_iters )|  StopWhenGradientMappingNormLess($stop_NCRPG)\n    )\n    it_NCRPG_bt, res_NCRPG_bt = get_record(rec_NCRPG_bt)[end]\n    # RADMM\n    res_RADMM, succ, it_RADMM = RADMM_blind_deconv(A, M, N, r, c, y, prox_norm12_global;\n        max_iters  = max_iters ,\n        start = start,\n        η = step_size,\n        stop = stop_RADMM\n    )\n    bm_RADMM = @benchmark RADMM_blind_deconv($A, $M, $N, $r, $c, $y, $prox_norm12_global;\n        max_iters  = max_iters ,\n        start = $start,\n        η = $step_size,\n        stop = stop_RADMM\n    )\n    #\n    # Collect results\n    # Distances between the results\n    dist_NCRPG_RADMM = distance(Mr, res_NCRPG, res_RADMM, OrthographicInverseRetraction())\n    dist_NCRPG_bt_RADMM = distance(Mr, res_NCRPG_bt, res_RADMM, OrthographicInverseRetraction())\n    dist_NCRPG_NCRPG_bt = distance(Mr, res_NCRPG, res_NCRPG_bt, OrthographicInverseRetraction())\n    # Times\n    time_RADMM    = median(bm_RADMM   ).time/1e9\n    time_NCRPG    = median(bm_NCRPG   ).time/1e9\n    time_NCRPG_bt = median(bm_NCRPG_bt).time/1e9\n    # Errors\n    error_RADMM    = distance(Mr, Sol, res_RADMM,    OrthographicInverseRetraction())\n    error_NCRPG    = distance(Mr, Sol, res_NCRPG,    OrthographicInverseRetraction())\n    error_NCRPG_bt = distance(Mr, Sol, res_NCRPG_bt, OrthographicInverseRetraction())\n    # Mean zero row errors\n    mean_zero_row_error_NCRPG    = mean_error_zero_rows(Mr, res_NCRPG, zero_rows    )\n    mean_zero_row_error_NCRPG_bt = mean_error_zero_rows(Mr, res_NCRPG_bt, zero_rows )\n    mean_zero_row_error_RADMM    = mean_error_zero_rows(Mr, res_RADMM, zero_rows    )\n    #\n    # Push results to dataframes\n    push!(df_RADMM,\n        [\n            M, N, m, r, s,\n            step_size,\n            time_RADMM,\n            error_RADMM,\n            it_RADMM,\n            mean_zero_row_error_RADMM\n        ]\n    )\n    push!(df_NCRPG,\n        [\n            M, N, m, r, s,\n            step_size,\n            time_NCRPG,\n            error_NCRPG,\n            it_NCRPG,\n            mean_zero_row_error_NCRPG\n        ]\n    )\n    push!(df_NCRPG_bt,\n        [\n            M, N, m, r, s,\n            init_step_size_bt,\n            time_NCRPG_bt,\n            error_NCRPG_bt,\n            it_NCRPG_bt,\n            mean_zero_row_error_NCRPG_bt\n        ]\n    )\n    push!(df_distances,\n        [\n            M, N, m, r, s,\n            dist_NCRPG_NCRPG_bt,\n            dist_NCRPG_RADMM,\n            dist_NCRPG_bt_RADMM\n        ]\n    )\nend\n\nWe export the results to CSV files\n\n<details class=\"code-fold\"> <summary>Code</summary>\n\nCSV.write(joinpath(results_folder, \"results-fixed-rank-RADMM.csv\"), df_RADMM)\nCSV.write(joinpath(results_folder, \"results-fixed-rank-NCRPG.csv\"), df_NCRPG)\nCSV.write(joinpath(results_folder, \"results-fixed-rank-NCRPG-bt.csv\"), df_NCRPG_bt)\nCSV.write(joinpath(results_folder, \"results-fixed-rank-distances.csv\"), df_distances )\n\n</details>\n\nWe can take a look at how the algorithms compare to each other in their performance with the following tables. The first table shows the performance RADMM.\n\nM N m r s stepsize time (s) error iterations mean zero row error\n500.0 100.0 300.0 1.0 10.0 0.25 10.2648 0.00052812 1431.0 5.37441e-9\n500.0 100.0 500.0 2.0 10.0 0.25 16.7196 0.000725431 1354.0 6.09862e-9\n500.0 100.0 700.0 3.0 10.0 0.25 19.2542 0.000772805 1414.0 7.4266e-9\n\nThe next table shows the performance of NCRPG with a constant stepsize.\n\n| **M** | **N** | **m** | **r** | **s** | **stepsize** | **time (s)** |   **error** | **iterations** | **mean zero row error** |\n|------:|------:|------:|------:|------:|-------------:|-------------:|------------:|---------------:|------------------------:|\n| 500.0 | 100.0 | 300.0 |   1.0 |  10.0 |         0.25 |      7.75729 | 0.000528145 |         1049.0 |                     0.0 |\n| 500.0 | 100.0 | 500.0 |   2.0 |  10.0 |         0.25 |      11.6917 | 0.000725859 |         1047.0 |             1.58429e-21 |\n| 500.0 | 100.0 | 700.0 |   3.0 |  10.0 |         0.25 |      16.4164 | 0.000775127 |         1120.0 |             6.52608e-21 |\n\nThe next table shows the performance of NCRPG with a backtracked stepsize. In this case, the column “stepsize” indicates the initial stepsize for the backtracking procedure.\n\n| **M** | **N** | **m** | **r** | **s** | **stepsize** | **time (s)** |   **error** | **iterations** | **mean zero row error** |\n|------:|------:|------:|------:|------:|-------------:|-------------:|------------:|---------------:|------------------------:|\n| 500.0 | 100.0 | 300.0 |   1.0 |  10.0 |          0.5 |      8.07119 | 0.000528144 |          562.0 |                     0.0 |\n| 500.0 | 100.0 | 500.0 |   2.0 |  10.0 |          0.5 |      15.0202 | 0.000725847 |          604.0 |             3.19594e-21 |\n| 500.0 | 100.0 | 700.0 |   3.0 |  10.0 |          0.5 |      1325.19 | 0.000778709 |         5000.0 |             3.79913e-20 |\n\nSecond, we look at the distances of the solutions found by each algorithm.\n\nM N m r s distNCRPGNCRPG_bt distNCRPGRADMM distNCRPGNCRPG_bt\n500.0 100.0 300.0 1.0 10.0 1.08617e-8 5.59207e-7 5.49924e-7\n500.0 100.0 500.0 2.0 10.0 2.18404e-8 7.53362e-7 7.33125e-7\n500.0 100.0 700.0 3.0 10.0 1.38496e-5 2.39003e-5 2.51754e-5","category":"section"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#Technical-details","page":"Row-Sparse Low-Rank Matrix Recovery","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 15, 2025, 18:40:14.","category":"section"},{"location":"examples/NCRPG-Row-Sparse-Low-Rank/#Literature","page":"Row-Sparse Low-Rank Matrix Recovery","title":"Literature","text":"R. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Nononvex Optimization, preprint (2025), arXiv:2506.09775.\n\n\n\nW. Huang and K. Wei. Riemannian proximal gradient methods. Mathematical Programming 194, 371–413 (2021).\n\n\n\n","category":"section"},{"location":"examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","page":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","text":"Ronny Bergmann, Laura Weigl 2023-07-02\n\nFor this example we first load the necessary packages.\n\nusing Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,\n\nusing LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)","category":"section"},{"location":"examples/Robust-PCA/#Computing-a-Robust-PCA","page":"Robust PCA","title":"Computing a Robust PCA","text":"For a given matrix D  ℝ^dn whose columns represent points in ℝ^d, a matrix p  ℝ^dm is computed for a given dimension m  n: p represents an ONB of ℝ^dm such that the column space of p approximates the points (columns of D), i.e. the vectors D_i as well as possible.\n\nWe compute p as a minimizer over the Grassmann manifold of the cost function:\n\nbeginsplit\nf(p)\n = frac1nsum_i=1^noperatornamedist(D_i operatornamespan(p))\n\n = frac1n sum_i=1^nlVert pp^TD_i - D_irVert\nendsplit\n\nThe output cost represents the average distance achieved with the returned p, an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that f is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter ε.\n\nf_ϵ(p) = frac1n sum_i=1^nℓ_ϵ(lVert pp^mathrmTD_i - D_irVert)\n\nwhere ℓ_ϵ(x) = sqrtx^2 + ϵ^2 - ϵ.\n\nThe smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts).\n\nFirst, we generate random data. For illustration purposes we take points in mathbb R^2 and m=1, that is we aim to find a robust regression line.\n\nn = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1\n\nWe use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure.\n\nM = Grassmann(d,m);\n\nFor the initial matrix p_0 we use classical PCA via singular value decomposition. Thus, we use the first d left singular vectors.\n\nThen, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in Manopt.jl.\n\nFurthermore the cost and gradient are implemented in ManoptExamples.jl. Since these are Huber regularized, both functors have the ϵ as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection pp^T - I, which converts the Euclidean to the Riemannian gradient.\n\nThe trust-region method also requires the Hessian Matrix. By using ApproxHessianFiniteDifference using a finite difference scheme we get an approximation of the Hessian Matrix.\n\nWe run the procedure several times, where the smoothing parameter ε is reduced iteratively.\n\nε = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m]\n\n2×1 Matrix{Float64}:\n -0.14057143527189225\n  0.9900705386918653\n\nLet’s generate the cost and gradient we aim to use here\n\nf = ManoptExamples.RobustPCACost(M, data, ε)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, ε)\n\nManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([0.7726062030458439 1.6583418797018163 … 30.833523701909474 30.512999245062304; -0.8954212160206203 -1.7120433539256108 … -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])\n\nand check the initial cost\n\nf(M, p0)\n\n16.158396619704558\n\nNow we iterate the opimization with reducing ε after every iteration, which we update in f and grad_f.\n\nq = copy(M, p0)\nεi = ε\nfor i in 1:iterations\n    f.ε = εi\n    grad_f.ε = εi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global εi *= reduction\nend\n\nWhen finally setting ε we can investigate the final cost\n\nf.ε = 0.0\nf(M, q)\n\n12.890044794413498\n\nFinally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines.\n\nfig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)\n\n(Image: )","category":"section"},{"location":"examples/Robust-PCA/#Technical-details","page":"Robust PCA","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:11:57.","category":"section"},{"location":"examples/Total-Variation/#Total-Variation-Minimization","page":"Total Variation","title":"Total Variation Minimization","text":"Ronny Bergmann 2023-06-06","category":"section"},{"location":"examples/Total-Variation/#Introduction","page":"Total Variation","title":"Introduction","text":"Total Variation denoising is an optimization problem used to denoise signals and images. The corresponding (Euclidean) objective is often called Rudin-Osher-Fatemi (ROF) model based on the paper [ROF92].\n\nThis was generalized to manifolds in [WDS14]. In this short example we will look at the ROF model for manifold-valued data, its generalizations, and how they can be solved using Manopt.jl.","category":"section"},{"location":"examples/Total-Variation/#The-manifold-valued-ROF-model","page":"Total Variation","title":"The manifold-valued ROF model","text":"Generalizing the ROF model to manifolds can be phrased as follows: Given a (discrete) signal on a manifold s = (s_i)_i=1^N in mathbb M^n of length n in mathbb N, we usually assume that this signal might be noisy. For the (Euclidean) ROF model we assume that the noise is Gaussian. Then variational models for denoising usually consist of a data term D(ps) to “stay close to” s and a regularizer R(p). For TV regularization the data term is the squared distance and the regularizer models that without noise, neighboring values are close. We obtain\n\noperatorname*argmin_pinmathcal M^n\nf(p)\nqquad\nf(p) = D(ps) + α R(p) = sum_i=1^n d_mathcal M^2(s_ip_i) + αsum_i=1^n-1 d_mathcal M(p_ip_i+1)\n\nwhere α  0 is a weight parameter.\n\nThe challenge here is that most classical algorithm, like gradient descent or Quasi Newton, assume the cost f(p) to be smooth such that the gradient exists at every point. In our setting that is not the case since the distacen is not differentiable for any p_i=p_i+1. So we have to use another technique.","category":"section"},{"location":"examples/Total-Variation/#The-Cyclic-Proximal-Point-algorithm","page":"Total Variation","title":"The Cyclic Proximal Point algorithm","text":"If the cost consists of a sum of functions, where each of the proximal maps is “easy to evaluate”, for best of cases in closed form, we can “apply the proximal maps in a cyclic fashion” and optain the Cyclic Proximal Point Algorithm [Bac14].\n\nBoth for the distance and the squared distance, we have generic implementations; since this happens in a cyclic manner, there is also always one of the arguments involved in the prox and never both. We can improve the performance slightly by computing all proes in parallel that do not interfer. To be precise we can compute first all proxes of distances in the regularizer that start with an odd index in parallel. Afterwards all that start with an even index.","category":"section"},{"location":"examples/Total-Variation/#The-Optimsation","page":"Total Variation","title":"The Optimsation","text":"using Manifolds, Manopt, ManoptExamples, ManifoldDiff\nusing ManifoldDiff: prox_distance\nusing ManoptExamples: prox_Total_Variation\nn = 500 #Signal length\nσ = 0.2 # amount of noise\nα = 0.5# in the TV model\n\nWe define a few colors\n\nusing Colors, NamedColors, ColorSchemes, Plots, Random\ndata_color = RGBA{Float64}(colorant\"black\")\nlight_color = RGBA{Float64}(colorant\"brightgrey\")\nrecon_color = RGBA{Float64}(colorant\"vibrantorange\")\nnoisy_color = RGBA{Float64}(colorant\"vibrantteal\")\n\nAnd we generate our data on the Circle, since that is easy to plot and nice to compare to the Euclidean case of a real-valued signal.\n\nRandom.seed!(23)\nM = Circle()\nN = PowerManifold(M, n)\ndata = ManoptExamples.artificial_S1_signal(n)\ns = [exp(M, d, rand(M; vector_at=d, σ=0.2)) for d in data]\nt = range(0.0, 1.0; length=n)\nscene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=noisy_color,\n    markerstrokecolor=noisy_color,\n    lab=\"noisy\",\n)\nyticks!(\n    [-π, -π / 2, 0, π / 2, π],\n    [raw\"$-\\pi$\", raw\"$-\\frac{\\pi}{2}$\", raw\"$0$\", raw\"$\\frac{\\pi}{2}$\", raw\"$\\pi$\"],\n)\n\n(Image: )\n\nAs mentioned above, total variation now minimized different neighbors – while keeping jumps if the are large enough. One notable difference between Euclidean and Cyclic data is, that the y-axis is in our case periodic, hence the first jump is actually not a jump but a “linear increase” that “wraps around” and the second large jump –or third overall– is actually only as small as the second jump.\n\nDefining cost and the proximal maps, which are actually 3 proxes to be precise.\n\nf(N, p) = ManoptExamples.L2_Total_Variation(N, s, α, p)\nproxes_f = ((N, λ, p) -> prox_distance(N, λ, s, p, 2), (N, λ, p) -> prox_Total_Variation(N, α * λ, p))\n\nWe run the algorithm\n\no = cyclic_proximal_point(\n    N,\n    f,\n    proxes_f,\n    s;\n    λ=i -> π / (2 * i),\n    debug=[\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        :Cost,\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    record=[:Iteration, :Cost, :Change, :Iterate],\n    return_state=true,\n);\n\nInitial  | λ:Inf | f(x): 59.187445 | \n# 1000   | λ:0.0015707963267948967 | f(x): 13.963912 | Last Change: 1.773283\n# 2000   | λ:0.0007853981633974483 | f(x): 13.947124 | Last Change: 0.011678\n# 3000   | λ:0.0005235987755982988 | f(x): 13.941538 | Last Change: 0.003907\n# 4000   | λ:0.00039269908169872416 | f(x): 13.938748 | Last Change: 0.001957\n# 5000   | λ:0.0003141592653589793 | f(x): 13.937075 | Last Change: 0.001175\nAt iteration 5000 the algorithm reached its maximal number of iterations (5000).\n\nWe can see that the cost reduces nicely. Let’s extract the result an the recorded values\n\nrecon = get_solver_result(o)\nrecord = get_record(o)\n\nWe get\n\nscene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=light_color,\n    markerstrokecolor=light_color,\n    lab=\"noisy\",\n)\nscatter!(\n    scene,\n    t,\n    recon;\n    markersize=2,\n    markercolor=recon_color,\n    markerstrokecolor=recon_color,\n    lab=\"reconstruction\",\n)\n\n(Image: )\n\nWhich contains the usual stair casing one expects for TV regularization, but here in a “cyclic manner”","category":"section"},{"location":"examples/Total-Variation/#Outlook","page":"Total Variation","title":"Outlook","text":"We can generalize the total variation also to a second order total variation. Again intuitively, while TV prefers constant areas, the operatornameTV_2 yields a cost 0 for anything linear, which on manifolds can be generalized to equidistant on a geodesic [BBSW16]. Here we can again derive proximal maps, which for the circle again have a closed form solutoin [BLSW14] but on general manifolds these have again to be approximated.\n\nAnother extension for both first and second order TV is to apply this for manifold-valued images S = (S_ij)_ij=1^mn in mathcal M^mn, where the distances in the regularizer are then used in both the first dimension i and the second dimension j in the data.","category":"section"},{"location":"examples/Total-Variation/#Technical-details","page":"Total Variation","title":"Technical details","text":"This version of the example was generated with the following package versions.\n\nPkg.status()\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0","category":"section"},{"location":"examples/Total-Variation/#Literature","page":"Total Variation","title":"Literature","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"section"},{"location":"examples/Rosenbrock/#The-Rosenbrock-Function","page":"Rosenbrock","title":"The Rosenbrock Function","text":"Ronny Bergmann 2023-01-03\n\nAfter loading the necessary packages\n\nusing Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,\n\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\n\nWe fix the parameters for the 📖 Rosenbrock (where the wikipedia page has a slightly different parameter naming).\n\na = 100.0\nb = 1.0\np0 = [1/10, 2/10]\n\nwhich is defined on mathbb R^2, so we need\n\nM = ℝ^2\n\nEuclidean(2; field=ℝ)\n\nand can then generate both the cost and the gradient\n\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)\n\nManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0)\n\nFor comparison, we look at the initial cost\n\nf(M, p0)\n\n4.42\n\nAnd to illustrate, we run two small solvers with their default settings as a comparison.","category":"section"},{"location":"examples/Rosenbrock/#Gradient-Descent","page":"Rosenbrock","title":"Gradient Descent","text":"We start with the gradient descent solver.\n\nSince we need the state anyways to access the record, we also get from the return_state=true a short summary of the solver run.\n\ngd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true)\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0,\n    retraction_method=ManifoldsBase.ExponentialRetraction(),\n    contraction_factor=0.95,\n    sufficient_decrease=0.1,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  reached\n  * |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)\n\nFrom the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost\n\ngd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state))\n\n0.10562873187751265","category":"section"},{"location":"examples/Rosenbrock/#Quasi-Newton","page":"Rosenbrock","title":"Quasi Newton","text":"We can improve this using the quasi Newton algorithm\n\nqn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n)\n\n# Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 26 iterations\n\n## Parameters\n* direction update:        limited memory Manopt.InverseBFGS (size 2) initial scaling 1.0and ParallelTransport() as vector transport.\n* retraction method:       ManifoldsBase.ExponentialRetraction()\n* vector transport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(;\n    sufficient_decrease = 0.0001,\n    sufficient_curvature = 0.999,\n    retraction_method = ManifoldsBase.ExponentialRetraction(),\n    vector_transport_method = ParallelTransport(),\n    stop_when_stepsize_less = 1.0e-10,\n    stop_increasing_at_step = 100,\n    stop_decreasing_at_step = 1000,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 1000: not reached\n  * |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)\n\nAnd we see it stops far earlier, after 45 Iterations. We again collect the recorded values\n\nqn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state))\n\n1.4404666436813376e-18\n\nand see that the final value is close to the one of the minimizer\n\nf(M, ManoptExamples.minimizer(f))\n\n0.0\n\nwhich we also see if we plot the recorded cost.\n\nfig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")\n\n(Image: )","category":"section"},{"location":"examples/Rosenbrock/#Technical-Details","page":"Rosenbrock","title":"Technical Details","text":"Status `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:12:20.","category":"section"},{"location":"examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","page":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","text":"Ronny Bergmann 2023-07-02","category":"section"},{"location":"examples/Riemannian-mean/#Preliminary-Notes","page":"Riemannian Mean","title":"Preliminary Notes","text":"Each of the example objectives or problems stated in this package should be accompanied by a Quarto notebook that illustrates their usage, like this one.\n\nFor this first example, the objective is a very common one, for example also used in the 🏔️ Get started with Manopt.jl tutorial of Manopt.jl.\n\nThe second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way.\n\nThere are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in examples/. If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having ManoptExamples.jl in development mode. this requires that your example does not have any (additional) dependencies beyond the ones ManoptExamples.jl has anyways.\n\nFor registered versions of ManoptExamples.jl use the environment of examples/ and – under development – add ManoptExamples.jl in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the examples/ environment again.\n\nusing Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"section"},{"location":"examples/Riemannian-mean/#Loading-packages-and-defining-data","page":"Riemannian Mean","title":"Loading packages and defining data","text":"Loading the necessary packages and defining a data set on a manifold\n\nusing ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nσ = π / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];","category":"section"},{"location":"examples/Riemannian-mean/#Variant-1:-Using-the-functions","page":"Riemannian Mean","title":"Variant 1: Using the functions","text":"We can define both the cost and gradient, RiemannianMeanCost and RiemannianMeanGradient!!, respectively. For their mathematical derivation and further explanations, we again refer to 🏔️ Get started with Manopt.jl.\n\nf = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data)\n\nThen we can for example directly call a gradient descent as\n\nx1 = gradient_descent(M, f, grad_f, first(data))\n\n3-element Vector{Float64}:\n 0.6868392793937866\n 0.006531600741910336\n 0.7267799821634966","category":"section"},{"location":"examples/Riemannian-mean/#Variant-2:-Using-the-objective","page":"Riemannian Mean","title":"Variant 2: Using the objective","text":"A shorter way to directly obtain the Manifold objective including these two functions. Here, we want to specify that the objective can do in-place-evaluations using the evaluation=-keyword. The objective can be obtained calling Riemannian_mean_objective as\n\nrmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n)\n\nTogether with a manifold, this forms a Manopt Problem, which would usually enable to switch manifolds between solver runs. Here we could for example switch to using Euclidean(3) instead for the same data the objective is build upon.\n\nrmp = DefaultManoptProblem(M, rmo)\n\nThis enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface\n\ns1 = GradientDescentState(M; p=copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1)\n\n3-element Vector{Float64}:\n 0.6868392793937866\n 0.006531600741910336\n 0.7267799821634966\n\nbut we can easily use a conjugate gradient instead\n\ns2 = ConjugateGradientDescentState(\n    M;\n    p=copy(M, first(data)),\n    stopping_criterion=StopAfterIteration(100),\n    stepsize=ArmijoLinesearch(M)(),\n    coefficient=FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2)\n\n3-element Vector{Float64}:\n 0.6868393592500729\n 0.006531543071382776\n 0.7267799072140452","category":"section"},{"location":"examples/Riemannian-mean/#Technical-details","page":"Riemannian Mean","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:11:28.","category":"section"},{"location":"examples/Constrained-Mean-Hn/#The-Constrained-mean-on-high-dimensional-Hyperbolic-space.","page":"Mean on mathbb H^n","title":"The Constrained mean on high-dimensional Hyperbolic space.","text":"Ronny Bergmann 2027-03-03","category":"section"},{"location":"examples/Constrained-Mean-Hn/#Introduction","page":"Mean on mathbb H^n","title":"Introduction","text":"In this example we compare the Projected Gradient Algorithm (PGA) as introduced in [BFNZ25] with both the Augmented Lagrangian Method (ALM) and the Exact Penalty Method (EPM) [LB19].\n\nusing Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random\n\nConsider the constrained Riemannian center of mass for a given set of points ``q_i M$ i=1ldotsN given by\n\noperatorname*argmin_pinmathcal C\nsum_i=1^N d_mathrmM^2(pq_i)\n\nconstrained to a set mathcal C subset mathcal M.\n\nFor this experiment set mathcal M = mathbb H^d for d=2ldots200, the Hyperbolic space and the constrained set mathcal C = C_cr as the ball of radius r around the center point c, where we choose here r=frac1sqrtn and c = (0ldots01)^mathrmT and a σ = frac32n^14.\n\nn_range = Vector(2:200)\nradius_range = [1 / sqrt(n) for n in n_range]\nN_range = [400 for n ∈ n_range]\nM_range = [Hyperbolic(n) for n ∈ n_range]\nσ_range = [ 1.5/sqrt(sqrt(n-1)) for n ∈ n_range]\n\nThe data consists of N=200 points, where we skew the data a bit to force the mean to be outside of the constrained set mathcal C.","category":"section"},{"location":"examples/Constrained-Mean-Hn/#Cost,-gradient-and-projection","page":"Mean on mathbb H^n","title":"Cost, gradient and projection","text":"We can formulate the constrained problem above in two different forms. Both share a cost and require a gradient. For performance reasons, we also provide a mutating variant of the gradient\n\nf(M, p; pts=[]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts)\n\ngrad_f(M, p; pts=[]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts)\n\nfunction grad_f!(M, X, p; pts=[])\n    zero_vector!(M, X, p)\n    Y = zero_vector(M, p)\n    for q in pts\n        log!(M, Y, p, q)\n        X .+= Y\n    end\n    X .*= -1 / length(pts)\n    return X\nend\n\nWe can model the constrained either with an inequality constraint g(p) geq 0 or using a projection onto the set. For the gradient of g and the projection we again also provide mutating variants.\n\ng(M, p; op=[], radius=1) = distance(M, op, p)^2 - radius^2;\nindicator_C(M, p; op=[], radius=1) = (g(M, p; op=op, radius=radius) ≤ 0) ? 0 : Inf;\n\nfunction project_C(M, p; op=[], radius=1)\n    X = log(M, op, p)\n    n = norm(M, op, X)\n    q = (n > radius) ? exp(M, op, (radius / n) * X) : copy(M, p)\n    return q\nend;\n\nfunction project_C!(M, q, p; radius=1, op=[], X=zero_vector(M, op))\n    log!(M, X, op, p)\n    n = norm(M, op, X)\n    if (n > radius)\n        exp!(M, q, op, (radius / n) * X)\n    else\n        copyto!(M, q, p)\n    end\n    return q\nend;\n\ngrad_g(M, p; op=[]) = -2 * log(M, p, op)\nfunction grad_g!(M, X, p; op=[])\n    log!(M, X, p, op)\n    X .*= -2\n    return X\nend","category":"section"},{"location":"examples/Constrained-Mean-Hn/#The-mean","page":"Mean on mathbb H^n","title":"The mean","text":"For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to mathcal C. We can then project this onto mathcal C. For the projected mean we obtain g(p) = 0 since the original mean is outside of the set, the projected one lies on the boundary.\n\nWe first generate all data\n\ncenters = [[zeros(n)..., 1.0] for n in n_range]\nbegin\n    Random.seed!(5)\n    data = [\n        [\n            exp(\n                M,\n                c,\n                get_vector(\n                    M, c, σ * randn(n) .+ 2 * r .* ones(n), DefaultOrthonormalBasis()\n                ),\n            ) for _ in 1:N\n        ] for\n        (c, r, n, N, M, σ) in zip(centers, radius_range, n_range, N_range, M_range, σ_range)\n    ]\nend\n\nmeans = [mean(M, d) for (M, d) in zip(M_range, data)]\ndc = [\n    indicator_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\nminimum(dc) # Sanity Check, this should be inf\n\nInf\n\nProj_means = [\n    project_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\n# Samll sanity check, these should all be about zero\nds = [distance(M, m, c) - r for (M, m, c, r) in zip(M_range, Proj_means, centers, radius_range)]\nmaximum(abs.(ds))\n\n1.1102230246251565e-16","category":"section"},{"location":"examples/Constrained-Mean-Hn/#The-experiment","page":"Mean on mathbb H^n","title":"The experiment","text":"We first define a single test function for one set of data for a manifold\n\nfunction bench_aep(Manifold, center, radius, data)\n    # local functions\n    _f(M, p) = f(M, p; pts=data)\n    _grad_f!(M, X, p) = grad_f!(M, X, p; pts=data)\n    _proj_C!(M, q, p) = project_C!(M, q, p; radius=radius, op=center)\n    _g(M, p) = g(M, p; radius=radius, op=center)\n    _grad_g!(M, X, p) = grad_g!(M, X, p; op=center)\n    #\n    #\n    # returns\n    stats = Dict(:PGA => Dict(), :ALM => Dict(), :EPM => Dict())\n    #\n    #\n    # first runs\n    # println(manifold_dimension(Manifold), \": \")\n    mean_pga = copy(Manifold, center)\n    pgas = projected_gradient_method!(\n        Manifold,\n        _f,\n        _grad_f!,\n        _proj_C!,\n        mean_pga;\n        evaluation=InplaceEvaluation(),\n        record=[:Cost],\n        stopping_criterion=StopAfterIteration(150) |\n                           StopWhenProjectedGradientStationary(Manifold, 1e-7),\n        return_state=true,\n    )\n    stats[:PGA][:Iter] = length(get_record(pgas, :Iteration))\n    mean_alm = copy(Manifold, center)\n    alms = augmented_Lagrangian_method!(\n        Manifold,\n        _f,\n        _grad_f!,\n        mean_alm;\n        evaluation=InplaceEvaluation(),\n        g=[_g],\n        grad_g=[_grad_g!],\n        record=[:Cost],\n        return_state=true,\n    )\n    stats[:ALM][:Iter] = length(get_record(alms, :Iteration))\n    mean_epm = copy(Manifold, center)\n    epms = exact_penalty_method!(\n        Manifold, _f, _grad_f!, mean_epm;\n        evaluation=InplaceEvaluation(), return_state=true,\n        g=[_g], grad_g=[_grad_g!], record=[:Cost],\n    )\n    stats[:EPM][:Iter] = length(get_record(epms, :Iteration))\n    #\n    #\n    # Benchmarks\n    pga_b = @be projected_gradient_method!($Manifold, $_f, $_grad_f!, $_proj_C!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        stopping_criterion=$(\n            StopAfterIteration(150) | StopWhenProjectedGradientStationary(Manifold, 1e-7)\n        ),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:PGA][:time] = mean(pga_b).time\n    alm_b = @be augmented_Lagrangian_method!($Manifold, $_f, $_grad_f!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        g=$([_g]), grad_g=$([_grad_g!]),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:ALM][:time] = mean(alm_b).time\n    epm_b = @be exact_penalty_method!($Manifold, $_f, $_grad_f!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        g=$([_g]), grad_g=$([_grad_g!]),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:EPM][:time] = mean(epm_b).time\n    return stats\nend\n\nbench_aep (generic function with 1 method)\n\nand run these\n\nThe resulting plot of runtime is\n\nfig = Figure()\naxis = Axis(fig[1, 1]; title=raw\"Time needed per dimension $\\mathbb H^d$\")\nlines!(axis, n_range, [bi[:PGA][:time] for bi in b]; label=\"PGA\")\nlines!(axis, n_range, [bi[:ALM][:time] for bi in b]; label=\"ALM\")\nlines!(axis, n_range, [bi[:EPM][:time] for bi in b]; label=\"EPM\")\naxis.xlabel = \"Manifold dimension d\"\naxis.ylabel = \"runtime (sec.)\"\naxislegend(axis; position=:lt)\nfig\n\n(Image: )\n\nand the number of iterations reads\n\nfig2 = Figure()\naxis2 = Axis(fig2[1, 1]; title=raw\"Iterations needed per dimension $\\mathbb H^d$\")\nlines!(axis2, n_range, [bi[:PGA][:Iter] for bi in b]; label=\"PGA\")\nlines!(axis2, n_range, [bi[:ALM][:Iter] for bi in b]; label=\"ALM\")\nlines!(axis2, n_range, [bi[:EPM][:Iter] for bi in b]; label=\"EPM\")\naxis2.xlabel = \"Manifold dimension d\"\naxis2.ylabel = \"# Iterations\"\naxislegend(axis2; position=:lt)\nfig2\n\n(Image: )","category":"section"},{"location":"examples/Constrained-Mean-Hn/#Literature","page":"Mean on mathbb H^n","title":"Literature","text":"R. Bergmann, O. P. Ferreira, S. Z. Németh and J. Zhu. On projection mappings and the gradient projection method                on hyperbolic space forms, arXiv preprint (2025).\n\n\n\nC. Liu and N. Boumal. Simple algorithms for optimization on Riemannian manifolds with constraints. Applied Mathematics & Optimization (2019), arXiv:1091.10000.\n\n\n\n","category":"section"},{"location":"examples/Constrained-Mean-Hn/#Technical-details","page":"Mean on mathbb H^n","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 16, 2025, 11:15:25.","category":"section"},{"location":"examples/RayleighQuotient/#The-Rayleigh-Quotient","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Ronny Bergmann 2024-03-09","category":"section"},{"location":"examples/RayleighQuotient/#Introduction","page":"The Rayleigh Quotient","title":"Introduction","text":"This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [Bou23] using the Rayleigh quotient and explores several different ways to use the algorithms from Manopt.jl.\n\nFor a symmetric matrix A in mathbb R^ntimes n we consider the 📖 Rayleigh Quotient\n\noperatorname*argmin_x in mathbb R^n backslash 0\nfracx^mathrmTAxlVert x rVert^2\n\nOn the sphere we can omit the denominator and obtain\n\nf(p) = p^mathrmTApqquad p  𝕊^n-1\n\nwhich by itself we can again continue in the embedding as\n\ntilde f(x) = x^mathrmTAxqquad x in mathbb R^n\n\nThis cost has the nice feature that at the minimizer p^*inmathbb S^n-1 the function falue f(p^*) is the smalles eigenvalue of A.\n\nFor the embedded function tilde f the gradient and Hessian can be computed with classical methods as\n\nbeginalign*\ntilde f(x) = 2Ax qquad x  ℝ^n\n\n^2tilde f(x)V = 2AV qquad x V  ℝ^n\nendalign*\n\nSimilarly, cf. Examples 3.62 and 5.27 of [Bou23], the Riemannian gradient and Hessian on the manifold mathcal M = mathbb S^n-1 are given by\n\nbeginalign*\noperatornamegrad f(p) = 2Ap - 2(p^mathrmTAp)*pqquad p  𝕊^n-1\n\noperatornameHess f(p)X =  2AX - 2(p^mathrmTAX)p - 2(p^mathrmTAp)Xqquad p  𝕊^n-1 X in T_p𝕊^n-1\nendalign*\n\nLet’s first generate an example martrx A.\n\nusing Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,\n\nusing LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random\nRandom.seed!(42)\nn = 500\nA = Symmetric(randn(n, n) / n)\n\nAnd the manifolds\n\nM = Sphere(n-1)\n\nSphere(499)\n\nE = get_embedding(M)\n\nEuclidean(500; field=ℝ)","category":"section"},{"location":"examples/RayleighQuotient/#Setup-the-corresponding-functions","page":"The Rayleigh Quotient","title":"Setup the corresponding functions","text":"Since RayleighQuotientCost, RayleighQuotientGrad!!, and RayleighQuotientHess!! are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute f is called with M as their first argument and tilde f if called with E.\n\nWe instantiate\n\nf = ManoptExamples.RayleighQuotientCost(A)\ngrad_f = ManoptExamples.RayleighQuotientGrad!!(A)\nHess_f = ManoptExamples.RayleighQuotientHess!!(A)\n\nthe suffix !! also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory\n\np0 = [1.0, zeros(n-1)...]\nX = zero_vector(M, p0)\n\nwe can both call\n\nY = grad_f(M, p0)  # Allocates memory\ngrad_f(M, X, p0)    # Computes in place of X and returns the result in X.\nnorm(M, p0, X-Y)\n\n0.0\n\nNow we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them.\n\nFirst of all let’s construct the actual result – since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute\n\nλ = min(eigvals(A)...)\n\n-0.08924035897103724","category":"section"},{"location":"examples/RayleighQuotient/#A-Solver-based-on-gradient-information","page":"The Rayleigh Quotient","title":"A Solver based on gradient information","text":"Let’s first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient nabla tilde f. We can “tell” the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally, Manopt.jl then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf. e.g. this tutorial section or Section 3.8 or more precisely Example 3.62 in [Bou23].\n\nBut instead of diving into all the tecnical details, we can just specify objective_type=:Euclidean to trigger the conversion. We start with a simple gradient descent\n\ns = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n    return_state=true,\n)\nq1 = get_solver_result(s)\ns\n\nInitial f(x): -0.000727|grad f(p)|:0.08694965832662949\n# 50    f(x): -0.088242|grad f(p)|:0.0038704743269815916\n# 100   f(x): -0.088680|grad f(p)|:0.00349565682886346\n# 150   f(x): -0.089026|grad f(p)|:0.0026514781676923505\n# 200   f(x): -0.089178|grad f(p)|:0.0015311603359229944\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: StabilizedRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0,\n    retraction_method=StabilizedRetraction(),\n    contraction_factor=0.95,\n    sufficient_decrease=0.1,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  reached\n  * |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 50]\n\nFrom the final cost we can already see that q1 is an eigenvector to the smallest eigenvalue we obtaines above.\n\nAnd we can compare this to running with the Riemannian gradient, since the RayleighQuotientGrad!! returns this one as well, when just called with the sphere as first Argument, we just have to remove the objective_type.\n\nq2 = gradient_descent(M, f, grad_f, p0;\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n)\n#Test that both are the same\nisapprox(M, q1,q2)\n\nInitial f(x): -0.000727|grad f(p)|:0.08694965832662949\n# 50    f(x): -0.088242|grad f(p)|:0.0038704743269816146\n# 100   f(x): -0.088680|grad f(p)|:0.003495656828863442\n# 150   f(x): -0.089026|grad f(p)|:0.002651478167692411\n# 200   f(x): -0.089178|grad f(p)|:0.0015311603359229851\n\ntrue\n\nWe can also benchmark both\n\n@benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean)\n\nBenchmarkTools.Trial: 21 samples with 1 evaluation per sample.\n Range (min … max):  227.060 ms … 281.275 ms  ┊ GC (min … max): 11.86% … 1.65%\n Time  (median):     239.494 ms               ┊ GC (median):    12.00%\n Time  (mean ± σ):   242.065 ms ±  11.510 ms  ┊ GC (mean ± σ):  11.54% ± 2.29%\n\n  ▁  ▁▁  ▁  ██▁██  █   ▁  ▁▁   ▁ ▁                            ▁  \n  █▁▁██▁▁█▁▁█████▁▁█▁▁▁█▁▁██▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  227 ms           Histogram: frequency by time          281 ms <\n\n Memory estimate: 774.82 MiB, allocs estimate: 9445.\n\n@benchmark gradient_descent($M, $f, $grad_f, $p0)\n\nBenchmarkTools.Trial: 108 samples with 1 evaluation per sample.\n Range (min … max):  39.905 ms … 56.003 ms  ┊ GC (min … max): 0.00% … 14.15%\n Time  (median):     46.833 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   46.324 ms ±  3.401 ms  ┊ GC (mean ± σ):  1.81% ±  2.43%\n\n                                 ▃▅█                           \n  ▇▄▁▅▇▅▄▁▄▄▁▅▇█▇▁▅▁▇▁▁▄▇▇▁▇▇█▄▅▇███▄▅▅▅▄▅▅▄▁█▄▅▄▁▁▄▄▁▄▁▅▄▁▄▇ ▄\n  39.9 ms         Histogram: frequency by time        53.1 ms <\n\n Memory estimate: 12.27 MiB, allocs estimate: 10032.\n\nFrom these results we see, that the conversion from the Euclidean to the Riemannian gradient does require a small amount of effort and hence reduces the performance slighly. Still, if the Euclidean Gradient is easier to compute or already available, this is in terms of coding the faster way. Finally this is a tradeoff between derivation and implementation efforts for the Riemannian gradient and a slight performance reduction when using the Euclidean one.","category":"section"},{"location":"examples/RayleighQuotient/#A-Solver-based-(also)-on-(approximate)-Hessian-information","page":"The Rayleigh Quotient","title":"A Solver based (also) on (approximate) Hessian information","text":"To also involve the Hessian, we consider the trust regions solver with three cases:\n\nEuclidean, approximating the Hessian\nEuclidean, providing the Hessian\nRiemannian, providing the Hessian but also using in-place evaluations.\n\nq3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);\n\nInitial f(x): -0.000727|grad f(p)|:0.08694965832662949\n# 10    f(x): -0.089234|grad f(p)|:0.0013541987348370072\n\nTo provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any struct is considered to (eventually) be the also optional starting point. For space reasons, let’s also shorten the debug print to only iterations 7 and 14.\n\nq4 = trust_regions(M, f, grad_f, (E, p, X) -> Hess_f(E, p, X), p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);\n\nInitial f(x): -0.000727|grad f(p)|:0.08694965832662949\n# 10    f(x): -0.089234|grad f(p)|:0.001356175521036781\n\nq5 = trust_regions(M, f, grad_f, (M, Y, p, X) -> Hess_f(M, Y, p, X), p0;\n    evaluation=InplaceEvaluation(),\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);\n\nInitial f(x): -0.000727|grad f(p)|:0.08694965832662949\n# 10    f(x): -0.089234|grad f(p)|:0.0013561755210367852\n\nLet’s also here compare them in benchmarks. Let’s here compare all variants in their (more performant) in-place versions.\n\n@benchmark trust_regions($M, $f, $grad_f, $p0;\n  objective_type=:Euclidean,\n  evaluation=InplaceEvaluation(),\n)\n\nBenchmarkTools.Trial: 109 samples with 1 evaluation per sample.\n Range (min … max):  38.653 ms … 63.149 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     45.762 ms              ┊ GC (median):    5.42%\n Time  (mean ± σ):   45.877 ms ±  2.899 ms  ┊ GC (mean ± σ):  5.27% ± 2.46%\n\n                        ▄▃▇█▇▂▅▂                               \n  ▅▁▁▃▁▃▅▁▁▁▅▁▆▁▁▁▁▁▁▅▇█████████▆▇▅▁▃▃▃▅▃▃▁▁▁▁▃▁▁▁▁▁▃▁▁▁▁▁▁▁▃ ▃\n  38.7 ms         Histogram: frequency by time        55.1 ms <\n\n Memory estimate: 48.63 MiB, allocs estimate: 16981.\n\n@benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -> Hess_f(E, Y, p, X)), $p0;\n  evaluation=InplaceEvaluation(),\n  objective_type=:Euclidean\n)\n\nBenchmarkTools.Trial: 151 samples with 1 evaluation per sample.\n Range (min … max):  28.489 ms … 48.556 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     32.827 ms              ┊ GC (median):    7.01%\n Time  (mean ± σ):   33.170 ms ±  2.465 ms  ┊ GC (mean ± σ):  6.56% ± 2.81%\n\n               ▂ ▆█▂ ▅▂                                        \n  ▃▁▁▃▅▁▃▃▁▃▁▃██▆███▅██▆▅▃▁▃▃▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▃▁▃▁▁▁▁▁▁▁▁▃ ▃\n  28.5 ms         Histogram: frequency by time        43.8 ms <\n\n Memory estimate: 45.22 MiB, allocs estimate: 14365.\n\n@benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -> Hess_f(M, Y, p, X)), $p0;\n    evaluation=InplaceEvaluation(),\n)\n\nBenchmarkTools.Trial: 204 samples with 1 evaluation per sample.\n Range (min … max):  20.355 ms … 29.813 ms  ┊ GC (min … max): 0.00% … 12.24%\n Time  (median):     24.757 ms              ┊ GC (median):    6.45%\n Time  (mean ± σ):   24.519 ms ±  1.791 ms  ┊ GC (mean ± σ):  4.99% ±  3.16%\n\n                           ▁▁  ▁▅▂▁ ▁ ▄▄▁▅  █ ▅▁▄  ▁    ▂      \n  ▆█▃▃██▁▃▁▁▁▁▃▅▃▃█▃▃█▅█▆█▃██▅█████▆█████████▆███▆▆██▆▆▅█▃▆▃▃ ▅\n  20.4 ms         Histogram: frequency by time        27.5 ms <\n\n Memory estimate: 16.65 MiB, allocs estimate: 14341.\n\nWe see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians.\n\nOf course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged.\n\n[distance(M, q1, q) for q ∈ [q2,q3] ]\n\n2-element Vector{Float64}:\n 9.280652881898308e-16\n 0.22194514626576828\n\n[distance(M, q3, q) for q ∈ [q4,q5] ]\n\n2-element Vector{Float64}:\n 9.995132285502582e-15\n 9.995132285502582e-15\n\nWhich we can also see in the final cost, comparing it to the Eigenvalue\n\n[f(M, q) - λ for q ∈ [q1, q2, q3, q4, q5] ]\n\n5-element Vector{Float64}:\n  6.211293387553551e-5\n  6.211293387553551e-5\n -1.3877787807814457e-16\n -1.249000902703301e-16\n -1.249000902703301e-16","category":"section"},{"location":"examples/RayleighQuotient/#Summary","page":"The Rayleigh Quotient","title":"Summary","text":"We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute.","category":"section"},{"location":"examples/RayleighQuotient/#Technical-details","page":"The Rayleigh Quotient","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.3\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.8\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.1\n  [31c24e10] Distributions v0.25.123\n  [e9467ef8] GLMakie v0.13.8\n  [4d00f742] GeometryTypes v0.8.5\n  [7073ff75] IJulia v1.34.0\n  [682c06a0] JSON v1.4.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.6.0\n  [ee78f7c6] Makie v0.24.8\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.10\n  [3362f125] ManifoldsBase v2.3.0\n  [0fc0a36d] Manopt v0.5.32\n  [5b8d5e80] ManoptExamples v0.1.18 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [6fe1bfb0] OffsetArrays v1.17.0\n  [91a5bcdd] Plots v1.41.4\n  [08abe8d2] PrettyTables v3.1.2\n  [6099a3de] PythonCall v0.9.31\n  [f468eda6] QuadraticModels v0.9.14\n  [731186ca] RecursiveArrayTools v3.44.0\n  [1e40b3f8] RipQP v0.7.0\n\nThis tutorial was last rendered January 20, 2026, 11:11:8.","category":"section"},{"location":"examples/RayleighQuotient/#Literature","page":"The Rayleigh Quotient","title":"Literature","text":"N. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\n","category":"section"},{"location":"helpers/error_measures/#Error-measures","page":"Error measures","title":"Error measures","text":"","category":"section"},{"location":"helpers/error_measures/#ManoptExamples.mean_average_error-Tuple{ManifoldsBase.AbstractManifold, Any, Any}","page":"Error measures","title":"ManoptExamples.mean_average_error","text":"mean_average_error(M,x,y)\n\nCompute the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"method"},{"location":"helpers/error_measures/#ManoptExamples.mean_squared_error-Union{Tuple{mT}, Tuple{mT, Any, Any}} where mT<:ManifoldsBase.AbstractManifold","page":"Error measures","title":"ManoptExamples.mean_squared_error","text":"mean_squared_error(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"method"},{"location":"examples/CRPG-Sparse-Approximation/#A-Sparse-Approximation-Problem-on-Hadamard-Manifolds","page":"Sparse Approximation on mathbb H^n","title":"A Sparse Approximation Problem on Hadamard Manifolds","text":"Hajg Jasa, Paula John 2025-07-02","category":"section"},{"location":"examples/CRPG-Sparse-Approximation/#Introduction","page":"Sparse Approximation on mathbb H^n","title":"Introduction","text":"In this example we use the Convex Riemannian Proximal Gradient (CRPG) method [BJJP25a] with the Cyclic Proximal Point Algorithm, which was introduced in [Bac14], on the hyperbolic space. This example reproduces the results from [BJJP25a], Section 6.2.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/CRPG-Sparse-Approximation/#The-Problem","page":"Sparse Approximation on mathbb H^n","title":"The Problem","text":"Let mathcal M = mathcal H^n be the Hadamard manifold given by the hyperbolic space, and q_1ldotsq_N in mathcal M denote N = 1000 Gaussian random data points. Let g colon mathcal M to mathbb R be defined by\n\ng(p) = frac12 sum_j = 1^N w_j  mathrmdist(p q_j)^2\n\nwhere w_j, j = 1 ldots N are positive weights such that sum_j = 1^N w_j = 1. In our experiments, we choose the weights w_j = frac1N. Observe that the function g is strongly convex with respect to the Riemannian metric on mathcal M.\n\nLet h colon mathcal M to mathbb R be defined by\n\nh(p) = mu Vert p Vert_1\n\nbe the sparsity-enforcing term given by the ell_1-norm, where mu  0 is a regularization parameter.\n\nWe define our total objective function as f = g + h. Notice that this objective function is strongly convex with respect to the Riemannian metric on mathcal M thanks to g. The goal is to find the minimizer of f on mathcal M, which is heuristically the point that is closest to the data points q_j in the sense of the Riemannian metric on mathcal M and has a sparse representation.","category":"section"},{"location":"examples/CRPG-Sparse-Approximation/#Numerical-Experiment","page":"Sparse Approximation on mathbb H^n","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\nrandom_seed = 42\nn_tests = 10 # number of tests for each parameter setting\n\natol = 1e-7\nmax_iters = 5000\nN = 1000 # number of data points\ndims = [2, 10, 100]\nμs = [0.1, 0.5, 1.0]\nσ = 1.0 # standard deviation for the Gaussian random data points\n\n# Objective, gradient, and proxes\ng(M, p, data) = 1/2length(data) * sum(distance.(Ref(M), data, Ref(p)).^2)\ngrad_g(M, p, data) = 1/length(data) * sum(ManifoldDiff.grad_distance.(Ref(M), data, Ref(p), 2))\n#\n# Proximal map for the $\\ell_1$-norm on the hyperbolic space\nfunction prox_l1_Hn(Hn, μ, x; t_0 = μ, max_it = 20, tol = 1e-7)\n    n = manifold_dimension(Hn)\n    t = t_0\n    y = zeros(n+1)\n    y[end] = x[end] + t\n    for i in 1:n\n        y[i] = sign(x[i])*max(0, abs(x[i]) - t)\n    end\n    y /= sqrt(abs(minkowski_metric(y, y)))\n    for k in 1:max_it\n        t_new = μ * sqrt(abs(minkowski_metric(x, y)^2 - 1 ))/distance(Hn, x, y)\n        if abs(t_new - t) ≤ tol\n            return y\n        end\n        y[end] = x[end] + t_new\n        for i in 1:n\n            y[i] = sign(x[i])*max(0, abs(x[i]) - t_new)\n        end\n        y /= sqrt(abs(minkowski_metric(y, y)))\n        t = copy(t_new)\n    end\n    return y\nend\nh(M, p, μ) = μ * norm(p, 1)\nprox_h(M, λ, p, μ) = prox_l1_Hn(M, λ * μ, p)\n#\nf(M, p, data, μ) = g(M, p, data) + h(M, p, μ)\n# CPPA needs the proximal operators for the total objective\nfunction proxes_f(data, μ)\n    proxes = Function[(M, λ, p) -> ManifoldDiff.prox_distance(M, λ / length(data), di, p, 2) for di in data]\n    push!(proxes, (M, λ, p) -> prox_l1_Hn(M, λ * μ, p))\n    return proxes\nend\n# Function to generate points close to the given point p\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend\n# Estimate Lipschitz constant of the gradient of g\nfunction estimate_lipschitz_constant(M, g, grad_g, anchor, R, N=10_000)\n    constants = []\n    for i in 1:N\n        p = close_point(M, anchor, R)\n        q = close_point(M, anchor, R)\n\n        push!(constants, 2/distance(M, q, p)^2 * (g(M, q) - g(M, p) - inner(M, p, grad_g(M, p), log(M, p, q))))\n    end\n    return maximum(constants)\nend\n\nWe introduce some keyword arguments for the solvers we will use in this experiment\n\n# Keyword arguments for CRPG with a constant stepsize\npgm_kwargs_cn(constant_stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(constant_stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs_cn(constant_stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(constant_stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n# Keyword arguments for CRPG with a backtracked stepsize\npgm_kwargs_bt(contraction_factor, initial_stepsize, warm_start_factor) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        contraction_factor=contraction_factor,\n        initial_stepsize=initial_stepsize,\n        strategy=:convex,\n        warm_start_factor=warm_start_factor,\n    ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs_bt(contraction_factor, initial_stepsize, warm_start_factor) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        contraction_factor=contraction_factor,\n        initial_stepsize=initial_stepsize,\n        strategy=:convex,\n        warm_start_factor=warm_start_factor,\n    ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n# Keyword arguments for CPPA\ncppa_kwargs(M) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenCriterionWithIterationCondition(StopWhenChangeLess(M, 1e-5*atol), 20)\n    ),\n]\ncppa_bm_kwargs(M) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenCriterionWithIterationCondition(StopWhenChangeLess(M, 1e-5*atol), 20)\n    ),\n]\n\nWe set up some variables to collect the results of the experiments and initialize the dataframes\n\nAnd run the experiments\n\nfor n in dims\n    # Set random seed for reproducibility\n    Random.seed!(random_seed)\n\n    # Define manifold\n    M = Hyperbolic(n)\n\n    for test in 1:n_tests\n        # Generate random data\n        anchor = rand(M)\n        data = [exp(M, anchor, rand(M; vector_at=anchor, σ=σ)) for _ in 1:N]\n\n        for (c, μ) in enumerate(μs)\n            # Initialize starting point for the optimization\n            p0 = rand(M)\n\n            # Initialize functions\n            g_hn(M, p) = g(M, p, data)\n            grad_g_hn(M, p) = grad_g(M, p, data)\n            proxes_f_hn = proxes_f(data, μ)\n            prox_h_hn(M, λ, p) = prox_h(M, λ, p, μ)\n            f_hn(M, p) = f(M, p, data, μ)\n            #\n            # Estimate stepsizes\n            D = 2.05 * maximum([distance(M, p0, di) for di in vcat(data, [anchor])])\n            L_g = Manopt.ζ_1(-1.0, D)\n            constant_stepsize = 1/L_g\n            initial_stepsize = 3/2 * constant_stepsize\n            contraction_factor = 0.9\n            warm_start_factor = 2.0\n            #\n            # Optimization\n            # Constant stepsize\n            pgm_cn = proximal_gradient_method(M, f_hn, g_hn, grad_g_hn, p0;\n                prox_nonsmooth=prox_h_hn,\n                pgm_kwargs_cn(constant_stepsize)...\n            )\n            pgm_result_cn = get_solver_result(pgm_cn)\n            pgm_record_cn = get_record(pgm_cn)\n            #\n            # Backtracked stepsize\n            pgm_bt = proximal_gradient_method(M, f_hn, g_hn, grad_g_hn, p0;\n                prox_nonsmooth=prox_h_hn,\n                pgm_kwargs_bt(contraction_factor, initial_stepsize, warm_start_factor)...\n            )\n            pgm_result_bt = get_solver_result(pgm_bt)\n            pgm_record_bt = get_record(pgm_bt)\n            #\n            # CPPA\n            cppa = cyclic_proximal_point(M, f_hn, proxes_f_hn, p0; cppa_kwargs(M)...)\n            cppa_result = get_solver_result(cppa)\n            cppa_record = get_record(cppa)\n            #\n            # Benchmark the algorithms\n            # Constant stepsize\n            pgm_bm_cn = @benchmark proximal_gradient_method($M, $f_hn, $g_hn, $grad_g_hn, $p0;\n                prox_nonsmooth=$prox_h_hn,\n                $pgm_bm_kwargs_cn($constant_stepsize)...\n            )\n            # Backtracked stepsize\n            pgm_bm_bt = @benchmark proximal_gradient_method($M, $f_hn, $g_hn, $grad_g_hn, $p0;\n                prox_nonsmooth=$prox_h_hn,\n                $pgm_bm_kwargs_bt($contraction_factor, $initial_stepsize, $warm_start_factor)...\n            )\n            # CPPA\n            cppa_bm = @benchmark cyclic_proximal_point($M, $f_hn, $proxes_f_hn, $p0; cppa_bm_kwargs($M)...)\n            #\n            # Collect times\n            time_pgm_cn = time(median(pgm_bm_cn)) * 1e-9\n            time_pgm_bt = time(median(pgm_bm_bt)) * 1e-9\n            time_cppa = time(median(cppa_bm)) * 1e-9\n            time_pgm_cn_means[c] += time_pgm_cn\n            time_pgm_bt_means[c] += time_pgm_bt\n            time_cppa_means[c] += time_cppa\n            #\n            # Collect sparsities\n            sparsity_pgm_cn = sum(abs.(pgm_result_cn) .< atol)/n\n            sparsity_pgm_bt = sum(abs.(pgm_result_bt) .< atol)/n\n            sparsity_cppa = sum(abs.(cppa_result) .< atol)/n\n            sparsity_pgm_cn_means[c] += sparsity_pgm_cn\n            sparsity_pgm_bt_means[c] += sparsity_pgm_bt\n            sparsity_cppa_means[c] += sparsity_cppa\n            #\n            # Collect objective values\n            objective_pgm_cn = f_hn(M, pgm_result_cn)\n            objective_pgm_bt = f_hn(M, pgm_result_bt)\n            objective_cppa = f_hn(M, cppa_result)\n            objective_pgm_cn_means[c] += objective_pgm_cn\n            objective_pgm_bt_means[c] += objective_pgm_bt\n            objective_cppa_means[c] += objective_cppa\n            #\n            # Collect iterations\n            iterations_pgm_cn = length(pgm_record_cn)\n            iterations_pgm_bt = length(pgm_record_bt)\n            iterations_cppa = length(cppa_record)\n            iterations_pgm_cn_means[c] += iterations_pgm_cn\n            iterations_pgm_bt_means[c] += iterations_pgm_bt\n            iterations_cppa_means[c] += iterations_cppa\n        end\n    end\n    for (c, μ) in enumerate(μs)\n        push!(df_pgm_cn,\n            [\n                μ, n, iterations_pgm_cn_means[c]/n_tests, time_pgm_cn_means[c]/n_tests, objective_pgm_cn_means[c]/n_tests, sparsity_pgm_cn_means[c]/n_tests\n            ]\n        )\n        push!(df_pgm_bt,\n            [\n                μ, n, iterations_pgm_bt_means[c]/n_tests, time_pgm_bt_means[c]/n_tests, objective_pgm_bt_means[c]/n_tests, sparsity_pgm_bt_means[c]/n_tests\n            ]\n        )\n        push!(df_cppa,\n            [\n                μ, n, iterations_cppa_means[c]/n_tests, time_cppa_means[c]/n_tests, objective_cppa_means[c]/n_tests, sparsity_cppa_means[c]/n_tests\n            ]\n        )\n    end\n    #\n    # Reset data collection variables\n    iterations_pgm_cn_means .= zeros(length(μs))\n    iterations_pgm_bt_means .= zeros(length(μs))\n    iterations_cppa_means .= zeros(length(μs))\n    time_pgm_cn_means .= zeros(length(μs))\n    time_pgm_bt_means .= zeros(length(μs))\n    time_cppa_means .= zeros(length(μs))\n    sparsity_pgm_cn_means .= zeros(length(μs))\n    sparsity_pgm_bt_means .= zeros(length(μs))\n    sparsity_cppa_means .= zeros(length(μs))\n    objective_pgm_cn_means .= zeros(length(μs))\n    objective_pgm_bt_means .= zeros(length(μs))\n    objective_cppa_means .= zeros(length(μs))\nend\n\nWe export the results to CSV files\n\n<details class=\"code-fold\"> <summary>Code</summary>\n\n# Sort the dataframes by the parameter μ and create the final results dataframes\ndf_pgm_cn = sort(df_pgm_cn, :μ)\ndf_pgm_bt = sort(df_pgm_bt, :μ)\ndf_cppa = sort(df_cppa, :μ)\ndf_results_time_iter = DataFrame(\n    μ             = df_pgm_cn.μ,\n    n             = Int.(df_pgm_cn.n),\n    CRPG_iter     = Int.(round.(df_pgm_cn.iterations, digits = 0)),\n    CRPG_time     = df_pgm_cn.time,\n    CRPG_bt_iter  = Int.(round.(df_pgm_bt.iterations, digits = 0)),\n    CRPG_bt_time  = df_pgm_bt.time,\n    CPPA_iter  = Int.(round.(df_cppa.iterations, digits = 0)),\n    CPPA_time     = df_cppa.time,\n)\ndf_results_obj_spar = DataFrame(\n    μ               = df_pgm_cn.μ,\n    n               = Int.(df_pgm_cn.n),\n    CRPG_obj       = df_pgm_cn.objective,\n    CRPG_sparsity  = df_pgm_cn.sparsity,\n    CRPG_bt_obj    = df_pgm_bt.objective,\n    CRPG_bt_sparsity = df_pgm_bt.sparsity,\n    CPPA_obj         = df_cppa.objective,\n    CPPA_sparsity    = df_cppa.sparsity,\n)\n# Write the results to CSV files\nCSV.write(joinpath(results_folder, \"results-Hn-time-iter-$(n_tests)-$(dims[end]).csv\"), df_results_time_iter)\nCSV.write(joinpath(results_folder, \"results-Hn-obj-spar-$(n_tests)-$(dims[end]).csv\"), df_results_obj_spar)\n\n</details>\n\nWe can take a look at how the algorithms compare to each other in their performance with the following tables. First, we look at the time and number of iterations for each algorithm.\n\nμ n CRPGconstiter CRPGconsttime CRPGbtiter CRPGbttime CPPA_iter CPPA_time\n0.1 2 167 0.049181 1561 1.4581 5000 3.9519\n0.1 10 111 0.0443115 2033 3.18836 5000 7.17354\n0.1 100 48 0.0358821 2928 12.569 5000 7.73864\n0.5 2 121 0.0317396 567 0.86611 4004 3.2404\n0.5 10 81 0.0267678 1117 2.01164 4502 5.1665\n0.5 100 45 0.0335185 1021 3.32761 5000 6.23522\n1.0 2 69 0.0185704 73 0.0460234 2511 2.28915\n1.0 10 67 0.0204594 1098 1.9168 3507 3.39929\n1.0 100 45 0.0336094 3069 13.005 4502 5.83509\n\nSecond, we look at the objective values and sparsity of the solutions found by each algorithm.\n\nμ n CRPGconstobj CRPGconstspar CRPGbtobj CRPGbtspar CPPA_obj CPPA_spar\n0.1 2 3.17406 0.05 3.17406 0.05 3.17406 0.05\n0.1 10 8.32265 0.15 8.32265 0.15 8.32265 0.15\n0.1 100 52.938 0.088 52.938 0.088 52.938 0.088\n0.5 2 3.89857 0.4 3.89857 0.4 3.89857 0.4\n0.5 10 9.47638 0.51 9.47638 0.51 9.47638 0.51\n0.5 100 55.593 0.442 55.593 0.442 55.593 0.442\n1.0 2 4.51909 0.6 4.51909 0.6 4.51909 0.6\n1.0 10 10.357 0.73 10.357 0.73 10.357 0.73\n1.0 100 57.1667 0.76 57.1667 0.76 57.167 0.763","category":"section"},{"location":"examples/CRPG-Sparse-Approximation/#Technical-details","page":"Sparse Approximation on mathbb H^n","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 15, 2025, 15:29:44.","category":"section"},{"location":"examples/CRPG-Sparse-Approximation/#Literature","page":"Sparse Approximation on mathbb H^n","title":"Literature","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization, preprint (2025), arXiv:2507.16055.\n\n\n\n","category":"section"},{"location":"examples/Constrained-Mean-H2/#The-Constrained-mean-on-Hyperbolic-space.","page":"Mean on mathbb H^2","title":"The Constrained mean on Hyperbolic space.","text":"Ronny Bergmann 2027-03-03","category":"section"},{"location":"examples/Constrained-Mean-H2/#Introduction","page":"Mean on mathbb H^2","title":"Introduction","text":"In this example we compare the Projected Gradient Algorithm (PGA) as introduced in [BFNZ25] with both the Augmented Lagrangian Method (ALM) and the Exact Penalty Method (EPM) [LB19].\n\nusing Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random\n\nConsider the constrained Riemannian center of mass for a given set of points ``q_i M$ i=1ldotsN given by\n\noperatorname*argmin_pinmathcal C\nsum_i=1^N d_mathrmM^2(pq_i)\n\nconstrained to a set mathcal C subset mathcal M.\n\nFor this experiment set mathcal M = mathbb H^2 is the Hyperbolic space and the constrained set mathcal C = C_cr as the ball of radius r around the center point c, where we choose here r=1 and $c = (0,0,1)^{}.\n\nM = Hyperbolic(2)\nc = Manifolds._hyperbolize(M, [0, 0])\nradius = 1.0\n# Sample the boundary\nunit_circle = [\n    exp(M, c, get_vector(M, c, radius .* [cos(α), sin(α)], DefaultOrthonormalBasis())) for\n    α in 0:(2π / 720):(2π)\n]\n\nOur data consists of N=200 points, where we skew the data a bit to force the mean to be outside of the constrained set mathcal C.\n\nN = 200;\nσ = 1.5\nRandom.seed!(42)\n# N random points moved to top left to have a mean outside\ndata_pts = [\n    exp(\n        M,\n        c,\n        get_vector(\n            M, c, σ .* randn(manifold_dimension(M)) .+ [2.5, 2.5], DefaultOrthonormalBasis()\n        ),\n    ) for _ in 1:N\n]","category":"section"},{"location":"examples/Constrained-Mean-H2/#Cost,-gradient-and-projection","page":"Mean on mathbb H^2","title":"Cost, gradient and projection","text":"We can formulate the constrained problem above in two different forms. Both share a cost and require a gradient. For performance reasons, we also provide a mutating variant of the gradient\n\nf(M, p; pts=[op]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts);\n\ngrad_f(M, p; pts=[op]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts);\n\nfunction grad_f!(M, X, p; pts=[op])\n    zero_vector!(M, X, p)\n    Y = zero_vector(M, p)\n    for q in pts\n        log!(M, Y, p, q)\n        X .+= Y\n    end\n    X .*= -1 / length(pts)\n    return X\nend;\n\nWe can model the constrained either with an inequality constraint g(p) geq 0 or using a projection onto the set. For the gradient of g and the projection we again also provide mutating variants.\n\ng(M, p) = distance(M, c, p)^2 - radius^2;\nindicator_C(M, p) = (g(M, p) ≤ 0) ? 0 : Inf;\n\nfunction project_C(M, p, r=radius)\n    X = log(M, c, p)\n    n = norm(M, c, X)\n    q = (n > r) ? exp(M, c, (r / n) * X) : copy(M, p)\n    return q\nend;\nfunction project_C!(M, q, p; X=zero_vector(M, c), r=radius)\n    log!(M, X, c, p)\n    n = norm(M, c, X)\n    if (n > r)\n        exp!(M, q, c, (r / n) * X)\n    else\n        copyto!(M, q, p)\n    end\n    return q\nend;\n\ngrad_g(M, p) = -2 * log(M, p, c);\nfunction grad_g!(M, X, p)\n    log!(M, X, p, c)\n    X .*= -2\n    return X\nend","category":"section"},{"location":"examples/Constrained-Mean-H2/#The-mean","page":"Mean on mathbb H^2","title":"The mean","text":"For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to mathcal C. We can then project this onto mathcal C. For the projected mean we obtain g(p) = 0 since the original mean is outside of the set, the projected one lies on the boundary.\n\nmean_data = mean(M, data_pts)\nmean_projected = project_C(M, mean_data)\ng(M, mean_projected)\n\n0.0","category":"section"},{"location":"examples/Constrained-Mean-H2/#The-experiment","page":"Mean on mathbb H^2","title":"The experiment","text":"We first define the specific data cost functions\n\n_f(M, p) = f(M, p; pts=data_pts)\n_grad_f(M, p) = grad_f(M, p; pts=data_pts)\n_grad_f!(M, X, p) = grad_f!(M, X, p; pts=data_pts)\n\nand in a first run record a projected gradient method solver run\n\nmean_pg = copy(M, c) # start at the center\n@time pgms = projected_gradient_method!(\n    M, _f, _grad_f!, project_C!, mean_pg;\n    evaluation=InplaceEvaluation(),\n    indicator=indicator_C,\n    debug=[:Iteration, :Cost, \" \", :GradientNorm, \"\\n\", 1, :Stop],\n    record=[:Iteration, :Iterate, :Cost, :Gradient],\n    stopping_criterion=StopAfterIteration(150) |\n                       StopWhenProjectedGradientStationary(M, 1e-7),\n    return_state=true,\n)\n\nInitial f(x): 8.519830 \n# 1     f(x): 5.741908 |grad f(p)|:3.560737798355543\n# 2     f(x): 5.741846 |grad f(p)|:1.881900575821152\n# 3     f(x): 5.741846 |grad f(p)|:1.8819696248924744\n# 4     f(x): 5.741846 |grad f(p)|:1.881964795224877\n# 5     f(x): 5.741846 |grad f(p)|:1.8819649705365404\n# 6     f(x): 5.741846 |grad f(p)|:1.8819649640556793\nAt iteration 6 algorithm has reached a stationary point, since the distance from the last iterate to the projected gradient (1.0030679901141345e-8) less than 1.0e-7.\n  1.804329 seconds (9.43 M allocations: 489.090 MiB, 3.91% gc time, 99.67% compilation time)\n\n# Solver state for `Manopt.jl`s Projected Gradient Method\nAfter 6 iterations\n\n## Parameters\n* inverse retraction method: ManifoldsBase.LogarithmicInverseRetraction()\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize for the gradient step\nConstantLength(1.0; type=:relative)\n\n## Stepsize for the complete step\nArmijoLinesearch(;\n    initial_stepsize=1.0,\n    retraction_method=ManifoldsBase.ExponentialRetraction(),\n    contraction_factor=0.95,\n    sufficient_decrease=0.1,\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 150:  not reached\n  * projected gradient stationary (<1.0e-7):    reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 1]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost(), RecordGradient{Vector{Float64}}()]),)\n\nand similarly perform a first run of both the augmented Lagrangian method and the exact penalty method\n\nmean_alm = copy(M, c)\n@time alms = augmented_Lagrangian_method!(\n    M, _f, _grad_f!, mean_alm;\n    evaluation=InplaceEvaluation(),\n    g=[g], grad_g=[grad_g!],\n    debug=[:Iteration, :Cost, \" \", \"\\n\", 10, :Stop],\n    record=[:Iteration, :Iterate, :Cost],\n    return_state=true,\n)\n\nInitial f(x): 8.519830 \n# 10    f(x): 5.741814 \n# 20    f(x): 5.741845 \nThe algorithm computed a step size (5.830448990119683e-11) less than 1.0e-10.\n  2.207807 seconds (12.38 M allocations: 635.823 MiB, 3.43% gc time, 99.41% compilation time)\n\n# Solver state for `Manopt.jl`s Augmented Lagrangian Method\nAfter 29 iterations\n\n## Parameters\n* ϵ: 0.0001348962882591652 (ϵ_min: 1.0e-6, θ_ϵ: 0.933254300796991)\n* λ: Float64[] (λ_min: -20.0, λ_max: 20.0)\n* μ: [0.94098958634295] (μ_max: 20.0)\n* ρ: 15241.579027587262 (θ_ρ: 0.3)\n* τ: 0.8\n* current penalty: 1.1472098826459387e-9\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * Stop When _all_ of the following are fulfilled:\n      * Field :ϵ ≤ 1.0e-6:  not reached\n      * |Δp| < 0.00014454397707459258: not reached\n    Overall: not reached\n  * Stepsize s < 1.0e-10:   reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", \"\\n\", 10]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost()]),)\n\nmean_epm = copy(M, c)\n@time epms = exact_penalty_method!(\n    M, _f, _grad_f!, mean_epm;\n    evaluation = InplaceEvaluation(),\n    g = [g], grad_g = [grad_g!],\n    debug = [:Iteration, :Cost, \" \", \"\\n\", 25, :Stop],\n    record = [:Iteration, :Iterate, :Cost],\n    return_state = true,\n)\n\nInitial f(x): 8.519830 \n# 25    f(x): 5.747352 \n# 50    f(x): 5.742157 \n# 75    f(x): 5.741863 \n# 100   f(x): 5.741847 \nThe value of the variable (ϵ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 101 the algorithm performed a step with a change (5.712257693422003e-8) less than 1.0e-6.\n  1.473903 seconds (10.30 M allocations: 504.499 MiB, 6.09% gc time, 92.78% compilation time)\n\n# Solver state for `Manopt.jl`s Exact Penalty Method\nAfter 101 iterations\n\n## Parameters\n* ϵ: 1.0e-6 (ϵ_min: 1.0e-6, θ_ϵ: 0.933254300796991)\n* u: 1.0e-6 (ϵ_min: 1.0e-6, θ_u: 0.8912509381337456)\n* ρ: 3.3333333333333335 (θ_ρ: 0.3)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * Stop When _all_ of the following are fulfilled:\n      * Field :ϵ ≤ 1.0e-6:  reached\n      * |Δp| < 1.0e-6: reached\n    Overall: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", \"\\n\", 25]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost()]),)","category":"section"},{"location":"examples/Constrained-Mean-H2/#Benchmark","page":"Mean on mathbb H^2","title":"Benchmark","text":"After a first run we now Benchmark the three algorithms with Chairmarks.jl\n\npg_b = @be projected_gradient_method!(\n    $M, $_f, $_grad_f!, $project_C!, $(copy(M, c));\n    evaluation=$(InplaceEvaluation()),\n    indicator=$indicator_C,\n    stopping_criterion=$(\n        StopAfterIteration(150) | StopWhenProjectedGradientStationary(M, 1e-7)\n    ),\n) evals = 1 samples = 5 seconds = 100\n\nBenchmark: 5 samples with 1 evaluation\n min    184.708 μs (3849 allocs: 160.688 KiB)\n median 219.917 μs (5473 allocs: 223.750 KiB)\n mean   276.942 μs (7340.60 allocs: 296.272 KiB)\n max    444.167 μs (17247 allocs: 680.953 KiB)\n\nalm_b = @be augmented_Lagrangian_method!(\n    $M, $_f, $_grad_f!, $(copy(M, c));\n    evaluation = $(InplaceEvaluation()),\n    g = $([g]),\n    grad_g = $([grad_g!]),\n) evals = 1 samples = 5 seconds = 100\n\nBenchmark: 5 samples with 1 evaluation\n min    9.858 ms (325530 allocs: 12.246 MiB)\n median 12.499 ms (395087 allocs: 14.854 MiB)\n mean   11.926 ms (373553.40 allocs: 14.046 MiB)\n max    13.190 ms (404833 allocs: 15.218 MiB)\n\nepm_b = @be exact_penalty_method!(\n    $M, $_f, $_grad_f!, $(copy(M, c));\n    evaluation = $(InplaceEvaluation()),\n    g = $([g]),\n    grad_g = $([grad_g!]),\n) evals = 1 samples = 5 seconds = 100\n\nBenchmark: 5 samples with 1 evaluation\n min    60.591 ms (2000556 allocs: 75.418 MiB)\n median 71.683 ms (2000556 allocs: 75.418 MiB)\n mean   78.335 ms (2000556 allocs: 75.418 MiB, 9.02% gc time)\n max    112.708 ms (2000556 allocs: 75.418 MiB, 45.09% gc time)","category":"section"},{"location":"examples/Constrained-Mean-H2/#Plots-and-results","page":"Mean on mathbb H^2","title":"Plots & results","text":"pb_x(data) = [convert(PoincareBallPoint, p).value[1] for p in data]\npb_y(data) = [convert(PoincareBallPoint, p).value[2] for p in data]\n\nThe results are\n\nfig = Figure()\naxis = Axis(fig[1, 1], title = \"The ball constrained mean comparison\", aspect = 1)\narc!(Point2f(0, 0), 1, -π, π; color = :black)\nlines!(axis, pb_x(unit_circle), pb_y(unit_circle); label = L\"δC\")\nscatter!(axis, pb_x(data_pts), pb_y(data_pts), label = L\"d_i\")\nscatter!(axis, pb_x([mean_data]), pb_y([mean_data]), label = L\"m\")\nscatter!(\n    axis,\n    pb_x([mean_projected]),\n    pb_y([mean_projected]),\n    label = L\"m_{\\text{proj}}\",\n)\nscatter!(axis, pb_x([mean_alm]), pb_y([mean_alm]), label = L\"m_{\\text{alm}}\")\nscatter!(axis, pb_x([mean_epm]), pb_y([mean_epm]), label = L\"m_{\\text{epm}}\")\nscatter!(axis, pb_x([mean_pg]), pb_y([mean_pg]), label = L\"m_{\\text{pg}}\")\naxislegend(axis, position = :rt)\nxlims!(axis, -1.02, 1.5)\nylims!(axis, -1.02, 1.5)\nhidespines!(axis)\nhidedecorations!(axis)\nfig\n\n(Image: )\n\nmin_cost = minimum(map(p -> _f(M, p), [mean_pg, mean_alm, mean_epm]))\n\n5.7418455315254855\n\nfig2 = Figure()\naxis2 = Axis(\n    fig2[1, 1];\n    title=\"Cost over iterations (log scale x)\",\n    xscale=log10,\n    yscale=identity,\n    xticks=[1, 10, 100],\n)\nlines!(\n    axis2,\n    get_record(pgms, :Iteration, :Iteration),\n    get_record(pgms, :Iteration, :Cost);\n    label=\"PG\",\n)\nlines!(\n    axis2,\n    get_record(alms, :Iteration, :Iteration),\n    get_record(alms, :Iteration, :Cost);\n    label=\"ALM\",\n)\nlines!(\n    axis2,\n    get_record(epms, :Iteration, :Iteration),\n    get_record(epms, :Iteration, :Cost);\n    label=\"EPM\",\n)\naxislegend(axis2; position=:rb)\n#ylims!(axis2, min_cost-0.001,)\naxis2.xlabel = \"Iterations (log scale)\"\naxis2.ylabel = \"Cost f\"\nfig2\n\n(Image: )","category":"section"},{"location":"examples/Constrained-Mean-H2/#Literature","page":"Mean on mathbb H^2","title":"Literature","text":"R. Bergmann, O. P. Ferreira, S. Z. Németh and J. Zhu. On projection mappings and the gradient projection method                on hyperbolic space forms, arXiv preprint (2025).\n\n\n\nC. Liu and N. Boumal. Simple algorithms for optimization on Riemannian manifolds with constraints. Applied Mathematics & Optimization (2019), arXiv:1091.10000.\n\n\n\n","category":"section"},{"location":"examples/Constrained-Mean-H2/#Technical-details","page":"Mean on mathbb H^2","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 16, 2025, 10:29:49.","category":"section"},{"location":"#Welcome-to-ManoptExample.jl","page":"Home","title":"Welcome to ManoptExample.jl","text":"This package provides a set of example tasks for Manopt.jl based on either generic manifolds from the ManifoldsBase.jl interface or specific manifolds from Manifolds.jl.\n\nEach example usually consists of\n\na cost function and additional objects, like the gradient or proximal maps, see objectives\nan example explaining how to use these, see examples\n\nHelping functions that are used in one or more examples can be found in the section of functions in the menu.","category":"section"},{"location":"#ManoptExamples.ManoptExamples","page":"Home","title":"ManoptExamples.ManoptExamples","text":"🏔️⛷️ ManoptExamples.jl – A collection of research and tutorial example problems for Manopt.jl\n\n📚 Documentation: juliamanifolds.github.io/ManoptExamples.jl\n📦 Repository: github.com/JuliaManifolds/ManoptExamples.jl\n💬 Discussions: github.com/JuliaManifolds/ManoptExamples.jl/discussions\n🎯 Issues: github.com/JuliaManifolds/ManoptExamples.jl/issues\n\n\n\n\n\n","category":"module"},{"location":"examples/H2-Signal-TV/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM,-and-the-CPPA-for-denoising-a-signal-on-the-hyperbolic-space","page":"Hyperbolic Signal Denoising","title":"A comparison of the RCBM with the PBA, the SGM, and the CPPA for denoising a signal on the hyperbolic space","text":"Hajg Jasa 2024-06-27","category":"section"},{"location":"examples/H2-Signal-TV/#Introduction","page":"Hyperbolic Signal Denoising","title":"Introduction","text":"In this example we compare the Riemannian Convex Bundle Method (RCBM) [BHJ24] with the Proximal Bundle Algorithm, which was introduced in [HNP23], and with the Subgradient Method (SGM), introduced in [FO98], to denoise an artificial signal on the Hyperbolic space mathcal H^2. This example reproduces the results from [BHJ24], Section 5.2.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/H2-Signal-TV/#The-Problem","page":"Hyperbolic Signal Denoising","title":"The Problem","text":"Let mathcal M = mathcal H^2 be the 2-dimensional hyperbolic space and let p q in mathcal M^n be two manifold-valued signals, for n in mathbb N. Let f colon mathcal M to mathbb R be defined by\n\n    f_q (p)\n    =\n    frac1n\n    left\n    frac12 sum_i = 1^n mathrmdist(p_i q_i)^2\n    +\n    alpha operatornameTV(p)\n    right\n    \n\nwhere operatornameTV(p), is the total variation term given by\n\n    operatornameTV(p)\n    =\n    sum_i = 1^n-1 mathrmdist(p_i p_i+1)\n    ","category":"section"},{"location":"examples/H2-Signal-TV/#Numerical-Experiment","page":"Hyperbolic Signal Denoising","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\nRandom.seed!(33)\nn = 496\nσ = 0.1 # Noise parameter\nα = 0.5 # TV parameter\natol = 1e-8\nk_max = 0.0\nk_min = -1.0\nmax_iters = 5000\n#\n# Colors\ndata_color = RGBA{Float64}(colorant\"#BBBBBB\")\nnoise_color = RGBA{Float64}(colorant\"#33BBEE\") # Tol Vibrant Teal\nresult_color = RGBA{Float64}(colorant\"#EE7733\") # Tol Vibrant Orange\n\nfunction artificial_H2_signal(\n    pts::Integer=100; a::Real=0.0, b::Real=1.0, T::Real=(b - a) / 2\n)\n    t = range(a, b; length=pts)\n    x = [[s, sign(sin(2 * π / T * s))] for s in t]\n    y = [\n        [x[1]]\n        [\n            x[i] for\n            i in 2:(length(x) - 1) if (x[i][2] != x[i + 1][2] || x[i][2] != x[i - 1][2])\n        ]\n        [x[end]]\n    ]\n    y = map(z -> Manifolds._hyperbolize(Hyperbolic(2), z), y)\n    data = []\n    geodesics = []\n    l = Int(round(pts * T / (2 * (b - a))))\n    for i in 1:2:(length(y) - 1)\n        append!(\n            data,\n            shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n        )\n        if i + 2 ≤ length(y) - 1\n            append!(\n                geodesics,\n                shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n            )\n            append!(\n                geodesics,\n                shortest_geodesic(\n                    Hyperbolic(2), y[i + 1], y[i + 2], range(0.0, 1.0; length=l)\n                ),\n            )\n        end\n    end\n    return data, geodesics\nend\nfunction matrixify_Poincare_ball(input)\n    input_x = []\n    input_y = []\n    for p in input\n        push!(input_x, p.value[1])\n        push!(input_y, p.value[2])\n    end\n    return hcat(input_x, input_y)\nend\n\nWe now fix the data for the experiment…\n\nH = Hyperbolic(2)\nsignal, geodesics = artificial_H2_signal(n; a=-6.0, b=6.0, T=3)\nnoise = map(p -> exp(H, p, rand(H; vector_at=p, σ=σ)), signal)\ndiameter = 3 * maximum([distance(H, noise[i], noise[j]) for i in 1:n, j in 1:n])\nHn = PowerManifold(H, NestedPowerRepresentation(), length(noise))\n\n… As well as objective, subdifferential, and proximal map.\n\nfunction f(M, p)\n    return 1 / length(noise) *\n           (1 / 2 * distance(M, noise, p)^2 + α * ManoptExamples.Total_Variation(M, p))\nend\ndomf(M, p) = distance(M, p, noise) < diameter / 2 ? true : false\nfunction ∂f(M, p)\n    return 1 / length(noise) * (\n        ManifoldDiff.grad_distance(M, noise, p) +\n        α * ManoptExamples.subgrad_Total_Variation(M, p; atol=atol)\n    )\nend\nproxes = (\n    (M, λ, p) -> ManifoldDiff.prox_distance(M, λ, noise, p, 2),\n    (M, λ, p) -> ManoptExamples.prox_Total_Variation(M, α * λ, p),\n)\n\nWe can now plot the initial setting.\n\nglobal ball_scene = plot()\nif export_orig\n    ball_signal = convert.(PoincareBallPoint, signal)\n    ball_noise = convert.(PoincareBallPoint, noise)\n    ball_geodesics = convert.(PoincareBallPoint, geodesics)\n    plot!(ball_scene, H, ball_signal; geodesic_interpolation=100, label=\"Geodesics\")\n    plot!(\n        ball_scene,\n        H,\n        ball_signal;\n        markercolor=data_color,\n        markerstrokecolor=data_color,\n        label=\"Signal\",\n    )\n    plot!(\n        ball_scene,\n        H,\n        ball_noise;\n        markercolor=noise_color,\n        markerstrokecolor=noise_color,\n        label=\"Noise\",\n    )\n    matrix_data = matrixify_Poincare_ball(ball_signal)\n    matrix_noise = matrixify_Poincare_ball(ball_noise)\n    matrix_geodesics = matrixify_Poincare_ball(ball_geodesics)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-noise.csv\"),\n        DataFrame(matrix_data, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-noise.csv\"),\n        DataFrame(matrix_noise, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-geodesics.csv\"),\n        DataFrame(matrix_geodesics, :auto);\n        header=[\"x\", \"y\"],\n    )\n    display(ball_scene)\nend\n\n(Image: )\n\nWe introduce some keyword arguments for the solvers we will use in this experiment\n\nrcbm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.8f \"),\n        (:ξ, \"ξ: %1.16f \"),\n        (:ε, \"ε: %1.16f \"),\n        :WarnBundle,\n        :Stop,\n        1000,\n        \"\\n\",\n        ],\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\nrcbm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ν, \"ν: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:μ, \"μ: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(max_iters),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenSubgradientNormLess(√atol) | StopAfterIteration(max_iters),\n]\ncppa_kwargs = [\n    #=\n    :debug => [\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        (:Cost, \"F(p): %1.16f \"),\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    =#\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(StopAfterIteration(max_iters), StopWhenChangeLess(Hn, atol)),\n]\ncppa_bm_kwargs = [\n    :stopping_criterion => StopWhenAny(StopAfterIteration(max_iters), StopWhenChangeLess(Hn, atol)),\n]\n\nFinally, we run the optimization algorithms…\n\nrcbm = convex_bundle_method(Hn, f, ∂f, noise; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(Hn, f, ∂f, noise; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(Hn, f, ∂f, noise; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm)\n#\ncppa = cyclic_proximal_point(Hn, f, proxes, noise; cppa_kwargs...)\ncppa_result = get_solver_result(cppa)\ncppa_record = get_record(cppa)\n\n… And we benchmark their performance.\n\nif benchmarking\n    pba_bm = @benchmark proximal_bundle_method($Hn, $f, $∂f, $noise; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($Hn, $f, $∂f, $noise; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($Hn, $f, $∂f, $noise; $sgm_bm_kwargs...)\n    cppa_bm = @benchmark cyclic_proximal_point($Hn, $f, $proxes, $noise; $cppa_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\", \"CPPA\"]\n    records = [rcbm_record, pba_record, sgm_record, cppa_record]\n    results = [rcbm_result, pba_result, sgm_result, cppa_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n        median(cppa_bm).time * 1e-9,\n    ]\n    #\n    global B = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records],\n        [distance(Hn, noise, result) / length(noise) for result in results];\n        dims=2,\n    )\n    #\n    global header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\", \"Error\"]\n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            A = cat(first.(record), [r[2] for r in record]; dims=2)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(A, :auto);\n                header=[\"i\", \"cost\"],\n            )\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(B, :auto);\n            header=header,\n        )\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table…\n\nAlgorithm Iterations Time (s) Objective Error\nRCBM 5000 9.24497 0.139219 0.0139671\nPBA 3461 12.0163 0.140166 0.0134865\nSGM 5000 3.40848 0.146256 0.0124733\nCPPA 5000 3.05321 0.132025 0.0173612\n\nLastly, we plot the results.\n\nif export_result\n    # Convert hyperboloid points to Poincaré ball points\n    ball_b = convert.(PoincareBallPoint, rcbm_result)\n    ball_p = convert.(PoincareBallPoint, pba_result)\n    ball_s = convert.(PoincareBallPoint, sgm_result)\n    ball_c = convert.(PoincareBallPoint, cppa_result)\n    #\n    # Plot results\n    plot!(\n        ball_scene,\n        H,\n        ball_b;\n        markercolor=result_color,\n        markerstrokecolor=result_color,\n        label=\"Convex Bundle Method\",\n    )\n    #\n    # Write csv files\n    matrix_b = matrixify_Poincare_ball(ball_b)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-bundle_optimum.csv\"),\n        DataFrame(matrix_b, :auto);\n        header=[\"x\", \"y\"],\n    )\n    #\n    # Suppress some plots for clarity, since they are visually indistinguishable\n    # plot!(ball_scene, H, ball_p; label=\"Proximal Bundle Method\")\n    # plot!(ball_scene, H, ball_s; label=\"Subgradient Method\")\n    # plot!(ball_scene, H, ball_c; label=\"CPPA\")\n    display(ball_scene)\nend\n\n(Image: )","category":"section"},{"location":"examples/H2-Signal-TV/#Technical-details","page":"Hyperbolic Signal Denoising","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 16, 2025, 11:21:16.","category":"section"},{"location":"examples/H2-Signal-TV/#Literature","page":"Hyperbolic Signal Denoising","title":"Literature","text":"R. Bergmann, R. Herzog and H. Jasa. The Riemannian Convex Bundle Method, preprint (2024), arXiv:2402.13670.\n\n\n\nO. Ferreira and P. R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Optimization Theory and Applications 97, 93–104 (1998).\n\n\n\nN. Hoseini Monjezi, S. Nobakhtian and M. R. Pouryayevali. A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds. IMA Journal of Numerical Analysis 43, 293–325 (2023).\n\n\n\n","category":"section"},{"location":"examples/NCRPG-Grassmann/#An-NCRPG-run-on-the-Grassmann-manifold","page":"Grassmann Experiment","title":"An NCRPG run on the Grassmann manifold","text":"Hajg Jasa 2025-05-09","category":"section"},{"location":"examples/NCRPG-Grassmann/#Introduction","page":"Grassmann Experiment","title":"Introduction","text":"In this example we compare the Nonconvex Riemannian Proximal Gradient (NCRPG) method [BJJP25b] with the Cyclic Proximal Point Algorithm, which was introduced in [Bac14], on the space of symmetric positive definite matrices and on hyperbolic space. This example reproduces the results from [BJJP25b], Section 6.2. The numbers may vary slightly due to having run this notebook on a different machine.\n\nusing PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples","category":"section"},{"location":"examples/NCRPG-Grassmann/#The-Problem","page":"Grassmann Experiment","title":"The Problem","text":"Let mathcal M be a Riemannian manifold and q_1ldotsq_N in mathcal M denote N = 1000 Gaussian random data points. Let g colon mathcal M to mathbb R be defined by\n\ng(p) = sum_j = 1^N w_j  mathrmdist(p q_j)^2\n\nwhere w_j, j = 1 ldots N are positive weights such that sum_j = 1^N w_j = 1. In our experiments, we choose the weights w_j = frac12N. Observe that the function g is strongly convex with respect to the Riemannian metric on mathcal M. The Riemannian geometric median p^* of the dataset q_1ldotsq_N\n\nmathcal D = \n    q_1ldotsq_N  vert  q_j in mathcal Mtext for all  j = 1ldotsN\n\n\nis then defined as\n\n    p^* in operatorname*argmin_p in mathcal M g(p)\n\nLet now bar q in mathcal M be a given point, and let h colon mathcal M to mathbb R be defined by\n\nh(p) = alpha mathrmdist(p bar q)\n\nWe define our total objective function as f = g + h. Notice that this objective function is strongly convex with respect to the Riemannian metric on mathcal M thanks to g. The goal is to find the minimizer of f on mathcal M, which heuristically is an interpolation between the geometric median p^* and bar q.","category":"section"},{"location":"examples/NCRPG-Grassmann/#Numerical-Experiment","page":"Grassmann Experiment","title":"Numerical Experiment","text":"We initialize the experiment parameters, as well as some utility functions.\n\nrandom_seed = 100\nexperiment_name = \"NCRPG-Grassmann\"\nresults_folder = joinpath(@__DIR__, experiment_name)\n!isdir(results_folder) && mkdir(results_folder)\n\natol = 1e-7\nmax_iters = 5000\nN = 1000 # number of data points\nα = 1/2 # weight for the median component (h)\nδ = 1e-2 # parameter for the estimation of the constant stepsize\nσ = 1.0 # standard deviation for the Gaussian data points\nk_max_gr = 2.0 # maximum curvature of the Grassmann manifold\ngr_dims = [(5, 2), (10, 4), (50, 10), (100, 20), (200, 40)] # dimensions of the Grassmann manifold\n\n# Objective, gradient, and proxes\ng(M, p, data) = 1/2length(data) * sum(distance.(Ref(M), data, Ref(p)).^2)\ngrad_g(M, p, data) = 1/length(data) * sum(ManifoldDiff.grad_distance.(Ref(M), data, Ref(p), 2))\n#\nh(M, p, q) = α * distance(M, p, q)\nprox_h(M, λ, p, q) = ManifoldDiff.prox_distance(M, α * λ, q, p, 1)\n#\nf(M, p, data, q) = g(M, p, data) + h(M, p, q)\n# CPPA needs the proximal operators for the total objective\nfunction proxes_f(data, q)\n    proxes = Function[(M, λ, p) -> ManifoldDiff.prox_distance(M, λ / length(data), di, p, 2) for di in data]\n    push!(proxes, (M, λ, p) -> ManifoldDiff.prox_distance(M, α * λ, q, p, 1))\n    return proxes\nend\n# Function to shorten vectors if they are too long (for convexity reasons)\nfunction shorten_vectors!(M, p, vectors)\n    # If the i-th vector is of length greater than π/(2 * √(k_max_gr)), randomly shorten it\n    # to a length between 0 and π/(2 * √(k_max_gr)) (excluded)\n    for i in 1:length(vectors)\n        if norm(M, p, vectors[i]) ≥ π/(2 * √(k_max_gr))\n            # Randomly shorten the vector to a length between 0 and π/(2 * √(k_max_gr))\n            new_length = rand() * π/(2 * √(k_max_gr))\n            vectors[i] = new_length * (vectors[i] / norm(M, p, vectors[i]))\n        end\n    end\n    return vectors\nend\n# Function to generate points close to the given point p\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend\n# Functions for estimating the constant stepsize\nζ(δ) = π/((2 + δ) * √k_max_gr) * cot(π/((2 + δ) * √k_max_gr))\nfunction λ_δ(δ, M, h, grad_g, p1, k_max, D, N=100)\n    points = [close_point(M, p1, D/2) for _ in 1:N]\n    α_g = maximum([norm(M, p, grad_g(M, p)) for p in points])\n    α_1 = minimum([h(M, p) for p in points])\n    α_2 = maximum([h(M, p) for p in points])\n    π_k = π / √k_max\n    return (√(4*(α_2 - α_1)^2 + π_k^2/(2+δ)^2 * α_g^2) - 2*(α_2 - α_1))/(2*α_g^2)\nend\n\nWe introduce some keyword arguments for the solvers we will use in this experiment\n\npgm_kwargs(initial_stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        strategy=:nonconvex,\n        initial_stepsize=initial_stepsize,\n        stop_when_stepsize_less=atol,\n        ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs(initial_stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ProximalGradientMethodBacktracking(;\n        strategy=:nonconvex,\n        initial_stepsize=initial_stepsize,\n        stop_when_stepsize_less=atol,\n        ),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n#\npgm_kwargs_constant(stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\npgm_bm_kwargs_constant(stepsize) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => ConstantLength(stepsize),\n    :stopping_criterion => StopWhenAny(\n        StopWhenGradientMappingNormLess(atol), StopAfterIteration(max_iters)\n    ),\n]\n#\ncppa_kwargs(M) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenChangeLess(M, atol)\n    ),\n]\ncppa_bm_kwargs(M) = [\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(\n        StopAfterIteration(max_iters), StopWhenChangeLess(M, atol)\n    ),\n]\n\nBefore running the experiments, we initialize data collection functions that we will use later\n\nglobal col_names_1 = [\n    :Dimension,\n    :Time_1,\n    :Iterations_1,\n    :Objective_1,\n    :Time_2,\n    :Iterations_2,\n    :Objective_2,\n]\ncol_types_1 = [\n    Int64,\n    Float64,\n    Int64,\n    Float64,\n    Float64,\n    Int64,\n    Float64,\n]\nnamed_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)\nfunction initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1)\n    A1 = DataFrame(named_tuple_1)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"-Comparisons.csv\",\n        ),\n        A1;\n        header=false,\n    )\n    return A1\nend\n\nfunction export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1)\n    B1 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Time_1=times[1],\n        Iterations_1=maximum(first.(records[1])),\n        Objective_1=minimum([r[2] for r in records[1]]),\n        Time_2=times[2],\n        Iterations_2=maximum(first.(records[2])),\n        Objective_2=minimum([r[2] for r in records[2]]),\n    )\n    return B1\nend\nfunction write_dataframes(\n    B1,\n    results_folder,\n    experiment_name,\n    subexperiment_name\n)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"-Comparisons.csv\",\n        ),\n        B1;\n        append=true,\n    )\nend\n\nsubexperiment_name = \"Gr\"\nglobal A1_Gr = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n)\n\nfor (n, m) in gr_dims\n\n    Random.seed!(random_seed)\n\n    M = Grassmann(Int(n), Int(m))\n    anchor = rand(M)\n    vectors = [rand(M; vector_at=anchor, σ=σ) for _ in 1:N+2]\n    shorten_vectors!(M, anchor, vectors)\n    data = [exp(M, anchor, vectors[i]) for i in 1:N]\n    q_bar = exp(M, anchor, vectors[N+1]) # point for the median component, i.e. h\n    p0 = exp(M, anchor, vectors[N+2])\n\n    g_gr(M, p) = g(M, p, data)\n    h_gr(M, p) = h(M, p, q_bar)\n    grad_g_gr(M, p) = grad_g(M, p, data)\n    proxes_f_gr = proxes_f(data, q_bar)\n    prox_h_gr(M, λ, p) = prox_h(M, λ, p, q_bar)\n    f_gr(M, p) = f(M, p, data, q_bar)\n\n    D = 2maximum([distance(M, anchor, pt) for pt in vcat(data, [q_bar], [p0])])\n    L_g = 1.0 # since k_min = 0.0\n    λ = minimum([λ_δ(δ, M, h_gr, grad_g_gr, anchor, k_max_gr, D, 100) for _ in 1:10]) # estimate λ_δ\n    constant_stepsize = max(0, min(λ, ζ(δ)/L_g))\n    initial_stepsize = 1.0\n\n    # Optimization\n    pgm_constant = proximal_gradient_method(M, f_gr, g_gr, grad_g_gr, p0; prox_nonsmooth=prox_h_gr, pgm_kwargs_constant(constant_stepsize)...)\n    pgm_constant_result = get_solver_result(pgm_constant)\n    pgm_constant_record = get_record(pgm_constant)\n\n    # We can also use a backtracked stepsize\n    pgm = proximal_gradient_method(M, f_gr, g_gr, grad_g_gr, p0; prox_nonsmooth=prox_h_gr, pgm_kwargs(initial_stepsize)...)\n    pgm_result = get_solver_result(pgm)\n    pgm_record = get_record(pgm)\n\n    records = [\n        pgm_constant_record,\n        pgm_record,\n    ]\n\n    if benchmarking\n        pgm_constant_bm = @benchmark proximal_gradient_method($M, $f_gr, $g_gr, $grad_g_gr, $p0; prox_nonsmooth=$prox_h_gr, $pgm_bm_kwargs_constant($constant_stepsize)...)\n        pgm_bm = @benchmark proximal_gradient_method($M, $f_gr, $g_gr, $grad_g_gr, $p0; prox_nonsmooth=$prox_h_gr, $pgm_bm_kwargs($initial_stepsize)...)\n\n        times = [\n            median(pgm_constant_bm).time * 1e-9,\n            median(pgm_bm).time * 1e-9,\n        ]\n\n        B1 = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n        )\n\n        append!(A1_Gr, B1)\n        (export_table) && (write_dataframes(B1, results_folder, experiment_name, subexperiment_name))\n    end\nend\n\nWe can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the NCRPG with a constant stepsize, while columns 5 to 7 refer to a backtracked stepsize…\n\n| **Dimension** | **Time\\_1** | **Iterations\\_1** | **Objective\\_1** | **Time\\_2** | **Iterations\\_2** | **Objective\\_2** |\n|--------------:|------------:|------------------:|-----------------:|------------:|------------------:|-----------------:|\n|             6 |     0.37101 |                90 |         0.853678 |   0.0940748 |                12 |         0.853678 |\n|            24 |    0.542643 |                58 |         0.858344 |    0.168517 |                 9 |         0.858344 |\n|           400 |     2.30952 |                36 |         0.868749 |    0.697674 |                 6 |         0.868749 |\n|          1600 |     8.28298 |                35 |         0.871773 |     5.48798 |                 6 |         0.871773 |\n|          6400 |     36.8967 |                34 |         0.873426 |     10.3206 |                 5 |         0.873426 |","category":"section"},{"location":"examples/NCRPG-Grassmann/#Technical-details","page":"Grassmann Experiment","title":"Technical details","text":"This tutorial is cached. It was last run on the following package versions.\n\n<details class=\"code-fold\"> <summary>Code</summary>\n\nusing Pkg\nPkg.status()\n\n</details>\n\nStatus `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.15.6\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.31.0\n  [5ae59095] Colors v0.13.1\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.122\n⌅ [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [b964fa9f] LaTeXStrings v1.4.0\n  [d3d80556] LineSearches v7.4.0\n  [ee78f7c6] Makie v0.24.6\n  [af67fdf4] ManifoldDiff v0.4.5\n  [1cead3c2] Manifolds v0.11.0\n  [3362f125] ManifoldsBase v2.0.0\n  [0fc0a36d] Manopt v0.5.25\n  [5b8d5e80] ManoptExamples v0.1.16 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.41.1\n  [08abe8d2] PrettyTables v3.1.0\n  [6099a3de] PythonCall v0.9.28\n  [f468eda6] QuadraticModels v0.9.14\n  [1e40b3f8] RipQP v0.7.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n\nThis tutorial was last rendered October 15, 2025, 16:5:25.","category":"section"},{"location":"examples/NCRPG-Grassmann/#Literature","page":"Grassmann Experiment","title":"Literature","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nR. Bergmann, H. Jasa, P. J. John and M. Pfeffer. The Intrinsic Riemannian Proximal Gradient Method for Nononvex Optimization, preprint (2025), arXiv:2506.09775.\n\n\n\n","category":"section"}]
}
