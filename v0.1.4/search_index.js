var documenterSearchIndex = {"docs":
[{"location":"examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","page":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Ronny Bergmann 2023-11-06","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Introduction","page":"Frank Wolfe comparison","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"In this example we compare the Difference of Convex Algprithm (DCA) [BFSS23] with the Frank-Wolfe Algorithm, which was introduced in [WS22]. This example reproduces the results from [BFSS23], Section 7.3.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We consider the collowing constraint maximimization problem of the Fréchet mean on the symmetric positive definite matrices mathcal P(n) with the affine invariant metric. Let q_1ldotsq_m in mathcal P(n) be a set of points and mu_1ldotsmu_m be a set of weights, such that they sum to one. We consider then","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmax_pinmathcal C  h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"h(p) =\nsum_j=1^m mu_j d^2(pq_i)\nquad text where \nd^2(pq_i) = operatornametrbigl(\n  log^2(p^-frac12q_jp^-frac12)\nbig)\nqquadtextandqquad\nmathcal C =  pin mathcal M  bar Lpreceq p preceq bar U ","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"for a lower bound L and an upper bound U for the matrices in the positive definite sense A preceq B Leftrightarrow (B-A) is positive semi-definite","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"When every one of the weights mu_1 ldots mu_m are equal, this function h is known as the of the set q_1 dots q_m.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And for our example we set","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Random.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use as lower and upper bound the arithmetic and geometric mean L and U, respectively.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"L = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As a starting point, the Frank-Wolfe algorithm requires a feasible point. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p0 = (L+U)/2","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can check that it is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","page":"Frank Wolfe comparison","title":"Common Functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Given p in mathcal M, X in T_pmathcal M on the symmetric positive definite matrices M, this method computes the closed form solution to","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_qin  mathcal C langle X log_p qrangle\n  = operatorname*argmin_qin  mathcal C operatornametr(Slog(YqY))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"where mathcal C =  q  L preceq q preceq U , S = p^-12Xp^-12, and Y=p^-12.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The solution is given by Z=X^-1Qbigl( P^mathrmT-operatornamesgn(D)_+P+hatLbigr)Q^mathrmTX^-1,@ where S=QDQ^mathrmT is a diagonalization of S, hatU-hatL=P^mathrmTP with hatL=Q^mathrmTXLXQ and hatU=Q^mathrmTXUXQ, where -mboxsgn(D)_+ is the diagonal matrix","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatornamediagbigl(\n  -operatornamesgn(d_11)_+ ldots -operatornamesgn(d_nn)_+\nbigr)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"and D=(d_ij).","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closeed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","page":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We use g(p) = iota_mathcal C(p) as the indicator funtion of the set mathcal C. We use","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"So we can first check that p0 is feasible","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"g(p0,L,U) == 0.0","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"true","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Now setting","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pinmathcal M g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We look for a maximum of h, where g is minimal, i.e. g(p) is zero or in other words p in mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"The gradient of h can also be implemented in closed form as","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"grad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we can further define the cost, which will just be +infty outside of mathcal C. We define","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Here we can omit the gradient of g in the definition of operatornamegrad f, since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of mathcal C.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As the last step, we can provide the closed form solver for the DC sub problem given at iteration k by","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"operatorname*argmin_pin mathcal C\n  biglangle -operatornamegrad h(p^(k)) exp^-1_p^(k)pbigrangle","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Which we con compute","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For safety, we might want to avoid ending up at the boundary of mathcal C. That is we reduce the distance we walk towards the solution q a bit.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"function dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    α = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= α\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($α)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","page":"Frank Wolfe comparison","title":"The DoC solver run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s compare both methods when they have the same stopping criteria","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial F(p): -0.77661458292831\n# 30       F(p): -0.78442486496540 |Δp|: 0.05520655655191  |grad f(p)|: 0.17699237  |Δgrad f(p)|: 0.17569106\n# 60       F(p): -0.78442319013514 |Δp|: 0.01172173294316  |grad f(p)|: 0.17697321  |Δgrad f(p)|: 0.02211809\nAt iteration 78 the change of the gradient (3.1520524323368916e-13) was less than 1.0e-9.\n 16.243844 seconds (16.86 M allocations: 2.325 GiB, 2.67% gc time, 49.72% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 78 iterations\n\n## Parameters\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 300:  not reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 30]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Let’s extract the final point and look at its cost","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844223312703434","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"As well as whether (and how well) it is feasible, that is the following values should all be larger than zero.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (6.29650307104728e-15, 0.06710268482236606)\n (2.6399491947503855e-6, 0.06680333533680119)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For the statistics we extract the recordings from the state","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","page":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"For Frank wolfe, the cost is just defined as -h(p) but the minimisation is constraint to mathcal C, which is enfored by the oracle.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"f_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","page":"Frank Wolfe comparison","title":"The FW Solver Run","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Similarly we can run the Frank-Wolfe algorithm with","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"@time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Δp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Δgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"Initial f(x): -0.776615\n# 2000     f(x): -0.784420 |Δp|: 0.04611942377555  |grad f(p)|: 0.17693408  |Δgrad f(p)|: 0.17555618\n# 4000     f(x): -0.784421 |Δp|: 0.00372201631993  |grad f(p)|: 0.17694619  |Δgrad f(p)|: 0.00749427\n# 6000     f(x): -0.784422 |Δp|: 0.00205683506781  |grad f(p)|: 0.17695204  |Δgrad f(p)|: 0.00414088\n# 8000     f(x): -0.784422 |Δp|: 0.00140675676252  |grad f(p)|: 0.17695565  |Δgrad f(p)|: 0.00283200\n# 10000    f(x): -0.784422 |Δp|: 0.00106177438607  |grad f(p)|: 0.17695815  |Δgrad f(p)|: 0.00213746\nThe algorithm reached its maximal number of iterations (10000).\n609.538654 seconds (55.97 M allocations: 93.630 GiB, 1.01% gc time, 0.10% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n* sub solver state:\n    | InplaceEvaluation()\n\n## Stepsize\nDecreasingStepsize(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2)\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 10000:    reached\n    |Δp| < 1.0e-14: not reached\n    |Δgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"f(x): %f\"), (:Change, \" |Δp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Δgrad f(p)|: %0.8f\"), \"\\n\", 2000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And we take a look at this result as well","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"p1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"-0.7844220281765242","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And its feasibility","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"[extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)]","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"2-element Vector{Tuple{Float64, Float64}}:\n (4.912896446145126e-10, 0.06659173821660597)\n (3.2456549828225906e-5, 0.06713970236076447)","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Statistics","page":"Frank Wolfe comparison","title":"Statistics","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"We extract the recorded values","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"# DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q))","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"And let’s plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\")","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"This indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"On the other hand, Frank-Wolfe still has not reached this level function value after 10^4 iterations.","category":"page"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/#Literature","page":"Frank Wolfe comparison","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Frank-Wolfe/","page":"Frank Wolfe comparison","title":"Frank Wolfe comparison","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\n","category":"page"},{"location":"references/#Literature","page":"References","title":"Literature","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"S. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nM. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\nM. Weber and S. Sra. Riemannian Optimization via Frank-Wolfe Methods. Mathematical Programming 199, 525–556 (2022).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing/#Contributing-to-Manopt.jl","page":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The following is a set of guidelines to ManoptExamples.jl.","category":"page"},{"location":"contributing/#Table-of-Contents","page":"Contributing to ManoptExamples.jl","title":"Table of Contents","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Contributing to Manopt.jl\nTable of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd an objective\nCode style","category":"page"},{"location":"contributing/#I-just-have-a-question","page":"Contributing to ManoptExamples.jl","title":"I just have a question","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on our GitHub discussion.","category":"page"},{"location":"contributing/#How-can-I-file-an-issue?","page":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"page"},{"location":"contributing/#How-can-I-contribute?","page":"Contributing to ManoptExamples.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing/#Add-an-objective","page":"Contributing to ManoptExamples.jl","title":"Add an objective","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"The objective in Manopt.jl represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a Problem.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have a specific objective you would like to provide here, feel free to start a new file in the src/objectives/ folder in your own fork and propose it later as a Pull Request.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in src/functions/ and follows the naming scheme:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"cost functions are always of the form cost_ and a fitting name\ngradient functions are always of the the gradient_ and a fitting name, followed by an !","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"for in-place gradients and by !! if it is a struct that can provide both.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"It would be great if you could also add a small test for the functions and the problem you defined in the test/ section.","category":"page"},{"location":"contributing/#Add-an-example","page":"Contributing to ManoptExamples.jl","title":"Add an example","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding Quarto Markdown file to the examples/ folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the bib/literature.yaml file to add references (in CSL_YAML, which can for example be exported e.g. from Zotero).","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Add any packages you need to the examples/ environment (see the containting Project.toml). The examples will not be run on CI, but their rendered CommonMark outpout should be included in the list of examples in the documentation of this package.","category":"page"},{"location":"contributing/#Code-style","page":"Contributing to ManoptExamples.jl","title":"Code style","text":"","category":"section"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"We also follow a few internal conventions:","category":"page"},{"location":"contributing/","page":"Contributing to ManoptExamples.jl","title":"Contributing to ManoptExamples.jl","text":"Any implemented function should be accompanied by its mathematical formulae if a closed form exists.\nwithin a file the structs should come first and functions second. The only exception are constructors for the structs\nwithin both blocks an alphabetical order is preferable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including in-place/non-mutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature.\nAll import/using/include should be in the main module file.\nThere should only be a minimum of exports within this file, all problems should usually be later addressed as ManoptExamples.[...]\nthe Quarto Markdown files are excluded from this formatting.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","page":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Introduction","page":"A Benchmark","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"In this Benchmark we compare the Difference of Convex Algprithm (DCA) [BFSS23] and the Difference of Convex Proximal Point Algorithm (DCPPA) [SO15] which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from [BFSS23], Section 7.1.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"operatorname*argmin_pinmathcal M   g(p) - h(p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where ghcolon mathcal M to mathbb R are geodesically convex function on the Riemannian manifold mathcal M.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and we load a few nice colors","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#The-DC-Problem","page":"A Benchmark","title":"The DC Problem","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We start with defining the two convex functions gh and their gradients as well as the DC problem f and its gradient for the problem","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"    operatorname*argmin_pinmathcal M  bigl( logbigr(det(p)bigr)bigr)^4 - bigl(log det(p) bigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where the critical points obtain a functional value of -frac14.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where mathcal M is the manifold of symmetric positive definite (SPD) matrices with the affine invariant metric, which is the default.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We first define the corresponding functions","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"g(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and their gradients","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"grad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which we can use to verify that the gradients of g and h are correct. We use for that","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"n = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"to tall both checks","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, g, grad_g, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"check_gradient(M, h, grad_h, p0, X0; plot=true)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"which both pass the test. We continue to define their inplace variants","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"function grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And compare times for both algorithms, with a bit of debug output.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |δp|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |δp|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |δp|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |δp|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000005 |δp|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (7.619584706652929e-11) is less than 1.0e-10.\n  3.531235 seconds (8.71 M allocations: 628.709 MiB, 3.52% gc time, 67.16% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The cost is","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dca)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25000000000000006","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Similarly the DCPPA performs","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    λ=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|δp|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantStepsize(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n);","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Initial f(p): 137.679053470 \n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|δp|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|δp|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|δp|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|δp|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|δp|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|δp|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|δp|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.341931 seconds (2.55 M allocations: 180.474 MiB, 2.46% gc time, 59.94% compilation time)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"It needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"f(M, p_min_dcppa)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"-0.25","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","page":"A Benchmark","title":"Benchmark I: Time comparison","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"and run a benchmark for both algorithms","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"for n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        λ=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantStepsize(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"Since we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"dims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N]","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","page":"A Benchmark","title":"Benchmark II: Iterations and cost.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"As a second benchmark, let’s collect the number of iterations needed and the development of the cost over dimensions.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"N2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}()","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"@time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        λ = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantStepsize(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"The iterations are like","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2)","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"And for the developtment of the cost","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"where we can see that the DCA needs less iterations than the DCPPA.","category":"page"},{"location":"examples/Difference-of-Convex-Benchmark/#Literature","page":"A Benchmark","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Benchmark/","page":"A Benchmark","title":"A Benchmark","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\nJ. C. Souza and P. R. Oliveira. A proximal point algorithm for DC fuctions on Hadamard manifolds. Journal of Global Optimization 63, 797–810 (2015).\n\n\n\n","category":"page"},{"location":"objectives/#List-of-Objectives-defined-for-the-Examples","page":"Objectives","title":"List of Objectives defined for the Examples","text":"","category":"section"},{"location":"objectives/#Rayleigh","page":"Objectives","title":"Rayleigh Quotient on the Sphere","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rayleigh example (TODO) to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RayleighQuotient.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RayleighQuotientCost","page":"Objectives","title":"ManoptExamples.RayleighQuotientCost","text":"RayleighQuotientCost\n\nA functor representing the Rayleigh Quotient cost function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Rayleigh Quotient in two forms. Either\n\nf(p) = p^mathrmTApqquad p  𝕊^n-1\n\nor extended into the embedding as\n\nf(x) = x^mathrmTAx qquad x  ℝ^n\n\nwhich is not the orignal Rayleigh quotient for performance reasons, but useful if you want to use this as the Euclidean cost in the emedding of 𝕊^n-1.\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientCost(A)\n\nCreate the Rayleigh cost function.\n\nSee also\n\nRayleighQuotientGrad!!, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientGrad!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientGrad!!","text":"RayleighQuotientGrad!!\n\nA functor representing the Rayleigh Quotient gradient function.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the gradient of the Rayleigh Quotient in two forms. Either\n\noperatornamegrad f(p) = 2 Ap - 2 (p^mathrmTAp)*pqquad p  𝕊^n-1\n\nor taking the Euclidean gradient of the Rayleigh quotient on the sphere as\n\nf(x) = 2Ax qquad x  ℝ^n\n\nFor details, see Example 3.62 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientGrad!!(A)\n\nCreate the Rayleigh quotient gradient function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientHess!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RayleighQuotientHess!!","page":"Objectives","title":"ManoptExamples.RayleighQuotientHess!!","text":"RayleighQuotientHess!!\n\nA functor representing the Rayleigh Quotient Hessian.\n\nLet A  ℝ^nn be a symmetric matrix. Then we can specify the Hessian of the Rayleigh Quotient in two forms. Either\n\noperatornameHess f(p)X = 2 bigl(AX - (p^mathrmTAX)p - (p^mathrmTAp)Xbigr)qquad p  𝕊^n-1 X in T_p𝕊^n-1\n\nor taking the Euclidean Hessian of the Rayleigh quotient on the sphere as\n\n^2f(x)V = 2AV qquad x V  ℝ^n\n\nFor details, see Example 5.27 of [Bou23].\n\nFields\n\nA – storing the matrix internally\n\nConstructor\n\nRayleighQuotientHess!!(A)\n\nCreate the Rayleigh quotient Hessian function.\n\nSee also\n\nRayleighQuotientCost, RayleighQuotientGrad!!\n\n\n\n\n\n","category":"type"},{"location":"objectives/#BezierCurves","page":"Objectives","title":"Bézier Curves","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Bezier Curves example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/BezierCurves.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.BezierSegment","page":"Objectives","title":"ManoptExamples.BezierSegment","text":"BezierSegment\n\nA type to capture a Bezier segment. With n points, a Bézier segment of degree n-1 is stored. On the Euclidean manifold, this yields a polynomial of degree n-1.\n\nThis type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an AbstractVector of BezierSegments where each of the points might be a nested array on a PowerManifold already.\n\nNot that this can also be used to represent tangent vectors on the control points of a segment.\n\nSee also: de_Casteljau.\n\nConstructor\n\nBezierSegment(pts::AbstractVector)\n\nGiven an abstract vector of pts generate the corresponding Bézier segment.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}, AbstractFloat, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.L2_acceleration_Bezier","text":"L2_acceleration_Bezier(M,B,pts,λ,d)\n\ncompute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e.\n\nfracλ2sum_i=0^N d_mathcal M(d_i c_B(i))^2+\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i and d_2 refers to the second order absolute difference second_order_Total_Variation (squared), the junction points are denoted by p_i, and to each p_i corresponds one data item in the manifold points given in d. For details on the acceleration approximation, see acceleration_Bezier. Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_L2_acceleration_Bezier, acceleration_Bezier, grad_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}}} where P","page":"Objectives","title":"ManoptExamples.acceleration_Bezier","text":"acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P}\n\ncompute the value of the discrete Acceleration of the composite Bezier curve\n\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i, i=1N, and d_2 refers to the second order absolute difference second_order_Total_Variation (squared). Note that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nThis acceleration discretization was introduced in Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, grad_L2_acceleration_Bezier\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    T::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    T::AbstractVector,\n    X::AbstractVector,\n)\n\nEvaluate the adjoint of the differential with respect to the controlpoints at several times T. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\n\nevaluate the adjoint of the differential of a composite Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, AbstractVector}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, Any}","page":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","text":"adjoint_differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t, η)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t,\n    η,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a point t01 on the curve and a tangent vector ηT_β(t)mathcal M. This can be computed in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.de_Casteljau-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any}}","page":"Objectives","title":"ManoptExamples.de_Casteljau","text":"de_Casteljau(M::AbstractManifold, b::BezierSegment NTuple{N,P}) -> Function\n\nreturn the Bézier curve β(b_0b_n) 01  mathcal M defined by the control points b_0b_nmathcal M, nmathbb N, as a BezierSegment. This function implements de Casteljau's algorithm Casteljau, 1959, Casteljau, 1963 generalized to manifolds by Popiel, Noakes, J Approx Theo, 2007: Let γ_ab(t) denote the shortest geodesic connecting abmathcal M. Then the curve is defined by the recursion\n\nbeginaligned\n    β(tb_0b_1) = gamma_b_0b_1(t)\n    β(tb_0b_n) = gamma_β(tb_0b_n-1) β(tb_1b_n)(t)\nendaligned\n\nand P is the type of a point on the Manifold M.\n\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) -> Function\n\nGiven a vector of Bézier segments, i.e. a vector of control points B=bigl( (b_00b_n_00)(b_0m b_n_mm) bigr), where the different segments might be of different degree(s) n_0n_m. The resulting composite Bézier curve c_B0m  mathcal M consists of m segments which are Bézier curves.\n\nc_B(t) =\n    begincases\n        β(t b_00b_n_00)  text if  t 01\n        β(t-i b_0ib_n_ii)  text if \n            t(ii+1 quad i1m-1\n    endcases\n\nde_Casteljau(M::AbstractManifold, b::BezierSegment, t::Real)\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}, t::Real)\nde_Casteljau(M::AbstractManifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\nde_Casteljau(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n) -> AbstractVector\n\nEvaluate the Bézier curve at time t or at times t in T.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Θ::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at the points in T, which are elementwise in 0N, and each depending the corresponding segment(s). Here, N is the length of B. For the mutating variant the result is computed in Θ.\n\nSee de_Casteljau for more details on the curve and Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at t0N, which depends only on the corresponding segment. Here, N is the length of B. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at the points T, elementwise in t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","text":"differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t::Float, X::BezierSegment)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    t,\n    X::BezierSegment\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X given in the tangent spaces of the control points. The result is the “change” of the curve at t01. The computation can be done in place of Y.\n\nSee de_Casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degree-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment}","page":"Objectives","title":"ManoptExamples.get_Bezier_degree","text":"get_Bezier_degree(M::AbstractManifold, b::BezierSegment)\n\nreturn the degree of the Bézier curve represented by the tuple b of control points on the manifold M, i.e. the number of points minus 1.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_degrees-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_degrees","text":"get_Bezier_degrees(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\n\nreturn the degrees of the components of a composite Bézier curve represented by tuples in B containing points on the manifold M.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_inner_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_inner_points","text":"get_Bezier_inner_points(M::AbstractManifold, B::AbstractVector{<:BezierSegment} )\nget_Bezier_inner_points(M::AbstractManifold, b::BezierSegment)\n\nreturns the inner (i.e. despite start and end) points of the segments of the composite Bézier curve specified by the control points B. For a single segment b, its inner points are returned\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junction_tangent_vectors-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","page":"Objectives","title":"ManoptExamples.get_Bezier_junction_tangent_vectors","text":"get_Bezier_junction_tangent_vectors(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junction_tangent_vectors(M::AbstractManifold, b::BezierSegment)\n\nreturns the tangent vectors at start and end points of the composite Bézier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points B, either a vector of segments of controlpoints.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.get_Bezier_junctions","page":"Objectives","title":"ManoptExamples.get_Bezier_junctions","text":"get_Bezier_junctions(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junctions(M::AbstractManifold, b::BezierSegment)\n\nreturns the start and end point(s) of the segments of the composite Bézier curve specified by the control points B. For just one segment b, its start and end points are returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_points","page":"Objectives","title":"ManoptExamples.get_Bezier_points","text":"get_Bezier_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_Bezier_points(M::AbstractManifold, b::BezierSegment, reduce::Symbol=:default)\n\nreturns the control points of the segments of the composite Bézier curve specified by the control points B, either a vector of segments of controlpoints or a.\n\nThis method reduces the points depending on the optional reduce symbol\n\n:default – no reduction is performed\n:continuous – for a continuous function, the junction points are doubled at b_0i=b_n_i-1i-1, so only b_0i is in the vector.\n:differentiable – for a differentiable function additionally log_b_0ib_1i = -log_b_n_i-1i-1b_n_i-1-1i-1 holds. hence b_n_i-1-1i-1 is omitted.\n\nIf only one segment is given, all points of b – i.e. b.pts is returned.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.get_Bezier_segments-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any, Symbol}} where P","page":"Objectives","title":"ManoptExamples.get_Bezier_segments","text":"get_Bezier_segments(M::AbstractManifold, c::AbstractArray{P}, d[, s::Symbol=:default])\n\nreturns the array of BezierSegments B of a composite Bézier curve reconstructed from an array c of points on the manifold M and an array of degrees d.\n\nThere are a few (reduced) representations that can get extended; see also get_Bezier_points. For ease of the following, let c=(c_1c_k) and d=(d_1d_m), where m denotes the number of components the composite Bézier curve consists of. Then\n\n:default – k = m + sum_i=1^m d_i since each component requires one point more than its degree. The points are then ordered in tuples, i.e.\nB = bigl c_1c_d_1+1 (c_d_1+2c_d_1+d_2+2 c_k-m+1+d_mc_k bigr\n:continuous – k = 1+ sum_i=1m d_i, since for a continuous curve start and end point of successive components are the same, so the very first start point and the end points are stored.\nB = bigl c_1c_d_1+1 c_d_1+1c_d_1+d_2+1 c_k-1+d_mb_k) bigr\n:differentiable – for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence k = 2 - m + sum_i=1m d_i and at a junction point b_n with its given prior point c_n-1, i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as b = exp_c_n(-log_c_n c_n-1) such that the assumed differentiability holds\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector, Any, AbstractVector{P}}} where P","page":"Objectives","title":"ManoptExamples.grad_L2_acceleration_Bezier","text":"grad_L2_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector,\n    λ,\n    d::AbstractVector{P}\n) where {P}\n\ncompute the gradient of the discretized acceleration of a composite Bézier curve on the Manifold M with respect to its control points B together with a data term that relates the junction points p_i to the data d with a weight λ compared to the acceleration. The curve is evaluated at the points given in pts (elementwise in 0N), where N is the number of segments of the Bézier curve. The summands are grad_distance for the data term and grad_acceleration_Bezier for the acceleration with interpolation constrains. Here the get_Bezier_junctions are included in the optimization, i.e. setting λ=0 yields the unconstrained acceleration minimization. Note that this is ill-posed, since any Bézier curve identical to a geodesic is a minimizer.\n\nNote that the Bézier-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_acceleration_Bezier, L2_acceleration_Bezier, acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_acceleration_Bezier-Tuple{ManifoldsBase.AbstractManifold, AbstractVector, AbstractVector{<:Integer}, AbstractVector}","page":"Objectives","title":"ManoptExamples.grad_acceleration_Bezier","text":"grad_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector,\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector\n)\n\ncompute the gradient of the discretized acceleration of a (composite) Bézier curve c_B(t) on the Manifold M with respect to its control points B given as a point on the PowerManifold assuming C1 conditions and known degrees. The curve is evaluated at the points given in T (elementwise in 0N, where N is the number of segments of the Bézier curve). The get_Bezier_junctions are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see grad_L2_acceleration_Bezier and set λ=0 therein. This gradient is computed using adjoint_Jacobi_fields. For details, see Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018. See de_Casteljau for more details on the curve.\n\nSee also\n\nacceleration_Bezier,  grad_L2_acceleration_Bezier, L2_acceleration_Bezier.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#RiemannianMean","page":"Objectives","title":"Riemannian Mean","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Riemannian mean example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RiemannianMean.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RiemannianMeanCost","page":"Objectives","title":"ManoptExamples.RiemannianMeanCost","text":"RiemannianMeanCost{P}\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\nf(p) = sum_j=i^N d_mathcal M^2(d_i p)\n\nwhere d_mathcal M is the distance on a Riemannian manifold.\n\nConstructor\n\nRiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P}\n\nInitialize the cost function to a data set data of points on a manfiold M, where each point is of type P.\n\nSee also\n\nRiemannianMeanGradient!!, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RiemannianMeanGradient!!","page":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","text":"RiemannianMeanGradient!!{P} where P\n\nA functor representing the Riemannian center of mass (or Riemannian mean) cost function.\n\nFor a given set of points d_1ldotsd_N this cost function is defined as\n\noperatornamegradf(p) = sum_j=i^N log_p d_i\n\nwhere d_mathcal M is the distance on a Riemannian manifold and we employ grad_distance to compute the single summands.\n\nThis functor provides both the allocating variant grad_f(M,p) as well as the in-place variant grad_f!(M, X, p) which computes the gradient in-place of X.\n\nConstructors\n\nRiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T}\n\nGenerate the Riemannian mean gradient based on some data points data an intial tangent vector initial_vector. If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant.\n\nRiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T}\n\nInitialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold M is provideed.\n\nSee also\n\nRiemannianMeanCost, Riemannian_mean_objective\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.Riemannian_mean_objective-Tuple{AbstractVector}","page":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","text":"Riemannian_mean_objective(data, initial_vector=nothing, evaluation=AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n)\n\nGenerate the objective for the Riemannian mean task for some given vector of data points on the Riemannian manifold M.\n\nSee also\n\nRiemannianMeanCost, RiemannianMeanGradient!!\n\nnote: Note\nThe first constructor should only be used if an additional storage of the vector is not feasible, since initialising the initial_vector to nothing disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#RobustPCA","page":"Objectives","title":"Robust PCA","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Robust PCA example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/RobustPCA.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RobustPCACost","page":"Objectives","title":"ManoptExamples.RobustPCACost","text":"RobustPCACost{D,F}\n\nA functor representing the Riemannian robust PCA function on the Grassmann manifold. For some given (column) data Dmathbb R^dtimes n the cost function is defined on some operatornameGr(dm), mn as the sum of the distances of the columns D_i to the subspace spanned by pinoperatornameGr(dm) (represented as a point on the Stiefel manifold). The function reads\n\nf(U) = frac1nsum_i=1^n lVert pp^mathrmTD_i - D_irVert\n\nThis cost additionally provides a Huber regularisation of the cost, that is for some ε0 one use ℓ_ε(x) = sqrtx^2+ε^2 - ε in\n\nf_ε(p) = frac1nsum_i=1^n ℓ_εbigl(lVert pp^mathrmTD_i - D_irVertbigr)\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCACost(data::AbstractMatrix, ε=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, ε=1.0)\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RobustPCAGrad!!","page":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","text":"RobustPCAGrad!!{D,F}\n\nA functor representing the Riemannian robust PCA gradient on the Grassmann manifold. For some given (column) data Xmathbb R^ptimes n the gradient of the RobustPCACost can be computed by projecting the Euclidean gradient onto the corresponding tangent space.\n\nNote that this is a mutable struct so you can adapt the ε later on.\n\nConstructor\n\nRobustPCAGrad!!(data, ε=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, ε=1.0; evaluation=AllocatingEvaluation())\n\nInitialize the robust PCA cost to some data D, and some regularization ε. The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the evaluation= keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.robust_PCA_objective","page":"Objectives","title":"ManoptExamples.robust_PCA_objective","text":"robust_PCA_objective(data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, ε=1.0; evaluation=AllocatingEvaluton())\n\nGenerate the objective for the robust PCA task for some given data D and Huber regularization parameter ε.\n\nSee also\n\nRobustPCACost, RobustPCAGrad!!\n\nnote: Note\nSince the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the evaluation, indeed the gradient always allows for both the allocating and the inplace variant to be used, though that keyword is used to setup the objective.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#Rosenbrock","page":"Objectives","title":"Rosenbrock Function","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Rosenbrock example  and The Difference of Convex Rosenbrock Example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/Rosenbrock.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.RosenbrockCost","page":"Objectives","title":"ManoptExamples.RosenbrockCost","text":"RosenbrockCost\n\nProvide the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nf(mathcal M p) = a(p_1^2-p_2)^2 + (p_1-b)^2\n\nwhich means that for the 2D case, the manifold mathcal M is ignored.\n\nSee also 📖 Rosenbrock (with slightly different parameter naming).\n\nConstructor\n\nf = Rosenbrock(a,b)\n\ngenerates the struct/function of the Rosenbrock cost.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockGradient!!","page":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","text":"RosenbrockGradient\n\nProvide Eclidean GRadient fo the Rosenbrock function in 2D, i.e. for some ab  ℝ\n\nnabla f(mathcal M p) = beginpmatrix\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \n    -2a(p_1^2-p_2)\nendpmatrix\n\ni.e. also here the manifold is ignored.\n\nConstructor\n\nRosenbrockGradient(a,b)\n\nFunctors\n\ngrad_f!!(M,p)\ngrad_f!!(M, X, p)\n\nevaluate the gradient at p the manifoldmathcal M is ignored.\n\n\n\n\n\n","category":"type"},{"location":"objectives/#ManoptExamples.RosenbrockMetric","page":"Objectives","title":"ManoptExamples.RosenbrockMetric","text":"RosenbrockMetric <: AbstractMetric\n\nA metric related to the Rosenbrock problem, where the metric at a point pmathbb R^2 is given by\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\nwhere the mathrmRb stands for Rosenbrock\n\n\n\n\n\n","category":"type"},{"location":"objectives/#Base.exp-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","page":"Objectives","title":"Base.exp","text":"q = exp(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, q, p, X)\n\nCompute the exponential map with respect to the RosenbrockMetric.\n\n    q = beginpmatrix p_1 + X_1  p_2+X_2+X_1^2endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Base.log-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any}","page":"Objectives","title":"Base.log","text":"X = log(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, X, p, q)\n\nCompute the logarithmic map with respect to the RosenbrockMetric. The formula reads for any j  1m\n\nX = beginpmatrix\n  q_1 - p_1 \n  q_2 - p_2 + (q_1 - p_1)^2\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.inverse_local_metric-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.inverse_local_metric","text":"inverse_local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the inverse of the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG^-1_p =\nbeginpmatrix\n    1  2p_1\n    2p_1  1+4p_1^2 \nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Manifolds.local_metric-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any}","page":"Objectives","title":"Manifolds.local_metric","text":"local_metric(::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p)\n\nReturn the local metric matrix of the RosenbrockMetric in the canonical unit vector basis of the tangent space T_pmathbb R^2 given as\n\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.change_representer-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","page":"Objectives","title":"ManifoldsBase.change_representer","text":"Y = change_representer(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, Y, ::EuclideanMetric, p, X)\n\nGiven the Euclidan gradient X at p, this function computes the corresponting Riesz representer Ysuch that⟨X,Z⟩ = ⟨ Y, Z ⟩_{\\mathrm{Rb},p}holds for allZ, in other wordsY = G(p)^{-1}X`.\n\nthis function is used in riemannian_gradient to convert a Euclidean into a Riemannian gradient.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManifoldsBase.inner-Tuple{Manifolds.MetricManifold{ℝ, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ℝ}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","page":"Objectives","title":"ManifoldsBase.inner","text":"inner(M::MetricManifold{ℝ,Euclidean{Tuple{2},ℝ},RosenbrockMetric}, p, X, Y)\n\nCompute the inner product on mathbb R^2 with respect to the RosenbrockMetric, i.e. for XY in T_pmathcal M we have\n\nXY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1\n  -2p_1  1\nendpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Rosenbrock_objective","page":"Objectives","title":"ManoptExamples.Rosenbrock_objective","text":"Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation())\n\nReturn the gradient objective of the Rosenbrock example.\n\nSee also RosenbrockCost, RosenbrockGradient!!\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","page":"Objectives","title":"ManoptExamples.minimizer","text":"minimizer(::RosenbrockCost)\n\nReturn the minimizer of the RosenbrockCost, which is given by\n\np^* = beginpmatrix bb^2 endpmatrix\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Total-Variation","page":"Objectives","title":"Total Variation","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"See the Total Variation example to see these in use.","category":"page"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"Modules = [ManoptExamples]\nPages = [\"objectives/TotalVariation.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"objectives/#ManoptExamples.Intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.Intrinsic_infimal_convolution_TV12","text":"Intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are second_order_Total_Variation and Total_Variation. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( βmathrmTV(v) + (1-β)mathrmTV_2(w) bigr)\n\nfor more details see [BFPS17, BFPS18].\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation-NTuple{4, Any}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation","text":"L2_Total_Variation(M, p_data, α, p)\n\ncompute the ℓ^2-TV functional on the PowerManifold M for given (fixed) data p_data (on M), a nonnegative weight α, and evaluated at p (on M), i.e.\n\nE(p) = d_mathcal M^2(fp) + alpha operatornameTV(p)\n\nSee also\n\nTotal_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_Total_Variation_1_2-Tuple{ManifoldsBase.PowerManifold, Vararg{Any, 4}}","page":"Objectives","title":"ManoptExamples.L2_Total_Variation_1_2","text":"L2_Total_Variation_1_2(M, f, α, β, x)\n\ncompute the ℓ^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + βoperatornameTV_2(x)\n\nSee also\n\nTotal_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.L2_second_order_Total_Variation-Tuple{ManifoldsBase.PowerManifold, Any, Any, Any}","page":"Objectives","title":"ManoptExamples.L2_second_order_Total_Variation","text":"L2_second_order_Total_Variation(M, f, β, x)\n\ncompute the ℓ^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + βoperatornameTV_2(x)\n\nas used in [BBSW16].\n\nSee also\n\nsecond_order_Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.Total_Variation","page":"Objectives","title":"ManoptExamples.Total_Variation","text":"Total_Variation(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1kcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nsee [WDS14] for more details. In long function names, this might be shortened to TV1 and the 1 might be ommitted if only total variation is present.\n\nSee also\n\ngrad_Total_Variation, prox_Total_Variation, second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\nadjoint_differential_forward_logs!(M, Y, p, X)\n\nCompute the adjoint differential of forward_logs F occurring, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n. The adjoint differential can be computed in place of Y.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOutput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.differential_forward_logs-Tuple{ManifoldsBase.PowerManifold, Any, Any}","page":"Objectives","title":"ManoptExamples.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\ndifferential_forward_logs!(M, Y, p, X)\n\ncompute the differential of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i  mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the   logs, where mathcal N is the power manifold with the number of dimensions added   to size(x). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽, TM, TSize, TPR}, Any}} where {𝔽, TM, TSize, TPR}","page":"Objectives","title":"ManoptExamples.forward_logs","text":"Y = forward_logs(M,x)\nforward_logs!(M, Y, x)\n\ncompute the forward logs F (generalizing forward differences) occurring, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i. This can also be done in place of ξ.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nY – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is the power manifold with the number of dimensions added to size(x). The computation can be done in place of Y.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, λ, x[, p=1])\ngrad_Total_Variation!(M, X, λ, x[, p=1])\n\nCompute the (sub)gradient partial F of all forward differences occurring, in the power manifold array, i.e. of the function\n\nF(x) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOutput\n\nX – resulting tangent vector in T_xmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Any}} where T","page":"Objectives","title":"ManoptExamples.grad_Total_Variation","text":"X = grad_Total_Variation(M, (x,y)[, p=1])\ngrad_Total_Variation!(M, X, (x,y)[, p=1])\n\ncompute the (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","page":"Objectives","title":"ManoptExamples.grad_intrinsic_infimal_convolution_TV12","text":"grad_u, grad_v = grad_intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see second_order_Total_Variation, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nβmathrmTV(v) + (1-β)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, grad_Total_Variation and grad_second_order_Total_Variation, respectively.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"Y = grad_second_order_Total_Variation(M, q[, p=1])\ngrad_second_order_Total_Variation!(M, Y, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1 q_2 q_3) with respect to all three components of qmathcal M^3, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\nmathcal C = bigl c  mathcal M   g(tfrac12q_1q_3) text for some geodesic gbigr\n\ndenote the mid points between q_1 and q_3 on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(q_1q_2q_3) = min_c  mathcal C_q_1q_3 d(c q_2)\n\nWhile the (sub)gradient with respect to q_2 is easy, the other two require the evaluation of an adjoint_Jacobi_field.\n\nThe derivation of this gradient can be found in [BBSW16].\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.grad_second_order_Total_Variation-2","page":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","text":"grad_second_order_Total_Variation(M::PowerManifold, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1q_2q_3) with respect to all q_1q_2q_3 occurring along any array dimension in the point q, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.project_collaborative_TV","page":"Objectives","title":"ManoptExamples.project_collaborative_TV","text":"project_collaborative_TV(M, λ, x, Ξ[, p=2,q=1])\nproject_collaborative_TV!(M, Θ, λ, x, Ξ[, p=2,q=1])\n\ncompute the projection onto collaborative Norm unit (or α-) ball, i.e. of the function\n\nF^q(x) = sum_imathcal G\n  Bigl( sum_jmathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for xmathcal M and mathcal I_i is the set of its forward neighbors. The computation can also be done in place of Θ.\n\nThis is adopted from the paper Duran, Möller, Sbert, Cremers, SIAM J Imag Sci, 2016, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"ξ = prox_Total_Variation(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all forward differences occurring in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting  point containing with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.prox_Total_Variation","text":"[y1,y2] = prox_Total_Variation(M, λ, [x1,x2] [,p=1])\nprox_Total_Variation!(M, [y1,y2] λ, [x1,x2] [,p=1])\n\nCompute the proximal map operatornameprox_λvarphi of φ(xy) = d_mathcal M^p(xy) with parameter λ. A derivation of this closed form solution is given in see [WDS14].\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_λφ((x1,x2)). The result can also be computed in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.prox_parallel_TV","page":"Objectives","title":"ManoptExamples.prox_parallel_TV","text":"y = prox_parallel_TV(M, λ, x [,p=1])\nprox_parallel_TV!(M, y, λ, x [,p=1])\n\ncompute the proximal maps operatornameprox_λφ of all forward differences occurring in the power manifold array, i.e. φ(x_ix_j) = d_mathcal M^p(x_ix_j) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements). The computation can also be done in place.\n\nSee also prox_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Any, Tuple{T, T, T}}, Tuple{ManifoldsBase.AbstractManifold, Any, Tuple{T, T, T}, Int64}} where T","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"(y1,y2,y3) = prox_second_order_Total_Variation(M,λ,(x1,x2,x3),[p=1], kwargs...)\nprox_second_order_Total_Variation!(M, y, λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_λvarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2. The result can be computed in place of y.\n\nNote that this function does not have a closed form solution but is solbed by a few steps of the subgradient mehtod from manopt.jl by default. See [BBSW16] for a derivation.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map. The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.prox_second_order_Total_Variation-Union{Tuple{T}, Tuple{N}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any, Int64}} where {N, T}","page":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","text":"y = prox_second_order_Total_Variation(M, λ, x[, p=1])\nprox_second_order_Total_Variation!(M, y, λ, x[, p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all centered second order differences occurring in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a manifold M\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting point with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifoldmanifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1kcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i() denotes the mid point between its two arguments that is nearest to x_i, see [BBSW16] for a derivation.\n\nIn long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation\n\n\n\n\n\n","category":"function"},{"location":"objectives/#ManoptExamples.second_order_Total_Variation-Union{Tuple{T}, Tuple{MT}, Tuple{MT, Tuple{T, T, T}}, Tuple{MT, Tuple{T, T, T}, Any}} where {MT<:ManifoldsBase.AbstractManifold, T}","page":"Objectives","title":"ManoptExamples.second_order_Total_Variation","text":"second_order_Total_Variation(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the function reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nsee [BBSW16] for a derivation. In long function names, this might be shortened to TV2.\n\nSee also\n\ngrad_second_order_Total_Variation, prox_second_order_Total_Variation, Total_Variation\n\n\n\n\n\n","category":"method"},{"location":"objectives/#Literature","page":"Objectives","title":"Literature","text":"","category":"section"},{"location":"objectives/","page":"Objectives","title":"Objectives","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Priors with coupled first and second order differences for manifold-valued image processing. Journal of Mathematical Imaging and Vision 60, 1459–1481 (2018), arXiv:1709.01343.\n\n\n\nR. Bergmann, J. H. Fitschen, J. Persch and G. Steidl. Infimal convolution coupling of first and second order differences on manifold-valued images. In: Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4–8, 2017, Proceedings, edited by F. Lauze, Y. Dong and A. B. Dahl (Springer International Publishing, 2017); pp. 447–459.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nN. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\nP. de Casteljau. Outillage methodes calcul (Enveloppe Soleau 40.040, Institute National de la Propriété Industrielle, Paris., 1959).\n\n\n\nP. de Casteljau. Courbes et surfaces à pôles (Microfiche P 4147-1, Institute National de la Propriété Industrielle, Paris., 1963).\n\n\n\nJ. Duran, M. Moeller, C. Sbert and D. Cremers. Collaborative Total Variation: A General Framework for Vectorial TV Models. SIAM Journal on Imaging Sciences 9, 116–151 (2016), arXiv:1508.01308.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"examples/#List-of-Examples","page":"Overview","title":"List of Examples","text":"","category":"section"},{"location":"examples/","page":"Overview","title":"Overview","text":"Name provides Documentation Comment\nA Benchmark for Difference of Convex contains a few simple functions  \nBézier Curves and Minimizing their Acceleration tools Bézier curves and their acceleration 📚 \nSolving Rosenbrock with Difference of Convex DoC split of Rosenbrock 📚 uses a Rosenbrock based metric\nDifference of Convex vs. Frank-Wolfe   closed-form sub solver\nRiemannian Mean f, operatornamegradf (A/I), objective 📚 \nRobust PCA f, operatornamegradf (A/I), objective 📚 \nRosenbrock f, operatornamegradf (A/I), objective, minimizer 📚 \nThe Rayleigh Quotient f, operatornamegradf (A/I), operatornameHessf (A/I), objective 📚 \nTotal Variation Minimization f, operatornameproxf (A/I), objective 📚 ","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"Symbols:","category":"page"},{"location":"examples/","page":"Overview","title":"Overview","text":"A Allocating variant\nI In-place variant\n📚 link to documented functions in the documentation","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","page":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Introduction","page":"Rosenbrock Metric","title":"Introduction","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"This example illustrates how the 📖 Rosenbrock problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [BFSS23], Section 7.2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Both the Rosenbrock problem","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"    operatorname*argmin_xinmathbb R^2 abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where ab0 and usually b=1 and a gg b, we know the minimizer x^* = (bb^2)^mathrmT, and also the (Euclidean) gradient","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"nabla f(x) =\n  beginpmatrix\n  4a(x_1^2-x_2) -2a(x_1^2-x_2)\n  endpmatrix\n  +\n  beginpmatrix\n  2(x_1-b) 0\n  endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"They are even available already here in ManifoldExamples.jl, see RosenbrockCost and RosenbrockGradient!!.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Furthermore, the RosenbrockMetric can be used on mathbb R^2, that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"XY_mathrmRbp = X^mathrmTG_pY qquad\nG_p = beginpmatrix\n  1+4p_1^2  -2p_1 \n  -2p_1  1\nendpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e. using a gradient but not a Hessian.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"The Euclidean Gradient\nThe Riemannian gradient descent with respect to the RosenbrockMetric\nThe Euclidean Difference of Convex Algorithm\nThe Riemannian Difference of Convex Algorithm respect to the RosenbrockMetric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Where we obtain a difference of convex problem by writing","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f(x) = abigl( x_1^2-x_2bigr)^2 + bigl(x_1-bbigr)^2\n = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 - bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"that is","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"g(x) = abigl( x_1^2-x_2bigr)^2 + 2bigl(x_1-bbigr)^2 quadtext and quad h(x) = bigl(x_1-bbigr)^2","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_manopt_parameter!\nRandom.seed!(42)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"To emphasize the effect, we choose a quite large value of a.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"a = 2*10^5\nb = 1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and use the starting point and a direction to check gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"p0 = [0.1, 0.2]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Euclidean gradient we can just use the same approach as in the Rosenbrock example","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M = ℝ^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n∇f!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"define a common debug vector","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"debug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|δp|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ]","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and call the gradient descent algorithm","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Eucl_GD_state = gradient_descent(M, f, ∇f!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000000 F(x): 8.9937e-06 |δp|: 1.3835e+00 | |grad f|: 8.170355e-03\n# 20000000 F(x): 2.9474e-09 |δp|: 6.5764e-03 | |grad f|: 1.419191e-04\n# 30000000 F(x): 9.8376e-13 |δp|: 1.1918e-04 | |grad f|: 2.526295e-06\n# 40000000 F(x): 3.2830e-16 |δp|: 2.1773e-06 | |grad f|: 4.526313e-08\n# 50000000 F(x): 1.0154e-19 |δp|: 3.9803e-08 | |grad f|: 6.838240e-10\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 53073227 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","page":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the Riemannian case, we define","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"MetricManifold(Euclidean(2; field=ℝ), ManoptExamples.RosenbrockMetric())","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and the gradient is now adopted to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_f!(M, X, p)\n    ∇f!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 1000000  F(x): 1.3571e-09 |δp|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |δp|: 3.6836e-05 | |grad f|: 9.240792e-09\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"For the convex case, we have to first introduce the two parts of the cost.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and their (Euclidan) gradients","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function ∇h!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ∇h(M, p; a=100, b=1)\n    X = zero(p)\n    ∇h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ∇g!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ∇g(M, p; a=100, b=1)\n    X = zero(p)\n    ∇g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"and we define for convenience","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_∇h!(M, X, p) = ∇h!(M, X, p; a=a, b=b)\ndocE_∇g!(M, X, p) = ∇g!(M, X, p; a=a, b=b)\nfunction docE_∇f!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_∇g!(M, X, p)\n  docE_∇h!(M, Y, p)\n  X .-= Y\n  return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we call the difference of convex algorithm on Eucldiean space mathbb R^2.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"E_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_∇h!, p0;\n    gradient=docE_∇f!,\n    grad_g = docE_∇g!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \n# 10000    F(x): 2.9705e-09 |δp|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |δp|: 1.2173e-04 | |grad f|: 4.541087e-08\nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","page":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"We first have to again defined the gradients with respect to the new metric","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"function grad_h!(M, X, p; a=100, b=1)\n    ∇h!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ∇g!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For X in partial h(p^(k)) the sunproblem top determine p^(k+1) reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatorname*argmin_pinmathcal M g(p) - langle X log_p^(k)prangle","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with X = operatornamegrad h(p^(k)) that","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"phi(p) = g(p) - langle X log_p^(k)prangle\n= abigl( p_1^2-p_2bigr)^2\n        + 2bigl(p_1-bbigr)^2 - 2(p^(k)_1-b)p_1 + 2(p^(k)_1-b)p^(k)_1","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"its Euclidean gradient reads","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"operatornamegradphi(p) =\n    nabla varphi(p)\n    = beginpmatrix\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^(k)_1-b)\n        -2a(p_1^2-p_2)\n    endpmatrix","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"where we can again employ the gradient conversion from before to obtain the Riemannian gradient.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"mutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (ϕ::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    ϕ(M, X, p)\n    return X\nend\nfunction (ϕ::SubGrad)(M, X, p)\n    X .= [\n        4 * ϕ.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - ϕ.b) - 2 * (ϕ.pk[1] - ϕ.b),\n        -2 * ϕ.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And in orer to update the subsolvers gradient correctly, we have to overwrite","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"set_manopt_parameter!(ϕ::SubGrad, ::Val{:p}, p) = (ϕ.pk .= p)\nset_manopt_parameter!(ϕ::SubGrad, ::Val{:X}, X) = (ϕ.Xk .= X)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we again introduce for ease of use","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"docR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^8) | StopWhenChangeLess(1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"Initial F(x): 7.2208e+03 \nThe algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch() with keyword parameters\n    |   * initial_stepsize    = 1.0\n    |   * retraction_method   = ExponentialRetraction()\n    |   * contraction_factor  = 0.95\n    |   * sufficient_decrease = 0.1\n    | \n    | ## Stopping Criterion\n    | Stop When _one_ of the following are fulfilled:\n    |     Max Iteration 2000:   reached\n    |     |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 100000000:    not reached\n    |Δp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Stop = :Stop\n    :All = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|δp|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","page":"Rosenbrock Metric","title":"Comparison in Iterations","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^8),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\")","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"(Image: )","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts.","category":"page"},{"location":"examples/Difference-of-Convex-Rosenbrock/#Literature","page":"Rosenbrock Metric","title":"Literature","text":"","category":"section"},{"location":"examples/Difference-of-Convex-Rosenbrock/","page":"Rosenbrock Metric","title":"Rosenbrock Metric","text":"R. Bergmann, O. P. Ferreira, E. M. Santos and J. C. Souza. The difference of convex algorithm on Hadamard manifolds. Preprint (2023), arXiv:2112.05250.\n\n\n\n","category":"page"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/Changelog.md\"","category":"page"},{"location":"changelog/#Changelog","page":"Changelog","title":"Changelog","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"All notable changes to this Julia package will be documented in this file.","category":"page"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"page"},{"location":"changelog/#[0.1.3]-–-11/12/2023","page":"Changelog","title":"[0.1.3] – 11/12/2023","text":"","category":"section"},{"location":"changelog/#Added","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Total variation Minimization cost, proxes, and an example\nBézier curve cost, gradients, and an example.","category":"page"},{"location":"changelog/#[0.1.3]-–-16/09/2023","page":"Changelog","title":"[0.1.3] – 16/09/2023","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Rayleigh Quotient functions added\nan example illustrating Euclidean gradient/HEssian conversion\nAdd Literature with DocumenterCitations","category":"page"},{"location":"changelog/#[0.1.2]-–-13/06/2023","page":"Changelog","title":"[0.1.2] – 13/06/2023","text":"","category":"section"},{"location":"changelog/#Added-3","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Update examples to use Quarto\nAdd DC examples","category":"page"},{"location":"changelog/#[0.1.1]-–-01/03/2023","page":"Changelog","title":"[0.1.1] – 01/03/2023","text":"","category":"section"},{"location":"changelog/#Added-4","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Rosenbrock function and examples","category":"page"},{"location":"changelog/#[0.1.0]-–-18/02/2023","page":"Changelog","title":"[0.1.0] – 18/02/2023","text":"","category":"section"},{"location":"changelog/#Added-5","page":"Changelog","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Changelog","title":"Changelog","text":"Setup the project to collect example objectives for Manopt.jl which are well-documented and well-tested\nSetup Documentation to provide one example Quarto file for every example objective to illustrate how to use them","category":"page"},{"location":"examples/Bezier-curves/#Minimizing-the-Acceleration-of-Bézier-Curves-on-the-Sphere","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"using Manifolds, Manopt, ManoptExamples","category":"page"},{"location":"examples/Bezier-curves/#Introduction","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Introduction","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Bézier Curves can be generalized to manifolds by generalizing the de Casteljau algorithm 📖 to work with geodesics instead of straight lines. An implementation in just a few lines as we demonstrated in [ABBR23] as","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"function bezier(M::AbstractManifold, t, pts::NTuple)\n    p = bezier(M, t, pts[1:(end - 1)])\n    q = bezier(M, t, pts[2:end])\n    return shortest_geodesic(M, p, q, t)\nend\nfunction bezier(M::AbstractManifold, t, pts::NTuple{2})\n    return shortest_geodesic(M, pts[1], pts[2], t)\nend","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"which is also available within this package as de_Casteljau using a simple BezierSegment struct to make it easier to also discuss the case where we compose a set of segments into a composite Bézier course.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"In the following we will need the following packages and functions. They are documented in the section on Bezier Curves and were derived in [BG18] based on [PN07].","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"using ManoptExamples:\n    artificial_S2_composite_Bezier_curve,\n    BezierSegment,\n    de_Casteljau,\n    get_Bezier_degrees,\n    get_Bezier_inner_points,\n    get_Bezier_junctions,\n    get_Bezier_junction_tangent_vectors,\n    get_Bezier_points,\n    get_Bezier_segments,\n    grad_L2_acceleration_Bezier,\n    L2_acceleration_Bezier","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"This notebook reproduces the example form Section 5.2 in [BG18].","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"The following image illustrates how the de-Casteljau algorithm works for one segment.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: A Bezier segment and illustration of the de-Casteljau algorithm)","category":"page"},{"location":"examples/Bezier-curves/#Approximating-data-by-a-curve-with-minimal-accelartion","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Approximating data by a curve with minimal accelartion","text":"","category":"section"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We first load our example data","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"M = Sphere(2)\nB = artificial_S2_composite_Bezier_curve()\ndata_points = get_Bezier_junctions(M, B)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Which is the following cure, which clearly starts and ends slower than its speed in the middle, which can be seen by the increasing length of the gangent vectors in the middle.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: The original curve)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We continue to recude the points, since we “know” sme points due to the C^1 property: the second to last control point of the first segment b_02, the joint junction point connecting both segments b_03=b_10 and the second control point b_11 of the second segment have to line in the tangent space of the joint junction point. Hence we only have to store one of the control points.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"We can use this reduced form as the variable to optimize and the one from the data as our initial point.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"pB = get_Bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB))","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"PowerManifold(Sphere(2, ℝ), NestedPowerRepresentation(), 8)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"And we further define the acceleration of the curve as our cost function, where we discretize the acceleration at a certain set of points and set the λ=10","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"curve_samples = [range(0, 3; length=101)...] # sample curve for the gradient\nλ = 10.0\nfunction f(M, pB)\n    return L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend\nfunction grad_f(M, pB)\n    return grad_L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, λ, data_points\n    )\nend","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"grad_f (generic function with 1 method)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Then we can optimize","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"x0 = pB\npB_opt = gradient_descent(\n    N,\n    f,\n    grad_f,\n    x0;\n    stepsize=ArmijoLinesearch(;\n        initial_stepsize=1.0,\n        retraction_method=ExponentialRetraction(),\n        contraction_factor=0.5,\n        sufficient_decrease=0.001,\n    ),\n    stopping_criterion=StopWhenChangeLess(1e-5) |\n                       StopWhenGradientNormLess(1e-7) |\n                       StopAfterIteration(300),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        25,\n        :Stop,\n    ],\n);","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"Initial  | f(x): 10.647244 |  |  | \n# 25     | f(x): 2.667564 | |grad f(p)|:0.890845571434862 | s:0.01326670131422904 | Last Change: 0.763281\n# 50     | f(x): 2.650064 | |grad f(p)|:0.05536989605165708 | s:0.05306680525691616 | Last Change: 0.081780\n# 75     | f(x): 2.649707 | |grad f(p)|:0.02135638585837997 | s:0.01326670131422904 | Last Change: 0.011590\n# 100    | f(x): 2.649700 | |grad f(p)|:0.0012887575647752057 | s:0.05306680525691616 | Last Change: 0.001745\nThe algorithm performed a step with a change (2.9063044690733034e-7) less than 1.0e-5.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"And we can again look at the result","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"The result looks as","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"(Image: The resulting curve)","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"where all control points are evenly spaced and we hence have less acceleration as the final cost compared to the initial one indicates. Note that the cost is not zero, since we always have a tradeoff between approximating the initial junctinons (data points) and minimizing the acceleration.","category":"page"},{"location":"examples/Bezier-curves/","page":"Minimizing the Acceleration of Bézier Curves on the Sphere","title":"Minimizing the Acceleration of Bézier Curves on the Sphere","text":"S. D. Axen, M. Baran, R. Bergmann and K. Rzecki. Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds. ACM Transactions on Mathematical Software (2023), arXiv:2021.08777.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nT. Popiel and L. Noakes. Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory 148, 111–127 (2007).\n\n\n\n","category":"page"},{"location":"data/#Data-sets","page":"Data","title":"Data sets","text":"","category":"section"},{"location":"data/#Signals-on-manifolds","page":"Data","title":"Signals on manifolds","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"Modules = [ManoptExamples]\nPages = [\"data/artificial_signals.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"data/#ManoptExamples.Lemniscate-Tuple{Number}","page":"Data","title":"ManoptExamples.Lemniscate","text":"Lemniscate(t::Float; kwargs...)\nLemniscate(n::integer; interval=[0.0, 2π], kwargs...)\n\ngenerate the Lemniscate of Bernoulli as a curve on a manifold, by generating the curve emplying the keyword arguments below.\n\nTo be precise on the manifold M we use the tangent space at p and generate the curve\n\nγ(t) fracasin^2(t) + 1 beginpmatrix cos(t)  cos(t)sin(t) endpmatrix\n\nin the plane spanned by X and Y in the tangent space. Note that this curve is 2π-periodic and a is the half-width of the curve.\n\nTo reproduce the first examples from [BBSW16] as a default, on the sphere p defaults to the North pole.\n\nTHe second variant generates n points equispaced in ìnterval` and calls the first variant.\n\nKeywords\n\nmanifold - (Sphere(2)) the manifold to build the lemniscate on\np        - ([0.0, 0.0, 1.0] on the sphere, `rand(M) else) the center point of the Lemniscate\na        – (π/2.0) half-width of the Lemniscate\nX        – ([1.0, 0.0, 0.0] for the 2-sphere with default p, the first DefaultOrthonormalBasis() vector otherwise) first direction for the plane to define the Lemniscate in, unit vector recommended.\nY        – ([0.0, 1.0, 0.0] if p is the default, the second DefaultOrthonormalBasis() vector otherwise) second direction for the plane to define the Lemniscate in, unit vector orthogonal to X recommended.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_signal","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to -pipi). This is data for an example from  Bergmann et. al., SIAM J Imag Sci, 2014.\n\nOptional\n\npts – (500) number of points to sample the function\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S1_signal-Tuple{Real}","page":"Data","title":"ManoptExamples.artificial_S1_signal","text":"artificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of  Bergmann et. al., SIAM J Imag Sci, 2014 for values outside that interval, this Signal is missing.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S1_slope_signal","page":"Data","title":"ManoptExamples.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the Circle with increasing slope.\n\nOptional\n\npts – (500) number of points to sample the function.\nslope – (4.0) initial slope that gets increased afterwards\n\nThis data set was introduced for the numerical examples in Bergmann et. al., SIAM J Imag Sci, 2014\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_composite_Bezier_curve-Tuple{}","page":"Data","title":"ManoptExamples.artificial_S2_composite_Bezier_curve","text":"artificial_S2_composite_Bezier_curve()\n\nGenerate a composite Bézier curve on the Sphere mathbb S^2 that was used in Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018.\n\nIt consists of 4 egments connecting the points\n\nmathbf d_0 = beginpmatrix 001endpmatrixquad\nmathbf d_1 = beginpmatrix 0-10endpmatrixquad\nmathbf d_2 = beginpmatrix -100endpmatrixtext and \nmathbf d_3 = beginpmatrix 00-1endpmatrix\n\nwhere instead of providing the two center control points explicitly we provide them as velocities from the corresponding points, such thtat we can directly define the curve to be C^1.\n\nWe define\n\nX_0 = fracπ8sqrt2beginpmatrix1-10endpmatrixquad\nX_1 = fracπ4sqrt2beginpmatrix101endpmatrixquad\nX_2 = fracπ4sqrt2beginpmatrix01-1endpmatrixtext and \nX_3 = fracπ8sqrt2beginpmatrix-110endpmatrix\n\nwhere we defined each X_i in T_d_imathbb S^2. We defined three BezierSegments\n\nof cubic Bézier curves as follows\n\nbeginalign*\nb_00 = d_0 quad  b_10 = exp_d_0X_0 quad  b_20 = exp_d_1X_1 quad  b_30 = d_1\nb_01 = d_1 quad  b_11 = exp_d_1(-X_1) quad  b_21 = exp_d_2X_2 quad  b_31 = d_2\nb_02 = d_2 quad  b_11 = exp_d_2(-X_2) quad  b_22 = exp_d_3X_3 quad  b_32 = d_3\nendalign*\n\n\n\n\n\n","category":"method"},{"location":"data/#images-on-manifolds","page":"Data","title":"images on manifolds","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"Modules = [ManoptExamples]\nPages = [\"data/artificial_images.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"data/#ManoptExamples.artificialIn_SAR_image-Tuple{Integer}","page":"Data","title":"ManoptExamples.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\nThis data set was introduced for the numerical examples in Bergmann et. al., SIAM J Imag Sci, 2014.\n\n\n\n\n\n","category":"method"},{"location":"data/#ManoptExamples.artificial_S2_rotation_image","page":"Data","title":"ManoptExamples.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\nCreate an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts – (64) number of pixels along one dimension\nrotations – ((.5,.5)) number of total rotations performed on the axes.\n\nThis dataset was used in the numerical example of Section 5.1 of Bačák et al., SIAM J Sci Comput, 2016.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_image","page":"Data","title":"ManoptExamples.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts::Int=64])\n\nGenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts – (64) size of the image in pts×pts pixel.\n\nThis example dataset was used in the numerical example in Section 5.5 of Laus et al., SIAM J Imag Sci., 2017\n\nIt is based on artificial_S2_rotation_image extended by small whirl patches.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_S2_whirl_patch","page":"Data","title":"ManoptExamples.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the pts×pts patch of Sphere(@ref)(2)-valued image data.\n\nThese patches are used within artificial_S2_whirl_image.\n\nOptional Parameters\n\npts – (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image","page":"Data","title":"ManoptExamples.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with a jump of size stepsize.\n\nThis dataset was used in the numerical example of Section 5.2 of Bačák et al., SIAM J Sci Comput, 2016.\n\n\n\n\n\n","category":"function"},{"location":"data/#ManoptExamples.artificial_SPD_image2","page":"Data","title":"ManoptExamples.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size pts×pts pixel with right hand side fraction is moved upwards.\n\nThis data set was introduced in the numerical examples of Section of Bergmann, Presch, Steidl, SIAM J Imag Sci, 2016\n\n\n\n\n\n","category":"function"},{"location":"data/#Literature","page":"Data","title":"Literature","text":"","category":"section"},{"location":"data/","page":"Data","title":"Data","text":"M. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann and P.-Y. Gousenbourger. A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics 4 (2018), arXiv:1807.10090.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nR. Bergmann, J. Persch and G. Steidl. A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds. SIAM Journal on Imaging Sciences 9, 901–937 (2016), arXiv:1512.02814.\n\n\n\nF. Laus, M. Nikolova, J. Persch and G. Steidl. A nonlocal denoising algorithm for manifold-valued images using second order statistics. SIAM Journal on Imaging Sciences 10, 416–448 (2017).\n\n\n\n","category":"page"},{"location":"examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","page":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Ronny BergmannLaura Weigl 2023-07-02","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For this example we first load the necessary packages.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)","category":"page"},{"location":"examples/Robust-PCA/#Computing-a-Robust-PCA","page":"Robust PCA","title":"Computing a Robust PCA","text":"","category":"section"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For a given matrix D  ℝ^dn whose columns represent points in ℝ^d, a matrix p  ℝ^dm is computed for a given dimension m  n: p represents an ONB of ℝ^dm such that the column space of p approximates the points (columns of D), i.e. the vectors D_i as well as possible.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We compute p as a minimizer over the Grassmann manifold of the cost function:","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"beginsplit\nf(p)\n = frac1nsum_i=1^noperatornamedist(D_i operatornamespan(p))\n\n = frac1n sum_i=1^nlVert pp^TD_i - D_irVert\nendsplit","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The output cost represents the average distance achieved with the returned p, an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that f is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter ε.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f_ϵ(p) = frac1n sum_i=1^nℓ_ϵ(lVert pp^mathrmTD_i - D_irVert)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"where ℓ_ϵ(x) = sqrtx^2 + ϵ^2 - ϵ.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts).","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"First, we generate random data. For illustration purposes we take points in mathbb R^2 and m=1, that is we aim to find a robust regression line.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"n = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"M = Grassmann(d,m);","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"For the initial matrix p_0 we use classical PCA via singular value decomposition. Thus, we use the first d left singular vectors.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Then, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in Manopt.jl.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Furthermore the cost and gradient are implemented in ManoptExamples.jl. Since these are Huber regularized, both functors have the ϵ as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection pp^T - I, which converts the Euclidean to the Riemannian gradient.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"The trust-region method also requires the Hessian Matrix. By using ApproxHessianFiniteDifference using a finite difference scheme we get an approximation of the Hessian Matrix.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"We run the procedure several times, where the smoothing parameter ε is reduced iteratively.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ε = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m]","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"2×1 Matrix{Float64}:\n -0.7494248652139397\n  0.6620893983436593","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Let’s generate the cost and gradient we aim to use here","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f = ManoptExamples.RobustPCACost(M, data, ε)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, ε)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"ManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([9.537606557855465 1.6583418797018163 … 30.833523701909474 30.512999245062304; -45.34339972619071 -1.7120433539256108 … -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"and check the initial cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f(M, p0)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.430690947905521","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Now we iterate the opimization with reducing ε after every iteration, which we update in f and grad_f.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"q = copy(M, p0)\nεi = ε\nfor i in 1:iterations\n    f.ε = εi\n    grad_f.ε = εi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global εi *= reduction\nend","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"When finally setting ε we can investigate the final cost","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"f.ε = 0.0\nf(M, q)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"9.412961981726742","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"Finally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines.","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"fig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)","category":"page"},{"location":"examples/Robust-PCA/","page":"Robust PCA","title":"Robust PCA","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Total-Variation/#Total-Variation-Minimization","page":"Total Variation","title":"Total Variation Minimization","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Ronny Bergmann 2023-06-06","category":"page"},{"location":"examples/Total-Variation/#Introduction","page":"Total Variation","title":"Introduction","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Total Variation denoising is an optimization problem used to denoise signals and images. The corresponding (Euclidean) objective is often called Rudin-Osher-Fatemi (ROF) model based on the paper [ROF92].","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"This was generalized to manifolds in [WDS14]. In this short example we will look at the ROF model for manifold-valued data, its generalizations, and how they can be solved using Manopt.jl.","category":"page"},{"location":"examples/Total-Variation/#The-manifold-valued-ROF-model","page":"Total Variation","title":"The manifold-valued ROF model","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Generalizing the ROF model to manifolds can be phrased as follows: Given a (discrete) signal on a manifold s = (s_i)_i=1^N in mathbb M^n of length n in mathbb N, we usually assume that this signal might be noisy. For the (Euclidean) ROF model we assume that the noise is Gaussian. Then variational models for denoising usually consist of a data term D(ps) to “stay close to” s and a regularizer R(p). For TV regularization the data term is the squared distance and the regularizer models that without noise, neighboring values are close. We obtain","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"operatorname*argmin_pinmathcal M^n\nf(p)\nqquad\nf(p) = D(ps) + α R(p) = sum_i=1^n d_mathcal M^2(s_ip_i) + αsum_i=1^n-1 d_mathcal M(p_ip_i+1)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"where α  0 is a weight parameter.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"The challenge here is that most classical algorithm, like gradient descent or Quasi Newton, assume the cost f(p) to be smooth such that the gradient exists at every point. In our setting that is not the case since the distacen is not differentiable for any p_i=p_i+1. So we have to use another technique.","category":"page"},{"location":"examples/Total-Variation/#THe-Cyclic-Proximal-Point-algorithm","page":"Total Variation","title":"THe Cyclic Proximal Point algorithm","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"If the cost consists of a sum of functions, where each of the proximal maps is “easy to evaluate”, for best of cases in closed form, we can “apply the proximal maps in a cyclic fashion” and optain the the Cyclic Proximal Point Algorithm [Bac14].","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Both for the distance and the squared distance, we have generic implementations; since this happens in a cyclic manner, there is also always one of the arguments involved in the prox and never both. We can improve the performance slightly by computing all proes in parallel that do not interfer. To be precise we can compute first all proxes of distances in the regularizer that start with an odd index in parallel. Afterwards all that start with an even index.","category":"page"},{"location":"examples/Total-Variation/#The-Optimsation","page":"Total Variation","title":"The Optimsation","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"using Manifolds, Manopt, ManoptExamples, ManifoldDiff\nusing ManifoldDiff: prox_distance\nn = 500 #Signal length\nσ = 0.2 # amount of noise\nα = 0.5# in the TV model","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We define a few colors","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"using Colors, NamedColors, ColorSchemes, Plots, Random\ndata_color = RGBA{Float64}(colorant\"black\")\nlight_color = RGBA{Float64}(colorant\"brightgrey\")\nrecon_color = RGBA{Float64}(colorant\"vibrantorange\")\nnoisy_color = RGBA{Float64}(colorant\"vibrantteal\")","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"And we generate our data on the Circle, since that is easy to plot and nice to compare to the Euclidean case of a real-valued signal.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Random.seed!(23)\nM = Circle()\nN = PowerManifold(M, n)\ndata = ManoptExamples.artificial_S1_signal(n)\ns = [exp(M, d, rand(M; vector_at=d, σ=0.2)) for d in data]\nt = range(0.0, 1.0; length=n)\nscene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=noisy_color,\n    markerstrokecolor=noisy_color,\n    lab=\"noisy\",\n)\nyticks!(\n    [-π, -π / 2, 0, π / 2, π],\n    [raw\"$-\\pi$\", raw\"$-\\frac{\\pi}{2}$\", raw\"$0$\", raw\"$\\frac{\\pi}{2}$\", raw\"$\\pi$\"],\n)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"(Image: )","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"As mentioned above, total variation now minimized different neighbors – while keeping jumps if the are large enough. One notable difference between Euclidean and Cyclic data is, that the y-axis is in our case periodic, hence the first jump is actually not a jump but a “linear increase” that “wraps around” and the second large jump –or third overall– is actually only as small as the second jump.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Defining cost and the proximal maps, which are actually 3 proxes to be precise.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"f(N, p) = ManoptExamples.L2_Total_Variation(N, s, α, p)\nproxes_f = ((N, λ, p) -> prox_distance(N, λ, s, p, 2), (N, λ, p) -> prox_TV(N, α * λ, p))","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We run the algorithm","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"o = cyclic_proximal_point(\n    N,\n    f,\n    proxes_f,\n    s;\n    λ=i -> π / (2 * i),\n    debug=[\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        :Cost,\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    record=[:Iteration, :Cost, :Change, :Iterate],\n    return_state=true,\n);","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Initial  |  | f(x): 59.187445 | \n# 1000   | λ:0.0015707963267948967 | f(x): 13.963912 | Last Change: 1.773283\n# 2000   | λ:0.0007853981633974483 | f(x): 13.947124 | Last Change: 0.011678\n# 3000   | λ:0.0005235987755982988 | f(x): 13.941538 | Last Change: 0.003907\n# 4000   | λ:0.00039269908169872416 | f(x): 13.938748 | Last Change: 0.001957\n# 5000   | λ:0.0003141592653589793 | f(x): 13.937075 | Last Change: 0.001175\nThe algorithm reached its maximal number of iterations (5000).","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We can see that the cost reduces nicely. Let’s extract the result an the recorded values","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"recon = get_solver_result(o)\nrecord = get_record(o)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We get","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"scene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=light_color,\n    markerstrokecolor=light_color,\n    lab=\"noisy\",\n)\nscatter!(\n    scene,\n    t,\n    recon;\n    markersize=2,\n    markercolor=recon_color,\n    markerstrokecolor=recon_color,\n    lab=\"reconstruction\",\n)","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"(Image: )","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Which contains the usual stair casing one expects for TV regularization, but here in a “cyclic manner”","category":"page"},{"location":"examples/Total-Variation/#Outlook","page":"Total Variation","title":"Outlook","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"We can generalize the total variation also to a second order total variation. Again intuitively, while TV prefers constant areas, the operatornameTV_2 yields a cost 0 for anything linear, which on manifolds can be generalized to equidistant on a geodesic [BBSW16]. Here we can again derive proximal maps, which for the circle again have a closed form solutoin [BLSW14] but on general manifolds these have again to be approximated.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Another extension for both first and second order TV is to apply this for manifold-valued images S = (S_ij)_ij=1^mn in mathcal M^mn, where the distances in the regularizer are then used in both the first dimension i and the second dimension j in the data.","category":"page"},{"location":"examples/Total-Variation/#Technical-Details","page":"Total Variation","title":"Technical Details","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"This version of the example was generated with the following package versions.","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Pkg.status()","category":"page"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"Status `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.4.0\n  [35d6a980] ColorSchemes v3.24.0\n  [5ae59095] Colors v0.12.10\n  [7073ff75] IJulia v1.24.2\n  [8ac3fa9e] LRUCache v1.6.0\n  [af67fdf4] ManifoldDiff v0.3.9\n  [1cead3c2] Manifolds v0.9.8\n  [3362f125] ManifoldsBase v0.15.4\n  [0fc0a36d] Manopt v0.4.43\n  [5b8d5e80] ManoptExamples v0.1.4 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.39.0","category":"page"},{"location":"examples/Total-Variation/#Literature","page":"Total Variation","title":"Literature","text":"","category":"section"},{"location":"examples/Total-Variation/","page":"Total Variation","title":"Total Variation","text":"M. Bačák. Computing medians and means in Hadamard spaces. SIAM Journal on Optimization 24, 1542–1566 (2014), arXiv:1210.2145.\n\n\n\nM. Bačák, R. Bergmann, G. Steidl and A. Weinmann. A second order non-smooth variational model for restoring manifold-valued images. SIAM Journal on Scientific Computing 38, A567–A597 (2016), arXiv:1506.02409.\n\n\n\nR. Bergmann, F. Laus, G. Steidl and A. Weinmann. Second order differences of cyclic data and applications in variational denoising. SIAM Journal on Imaging Sciences 7, 2916–2953 (2014), arXiv:1405.5349.\n\n\n\nL. I. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena 60, 259–268 (1992).\n\n\n\nA. Weinmann, L. Demaret and M. Storath. Total variation regularization for manifold-valued data. SIAM Journal on Imaging Sciences 7, 2226–2257 (2014).\n\n\n\n","category":"page"},{"location":"examples/Rosenbrock/#The-Rosenbrock-Function","page":"Rosenbrock","title":"The Rosenbrock Function","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Ronny Bergmann 2023-01-03","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"After loading the necessary packages","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Manifolds, Manopt, ManoptExamples\nusing Plots","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We fix the parameters for the 📖 Rosenbrock (where the wikipedia page has a slightly different parameter naming).","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"a = 100.0\nb = 1.0\np0 = [1/10, 2/10]","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which is defined on mathbb R^2, so we need","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"M = ℝ^2","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Euclidean(2; field=ℝ)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and can then generate both the cost and the gradient","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"ManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"For comparison, we look at the initial cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, p0)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"4.42","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And to illustrate, we run two small solvers with their default settings as a comparison.","category":"page"},{"location":"examples/Rosenbrock/#Gradient-Descent","page":"Rosenbrock","title":"Gradient Descent","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We start with the gradient descent solver.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Since we need the state anyways to access the record, we also get from the return_state=true a short summary of the solver run.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"From the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"gd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.10562873187751265","category":"page"},{"location":"examples/Rosenbrock/#Quasi-Newton","page":"Rosenbrock","title":"Quasi Newton","text":"","category":"section"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"We can improve this using the quasi Newton algorithm","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"# Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 44 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 2), projections, and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector trnasport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(DefaultManifold(), 0.0001, 0.999) with keyword arguments\n  * retraction_method = ExponentialRetraction()\n  * vector_transport_method = ParallelTransport()\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 1000: not reached\n    |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"And we see it stops far earlier, after 45 Iterations. We again collect the recorded values","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"qn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"2.359559352025148e-14","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"and see that the final value is close to the one of the minimizer","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"f(M, ManoptExamples.minimizer(f))","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"0.0","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"which we also see if we plot the recorded cost.","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"fig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")","category":"page"},{"location":"examples/Rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"(Image: Figure 1: The result of the robust PCA vs. SVD)","category":"page"},{"location":"examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","page":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Ronny Bergmann 2023-07-02","category":"page"},{"location":"examples/Riemannian-mean/#Preliminary-Notes","page":"Riemannian Mean","title":"Preliminary Notes","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Each of the example objectives or problems stated in this package should be accompanied by a Quarto notebook that illustrates their usage, like this one.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For this first example, the objective is a very common one, for example also used in the Get Started: Optimize! tutorial of Manopt.jl.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"The second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"There are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in examples/. If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having ManoptExamples.jl in development mode. this requires that your example does not have any (additional) dependencies beyond the ones ManoptExamples.jl has anyways.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"For registered versions of ManoptExamples.jl use the environment of examples/ and – under development – add ManoptExamples.jl in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the examples/ environment again.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/Riemannian-mean/#Loading-packages-and-defining-data","page":"Riemannian Mean","title":"Loading packages and defining data","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Loading the necessary packages and defining a data set on a manifold","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"using ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nσ = π / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];","category":"page"},{"location":"examples/Riemannian-mean/#Variant-1:-Using-the-functions","page":"Riemannian Mean","title":"Variant 1: Using the functions","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"We can define both the cost and gradient, RiemannianMeanCost and RiemannianMeanGradient!!, respectively. For their mathematical derivation and further explanations, we again refer to Get Started: Optimize!.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"f = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Then we can for example directly call a gradient descent as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"x1 = gradient_descent(M, f, grad_f, first(data))","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285","category":"page"},{"location":"examples/Riemannian-mean/#Variant-2:-Using-the-objective","page":"Riemannian Mean","title":"Variant 2: Using the objective","text":"","category":"section"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"A shorter way to directly obtain the Manifold objective including these two functions. Here, we want to specify that the objective can do inplace-evaluations using the evaluation=-keyword. The objective can be obtained calling Riemannian_mean_objective as","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"Together with a manifold, this forms a Manopt Problem, which would usually enable to switch manifolds between solver runs. Here we could for example switch to using Euclidean(3) instead for the same data the objective is build upon.","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"rmp = DefaultManoptProblem(M, rmo)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"This enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s1 = GradientDescentState(M, copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"but we can easily use a conjugate gradient instead","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"s2 = ConjugateGradientDescentState(\n    M,\n    copy(M, first(data)),\n    StopAfterIteration(100),\n    ArmijoLinesearch(M),\n    FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2)","category":"page"},{"location":"examples/Riemannian-mean/","page":"Riemannian Mean","title":"Riemannian Mean","text":"3-element Vector{Float64}:\n 0.6868393613136017\n 0.006531541407458413\n 0.7267799052788726","category":"page"},{"location":"examples/RayleighQuotient/#The-Rayleigh-Quotient","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Ronny Bergmann 2024-03-09","category":"page"},{"location":"examples/RayleighQuotient/#Introduction","page":"The Rayleigh Quotient","title":"Introduction","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [Bou23] using the Rayleigh quotient and explores several different ways to use the algorithms from Manopt.jl.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For a symmetric matrix A in mathbb R^ntimes n we consider the 📖 Rayleigh Quotient","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"operatorname*argmin_x in mathbb R^n backslash 0\nfracx^mathrmTAxlVert x rVert^2","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"On the sphere we can omit the denominator and obtain","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f(p) = p^mathrmTApqquad p  𝕊^n-1","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"which by itself we can again continue in the embedding as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"tilde f(x) = x^mathrmTAxqquad x in mathbb R^n","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"This cost has the nice feature that at the minimizer p^*inmathbb S^n-1 the function falue f(p^*) is the smalles eigenvalue of A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"For the embedded function tilde f the gradient and Hessian can be computed with classical methods as","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\ntilde f(x) = 2Ax qquad x  ℝ^n\n\n^2tilde f(x)V = 2AV qquad x V  ℝ^n\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Similarly, cf. Examples 3.62 and 5.27 of [Bou23], the Riemannian gradient and Hessian on the manifold mathcal M = mathbb S^n-1 are given by","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"beginalign*\noperatornamegrad f(p) = 2Ap - 2(p^mathrmTAp)*pqquad p  𝕊^n-1\n\noperatornameHess f(p)X =  2AX - 2(p^mathrmTAX)p - 2(p^mathrmTAp)Xqquad p  𝕊^n-1 X in T_p𝕊^n-1\nendalign*","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first generate an example martrx A.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"using LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random\nRandom.seed!(42)\nn = 500\nA = Symmetric(randn(n,n))","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And the manifolds","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"M = Sphere(n-1)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Sphere(499, ℝ)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"E = get_embedding(M)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean(500; field=ℝ)","category":"page"},{"location":"examples/RayleighQuotient/#Setup-the-corresponding-functions","page":"The Rayleigh Quotient","title":"Setup the corresponding functions","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Since RayleighQuotientCost, RayleighQuotientGrad!!, and RayleighQuotientHess!! are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute f is called with M as their first argument and tilde f if called with E.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We instantiate","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"f = ManoptExamples.RayleighQuotientCost(A)\ngrad_f = ManoptExamples.RayleighQuotientGrad!!(A)\nHess_f = ManoptExamples.RayleighQuotientHess!!(A)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"the suffix !! also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"p0 = [1.0, zeros(n-1)...]\nX = zero_vector(M, p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"we can both call","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Y = grad_f(M,p0)  # Allocates memory\ngrad_f(M,X,p0)    # Computes in place of X and returns the result in X.\nnorm(M, p0, X-Y)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"0.0","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Now we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"First of all let’s construct the actual result – since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"λ = min(eigvals(A)...)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"-44.8386050469405","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-on-gradient-information","page":"The Rayleigh Quotient","title":"A Solver based on gradient information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient nabla tilde f. We can “tell” the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally, Manopt.jl then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf. e.g. this tutorial section or Section 3.8 or more precisely Example 3.62 in [Bou23].","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"But instead of diving into all the tecnical details, we can just specify objective_type=:Euclidean to trigger the conversion. We start with a simple gradient descent","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"s = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n    return_state=true,\n)\nq1 = get_solver_result(s)\ns","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 50    f(x): -44.206244|grad f(p)|:2.3878466243532688\n# 100   f(x): -44.546883|grad f(p)|:2.2561253654599445\n# 150   f(x): -44.765220|grad f(p)|:1.3051578932969594\n# 200   f(x): -44.824730|grad f(p)|:0.575815360373987\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch() with keyword parameters\n  * initial_stepsize    = 1.0\n  * retraction_method   = ExponentialRetraction()\n  * contraction_factor  = 0.95\n  * sufficient_decrease = 0.1\n\n## Stopping Criterion\nStop When _one_ of the following are fulfilled:\n    Max Iteration 200:  reached\n    |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 50]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"From the final cost we can already see that q1 is an eigenvector to the smallest eigenvalue we obtaines above.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"And we can compare this to running with the Riemannian gradient, since the RayleighQuotientGrad!! returns this one as well, when just called with the sphere as first Argument, we just have to remove the objective_type.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q2 = gradient_descent(M, f, grad_f, p0;\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n)\n#Test that both are the same\nisapprox(M, q1,q2)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 50    f(x): -44.206244|grad f(p)|:2.3878466243532728\n# 100   f(x): -44.546883|grad f(p)|:2.2561253654599707\n# 150   f(x): -44.765220|grad f(p)|:1.305157893296953\n# 200   f(x): -44.824730|grad f(p)|:0.5758153603739836\n\ntrue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We can also benchmark both","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 19 samples with 1 evaluation.\n Range (min … max):  265.570 ms … 284.588 ms  ┊ GC (min … max): 7.90% … 7.34%\n Time  (median):     268.441 ms               ┊ GC (median):    7.96%\n Time  (mean ± σ):   270.099 ms ±   4.764 ms  ┊ GC (mean ± σ):  7.93% ± 0.26%\n\n      ▃ █▃    █                                                  \n  ▇▁▁▁█▇██▁▇▇▁█▁▁▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  266 ms           Histogram: frequency by time          285 ms <\n\n Memory estimate: 1.13 GiB, allocs estimate: 3853.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark gradient_descent($M, $f, $grad_f, $p0)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 162 samples with 1 evaluation.\n Range (min … max):  30.175 ms …  37.465 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     30.600 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   31.001 ms ± 901.457 μs  ┊ GC (mean ± σ):  1.10% ± 1.75%\n\n    ▅▇█▃▅▆█▃                       ▁ ▆                          \n  ▅█████████▆▄▄▁▁▁▃▁▃▁▁▁▁▁▃▁▁▁▁▃▃▆▄███▇▅▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃ ▃\n  30.2 ms         Histogram: frequency by time         33.3 ms <\n\n Memory estimate: 12.02 MiB, allocs estimate: 3246.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We see, that the conversion costs a bit of performance, but if the Euclidean gradient is easier to compute, this might still be ok.","category":"page"},{"location":"examples/RayleighQuotient/#A-Solver-based-(also)-on-(approximate)-Hessian-information","page":"The Rayleigh Quotient","title":"A Solver based (also) on (approximate) Hessian information","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To also involve the Hessian, we consider the trust regions solver with three cases:","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Euclidean, approximating the Hessian\nEuclidean, providing the Hessian\nRiemannian, providing the Hessian but also using in-place evaluations.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -44.054570|grad f(p)|:9.520440806961885\n# 20    f(x): -44.512159|grad f(p)|:3.8960976303377146\n# 30    f(x): -44.750745|grad f(p)|:4.0172063476264075\n# 40    f(x): -44.921406|grad f(p)|:4.0629541064444235\n# 50    f(x): -44.944986|grad f(p)|:1.5625926334092841\n# 60    f(x): -44.962671|grad f(p)|:1.4844829786311928\n# 70    f(x): -44.981071|grad f(p)|:1.406186078892307\n# 80    f(x): -44.998912|grad f(p)|:1.333230383118243\n# 90    f(x): -45.016067|grad f(p)|:1.2712292976953228\n# 100   f(x): -45.033852|grad f(p)|:1.2245396895601803\n# 110   f(x): -45.054112|grad f(p)|:1.1949140426096447\n# 120   f(x): -45.077761|grad f(p)|:1.181336063241073\n# 130   f(x): -45.104051|grad f(p)|:1.1807084432424522\n# 140   f(x): -45.131158|grad f(p)|:1.1891034869646029\n# 150   f(x): -45.157245|grad f(p)|:1.2028717827098632\n# 160   f(x): -45.181092|grad f(p)|:1.2191967547483598\n# 170   f(x): -45.202141|grad f(p)|:1.2361671256155067\n# 180   f(x): -45.220310|grad f(p)|:1.252614027690994\n# 190   f(x): -45.235779|grad f(p)|:1.2678969023508213\n# 200   f(x): -45.248841|grad f(p)|:1.2817199840828324\n# 210   f(x): -45.256727|grad f(p)|:1.2916659158016648\n# 220   f(x): -45.256727|grad f(p)|:1.2916659158016648\n# 230   f(x): -45.256727|grad f(p)|:1.2916659157663253\n# 240   f(x): -45.256727|grad f(p)|:1.2916659156249692\n# 250   f(x): -45.256727|grad f(p)|:1.291665915483601\n# 260   f(x): -45.256727|grad f(p)|:1.2916659153422445\n# 270   f(x): -45.256727|grad f(p)|:1.291665915200879\n# 280   f(x): -45.256727|grad f(p)|:1.2916659150595229\n# 290   f(x): -45.256727|grad f(p)|:1.2916659149181455\n# 300   f(x): -45.256727|grad f(p)|:1.291665914776787\n# 310   f(x): -45.256727|grad f(p)|:1.2916659146354228\n# 320   f(x): -45.256727|grad f(p)|:1.2916659144940739\n# 330   f(x): -45.256727|grad f(p)|:1.2916659143526876\n# 340   f(x): -45.256727|grad f(p)|:1.2916659142113394\n# 350   f(x): -45.256727|grad f(p)|:1.2916659140699758\n# 360   f(x): -45.256727|grad f(p)|:1.2916659139286075\n# 370   f(x): -45.256727|grad f(p)|:1.2916659137872586\n# 380   f(x): -45.256727|grad f(p)|:1.2916659136458828\n# 390   f(x): -45.256727|grad f(p)|:1.2916659135045347\n# 400   f(x): -45.256727|grad f(p)|:1.2916659133631683\n# 410   f(x): -45.256727|grad f(p)|:1.2916659132218014\n# 420   f(x): -45.256727|grad f(p)|:1.291665913080452\n# 430   f(x): -45.256727|grad f(p)|:1.2916659129390704\n# 440   f(x): -45.256727|grad f(p)|:1.2916659127977244\n# 450   f(x): -45.256727|grad f(p)|:1.2916659126563357\n# 460   f(x): -45.256727|grad f(p)|:1.2916659125149934\n# 470   f(x): -45.256727|grad f(p)|:1.291665912373618\n# 480   f(x): -45.256727|grad f(p)|:1.2916659122322556\n# 490   f(x): -45.256727|grad f(p)|:1.2916659120908875\n# 500   f(x): -45.256727|grad f(p)|:1.2916659119495382\n# 510   f(x): -45.256727|grad f(p)|:1.2916659118081695\n# 520   f(x): -45.256727|grad f(p)|:1.2916659116667941\n# 530   f(x): -45.256727|grad f(p)|:1.2916659115254383\n# 540   f(x): -45.256727|grad f(p)|:1.291665911384073\n# 550   f(x): -45.256727|grad f(p)|:1.291665911242725\n# 560   f(x): -45.256727|grad f(p)|:1.2916659111013584\n# 570   f(x): -45.256727|grad f(p)|:1.2916659109599946\n# 580   f(x): -45.256727|grad f(p)|:1.2916659108186235\n# 590   f(x): -45.256727|grad f(p)|:1.2916659106772588\n# 600   f(x): -45.256727|grad f(p)|:1.291665910535912\n# 610   f(x): -45.256727|grad f(p)|:1.2916659103945327\n# 620   f(x): -45.256727|grad f(p)|:1.291665910253165\n# 630   f(x): -45.256727|grad f(p)|:1.2916659101118186\n# 640   f(x): -45.256727|grad f(p)|:1.2916659099704497\n# 650   f(x): -45.256727|grad f(p)|:1.291665909829113\n# 660   f(x): -45.256727|grad f(p)|:1.29166590968772\n# 670   f(x): -45.256727|grad f(p)|:1.2916659095463716\n# 680   f(x): -45.256727|grad f(p)|:1.291665909404996\n# 690   f(x): -45.256727|grad f(p)|:1.2916659092636467\n# 700   f(x): -45.256727|grad f(p)|:1.2916659091222746\n# 710   f(x): -45.256727|grad f(p)|:1.2916659089809122\n# 720   f(x): -45.256727|grad f(p)|:1.2916659088395555\n# 730   f(x): -45.256727|grad f(p)|:1.2916659086982\n# 740   f(x): -45.256727|grad f(p)|:1.2916659085568292\n# 750   f(x): -45.256727|grad f(p)|:1.2916659084154538\n# 760   f(x): -45.256727|grad f(p)|:1.2916659082740989\n# 770   f(x): -45.256727|grad f(p)|:1.2916659081327297\n# 780   f(x): -45.256727|grad f(p)|:1.2916659079913815\n# 790   f(x): -45.256727|grad f(p)|:1.291665907850015\n# 800   f(x): -45.256727|grad f(p)|:1.291665907708641\n# 810   f(x): -45.256727|grad f(p)|:1.2916659075672852\n# 820   f(x): -45.256727|grad f(p)|:1.2916659074259267\n# 830   f(x): -45.256727|grad f(p)|:1.2916659072845595\n# 840   f(x): -45.256727|grad f(p)|:1.2916659071431842\n# 850   f(x): -45.256727|grad f(p)|:1.2916659070018381\n# 860   f(x): -45.256727|grad f(p)|:1.2916659068604721\n# 870   f(x): -45.256727|grad f(p)|:1.2916659067191119\n# 880   f(x): -45.256727|grad f(p)|:1.2916659065777438\n# 890   f(x): -45.256727|grad f(p)|:1.29166590643638\n# 900   f(x): -45.256727|grad f(p)|:1.2916659062950224\n# 910   f(x): -45.256727|grad f(p)|:1.291665906153663\n# 920   f(x): -45.256727|grad f(p)|:1.2916659060122988\n# 930   f(x): -45.256727|grad f(p)|:1.2916659058709288\n# 940   f(x): -45.256727|grad f(p)|:1.2916659057295612\n# 950   f(x): -45.256727|grad f(p)|:1.2916659055882171\n# 960   f(x): -45.256727|grad f(p)|:1.2916659054468413\n# 970   f(x): -45.256727|grad f(p)|:1.2916659053054909\n# 980   f(x): -45.256727|grad f(p)|:1.291665905164106\n# 990   f(x): -45.256727|grad f(p)|:1.2916659050227592\n# 1000  f(x): -45.256727|grad f(p)|:1.2916659048813917","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"To provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any struct is considered to (eventually) be the also optional starting point. For space reasons, let’s also shorten the debug print to only iterations 7 and 14.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q4 = trust_regions(M, f, grad_f, (E, p, X) -> Hess_f(E, p, X), p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -44.836478|grad f(p)|:1.681699351968465","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"q5 = trust_regions(M, f, grad_f, (M, Y, p, X) -> Hess_f(M, Y, p, X), p0;\n    evaluation=InplaceEvaluation(),\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n);","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Initial f(x): -0.363357\n# 10    f(x): -44.836478|grad f(p)|:1.681699351968463","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Let’s also here compare them in benchmarks. Let’s here compare all variants in their (more performant) in-place versions.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $p0;\n  objective_type=:Euclidean,\n  evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 9 samples with 1 evaluation.\n Range (min … max):  594.171 ms … 618.063 ms  ┊ GC (min … max): 8.40% … 8.11%\n Time  (median):     599.492 ms               ┊ GC (median):    8.37%\n Time  (mean ± σ):   600.660 ms ±   7.276 ms  ┊ GC (mean ± σ):  8.40% ± 0.16%\n\n  █ ██    █    █ █    █  █                                    █  \n  █▁██▁▁▁▁█▁▁▁▁█▁█▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  594 ms           Histogram: frequency by time          618 ms <\n\n Memory estimate: 1.97 GiB, allocs estimate: 62496.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -> Hess_f(E, Y, p, X)), $p0;\n  evaluation=InplaceEvaluation(),\n  objective_type=:Euclidean\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 203 samples with 1 evaluation.\n Range (min … max):  18.204 ms … 33.191 ms  ┊ GC (min … max): 6.49% … 6.59%\n Time  (median):     25.342 ms              ┊ GC (median):    8.72%\n Time  (mean ± σ):   24.617 ms ±  2.267 ms  ┊ GC (mean ± σ):  8.51% ± 2.03%\n\n            ▂                     ▆█▆                          \n  █▄▄▁▄▁▄▄▁▄█▇▁▄▁▁▄▆▄▁▁▄▆▁▁▄▁▁▄▄▄████▆▁▁▄▄▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▄▄ ▆\n  18.2 ms      Histogram: log(frequency) by time      30.9 ms <\n\n Memory estimate: 43.63 MiB, allocs estimate: 5471.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"@benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -> Hess_f(M, Y, p, X)), $p0;\n    evaluation=InplaceEvaluation(),\n)","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"BenchmarkTools.Trial: 424 samples with 1 evaluation.\n Range (min … max):  11.032 ms …  17.260 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     11.284 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   11.791 ms ± 870.710 μs  ┊ GC (mean ± σ):  3.86% ± 5.39%\n\n   ▁▇▇█ ▁                                                       \n  ▃██████▅▃▄▃▂▁▂▁▁▁▁▁▁▂▁▂▁▁▂▂▂▂▂▁▁▁▂▁▃▄▅▇▆▅▄▆▃▄▂▃▂▂▁▂▁▁▁▁▁▁▁▂▂ ▃\n  11 ms           Histogram: frequency by time         13.8 ms <\n\n Memory estimate: 13.15 MiB, allocs estimate: 5448.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Of course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged.","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q1, q) for q ∈ [q2,q3] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 7.359460685640475e-16\n 0.048053815279360104","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[distance(M, q3, q) for q ∈ [q4,q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"2-element Vector{Float64}:\n 0.08270031469111411\n 0.08270031469111411","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"Which we can also see in the final cost, comparing it to the Eigenvalue","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"[f(M, q) - λ for q ∈ [q1, q2, q3, q4, q5] ]","category":"page"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"5-element Vector{Float64}:\n  0.013874911807420176\n  0.013874911807405965\n -0.41812208889417946\n  3.552713678800501e-14\n  3.552713678800501e-14","category":"page"},{"location":"examples/RayleighQuotient/#Summary","page":"The Rayleigh Quotient","title":"Summary","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute.","category":"page"},{"location":"examples/RayleighQuotient/#Literature","page":"The Rayleigh Quotient","title":"Literature","text":"","category":"section"},{"location":"examples/RayleighQuotient/","page":"The Rayleigh Quotient","title":"The Rayleigh Quotient","text":"N. Boumal. An Introduction to Optimization on Smooth Manifolds. First Edition (Cambridge University Press, 2023).\n\n\n\n","category":"page"},{"location":"helpers/error_measures/#Error-measures","page":"Error measures","title":"Error measures","text":"","category":"section"},{"location":"helpers/error_measures/","page":"Error measures","title":"Error measures","text":"Modules = [ManoptExamples]\nPages = [\"ErrorMeasures.jl\"]\nOrder = [:type, :function]\nPrivate = true","category":"page"},{"location":"helpers/error_measures/#ManoptExamples.mean_average_error-Tuple{ManifoldsBase.AbstractManifold, Any, Any}","page":"Error measures","title":"ManoptExamples.mean_average_error","text":"mean_average_error(M,x,y)\n\nCompute the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"method"},{"location":"helpers/error_measures/#ManoptExamples.mean_squared_error-Union{Tuple{mT}, Tuple{mT, Any, Any}} where mT<:ManifoldsBase.AbstractManifold","page":"Error measures","title":"ManoptExamples.mean_squared_error","text":"mean_squared_error(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"method"},{"location":"#Welcome-to-ManoptExample.jl","page":"Home","title":"Welcome to ManoptExample.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ManoptExamples.ManoptExamples","category":"page"},{"location":"#ManoptExamples.ManoptExamples","page":"Home","title":"ManoptExamples.ManoptExamples","text":"🏔️⛷️ ManoptExamples.jl – A collection of research and tutorial example problems for Manopt.jl\n\n📚 Documentation: juliamanifolds.github.io/ManoptExamples.jl\n📦 Repository: github.com/JuliaManifolds/ManoptExamples.jl\n💬 Discussions: github.com/JuliaManifolds/ManoptExamples.jl/discussions\n🎯 Issues: github.com/JuliaManifolds/ManoptExamples.jl/issues\n\n\n\n\n\n","category":"module"},{"location":"","page":"Home","title":"Home","text":"This package provides a set of example tasks for Manopt.jl based on either generic manifolds from the ManifoldsBase.jl interface or specific manifolds from Manifolds.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each example usually consists of","category":"page"},{"location":"","page":"Home","title":"Home","text":"a cost function and additional objects, like the gradient or proximal maps, see objectives\nan example explaining how to use these, see examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Helping functions that are used in one or more examples can be found in the section of functions in the menu.","category":"page"}]
}
